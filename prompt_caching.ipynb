{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt caching through the Anthropic API\n",
    "\n",
    "Prompt caching allows you to store and reuse context within your prompt. This makes it more practical to include additional information in your prompt—such as detailed instructions and example responses—which help improve every response Claude generates.\n",
    "\n",
    "In addition, by fully leveraging prompt caching within your prompt, you can reduce latency by >2x and costs up to 90%. This can generate significant savings when building solutions that involve repetitive tasks around detailed book_content.\n",
    "\n",
    "In this cookbook, we will demonstrate how to use prompt caching in a single turn and across a multi-turn conversation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Setup\n",
    "\n",
    "First, let's set up our environment with the necessary imports and initializations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install scrapegraphai\n",
    "# %playwright install \n",
    "# %pip install 'scrapegraphai[burr]'\n",
    "# %pip install nest_asyncio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !ollama list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json \n",
    "# from scrapegraphai.graphs import SmartScraperGraph\n",
    "\n",
    "# # Load the graph\n",
    "# graph_config = {\n",
    "#   \"llm\": {\n",
    "#     \"model\": \"ollama/llama3.1:latest\",\n",
    "#     \"temperature\": 0.0,\n",
    "#     \"format\": \"json\",\n",
    "#     \"base_url\": \"http://localhost:11434\",\n",
    "#   },\n",
    "#   \"embeddings\": {\n",
    "#     \"model\": \"ollama/nomic-embed-text:latest\",\n",
    "#     \"base_url\": \"http://localhost:11434\",\n",
    "#   }\n",
    "# }\n",
    "\n",
    "# original_link = \"https://dp0-2.lsst.io/data-access-analysis-tools/adql-recipes.html#polygon-search\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # create the SmartScraperGraph instace \n",
    "# smart_scraper_graph = SmartScraperGraph(\n",
    "#   prompt=\"extract the website content and structure, don't forget any words\",\n",
    "#   source=original_link,\n",
    "#   config=graph_config\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nest_asyncio\n",
    "# import asyncio\n",
    "\n",
    "# # Apply the nest_asyncio patch\n",
    "# nest_asyncio.apply()\n",
    "\n",
    "# result = smart_scraper_graph.run()\n",
    "# print(json.dumps(result, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install PyPDF2\n",
    "# %pip install pdfplumber\n",
    "# %pip install requests pdfkit pdfplumber\n",
    "# !sudo apt-get install wkhtmltopdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "# import html2text\n",
    "# import os\n",
    "# import nbformat\n",
    "# from nbconvert import MarkdownExporter\n",
    "# import json\n",
    "# import hashlib\n",
    "# import time\n",
    "\n",
    "# class LinkCounter:\n",
    "#     def __init__(self):\n",
    "#         self.jupyter_count = 0\n",
    "#         self.other_count = 0\n",
    "\n",
    "#     def increment(self, is_jupyter):\n",
    "#         if is_jupyter:\n",
    "#             self.jupyter_count += 1\n",
    "#         else:\n",
    "#             self.other_count += 1\n",
    "\n",
    "#     def __str__(self):\n",
    "#         return f\"Jupyter notebooks: {self.jupyter_count}, Other links: {self.other_count}\"\n",
    "\n",
    "# def html_to_markdown(url):\n",
    "#     response = requests.get(url)\n",
    "#     html_content = response.text\n",
    "#     soup = BeautifulSoup(html_content, 'html.parser')\n",
    "#     for script in soup([\"script\", \"style\"]):\n",
    "#         script.decompose()\n",
    "#     h = html2text.HTML2Text()\n",
    "#     h.ignore_links = False\n",
    "#     h.ignore_images = False\n",
    "#     h.ignore_tables = False\n",
    "#     h.body_width = 0\n",
    "#     markdown_content = h.handle(str(soup))\n",
    "#     return markdown_content, soup\n",
    "\n",
    "# def extract_links(soup, base_url):\n",
    "#     links = {}\n",
    "#     for a in soup.find_all('a', href=True):\n",
    "#         href = a['href']\n",
    "#         if href.startswith('/'):\n",
    "#             href = base_url + href\n",
    "#         if href.startswith('http'):\n",
    "#             file_name = generate_unique_filename(href)\n",
    "#             links[file_name] = href\n",
    "#     return links\n",
    "\n",
    "# def generate_unique_filename(url):\n",
    "#     hash_object = hashlib.md5(url.encode())\n",
    "#     return hash_object.hexdigest()[:10] + '.md'\n",
    "\n",
    "# def jupyter_to_markdown(jupyter_path):\n",
    "#     with open(jupyter_path, \"r\") as file:\n",
    "#         notebook = nbformat.read(file, as_version=4)\n",
    "#     exporter = MarkdownExporter()           \n",
    "#     markdown, _ = exporter.from_notebook_node(notebook)\n",
    "#     return markdown\n",
    "\n",
    "# def save_markdown(content, file_path):\n",
    "#     with open(file_path, \"w\", encoding='utf-8') as file:\n",
    "#         file.write(content)\n",
    "\n",
    "# def load_processed_links(output_dir):\n",
    "#     processed_links_file = os.path.join(output_dir, \"processed_links.json\")\n",
    "#     if os.path.exists(processed_links_file):\n",
    "#         with open(processed_links_file, 'r') as f:\n",
    "#             return json.load(f)\n",
    "#     return {}\n",
    "\n",
    "# def save_processed_links(processed_links, output_dir):\n",
    "#     processed_links_file = os.path.join(output_dir, \"processed_links.json\")\n",
    "#     with open(processed_links_file, 'w') as f:\n",
    "#         json.dump(processed_links, f, indent=2)\n",
    "\n",
    "# def process_url(url, output_dir, processed_links, link_counter, depth=0, max_depth=100):\n",
    "#     if depth > max_depth:\n",
    "#         return processed_links\n",
    "\n",
    "#     os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "#     if url in processed_links.values():\n",
    "#         print(f\"Skipping already processed URL: {url}\")\n",
    "#         return processed_links\n",
    "\n",
    "#     print(f\"Processing URL: {url} (Depth: {depth})\")\n",
    "    \n",
    "#     try:\n",
    "#         markdown_content, soup = html_to_markdown(url)\n",
    "#         file_name = generate_unique_filename(url)\n",
    "#         file_path = os.path.join(output_dir, file_name)\n",
    "#         save_markdown(markdown_content, file_path)\n",
    "#         processed_links[file_name] = url\n",
    "#         print(f\"Content saved to {file_path}\")\n",
    "\n",
    "#         base_url = '/'.join(url.split('/')[:3])\n",
    "#         links = extract_links(soup, base_url)\n",
    "\n",
    "#         for file_name, link in links.items():\n",
    "#             if link in processed_links.values():\n",
    "#                 print(f\"Skipping already processed link: {link}\")\n",
    "#                 continue\n",
    "\n",
    "#             file_path = os.path.join(output_dir, file_name)\n",
    "#             if link.endswith('.ipynb'):\n",
    "#                 link_counter.increment(True)\n",
    "#                 try:\n",
    "#                     jupyter_content = requests.get(link).text\n",
    "#                     jupyter_file = file_path.replace('.md', '.ipynb')\n",
    "#                     with open(jupyter_file, 'w') as f:\n",
    "#                         f.write(jupyter_content)\n",
    "#                     markdown = jupyter_to_markdown(jupyter_file)\n",
    "#                     save_markdown(markdown, file_path)\n",
    "#                     processed_links[file_name] = link\n",
    "#                     print(f\"Jupyter notebook converted and saved to {file_path}\")\n",
    "#                 except Exception as e:\n",
    "#                     print(f\"Error processing Jupyter notebook {link}: {str(e)}\")\n",
    "#             else:\n",
    "#                 link_counter.increment(False)\n",
    "#                 processed_links[file_name] = link\n",
    "#                 print(f\"Link recorded: {link}\")\n",
    "\n",
    "#             save_processed_links(processed_links, output_dir)\n",
    "\n",
    "#             processed_links = process_url(link, output_dir, processed_links, link_counter, depth + 1, max_depth)\n",
    "\n",
    "#             time.sleep(1)\n",
    "\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error processing URL {url}: {str(e)}\")\n",
    "\n",
    "#     return processed_links\n",
    "\n",
    "# # Main execution\n",
    "# # url = \"https://dp0-3.lsst.io/tutorials-dp0-3/index.html#dp0-3-tutorials-contributed\"\n",
    "# url = \"https://www.lsst.io/\"\n",
    "\n",
    "# output_dir = \"extracted_content\"\n",
    "# processed_links = load_processed_links(output_dir)\n",
    "# link_counter = LinkCounter()\n",
    "# process_url(url, output_dir, processed_links, link_counter, max_depth=100)\n",
    "\n",
    "# print(\"\\nLink Processing Summary:\")\n",
    "# print(link_counter)\n",
    "# print(f\"Total links processed: {len(processed_links)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install nbformat nbconvert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import html2text\n",
    "import os\n",
    "import nbformat\n",
    "from nbconvert import MarkdownExporter\n",
    "import json\n",
    "import hashlib\n",
    "import time\n",
    "\n",
    "import re\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import html2text\n",
    "\n",
    "import urllib3\n",
    "from requests.exceptions import SSLError, RequestException\n",
    "\n",
    "# Disable SSL warnings\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "def html_to_markdown(url, verify_ssl=False):\n",
    "    try: \n",
    "         # Fetch HTML content from the URL\n",
    "        response = requests.get(url, verify=verify_ssl, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        html_content = response.text\n",
    "        \n",
    "        # Parse HTML content\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        # # Remove script and style elements\n",
    "        # for script in soup([\"script\", \"style\"]):\n",
    "        #     script.decompose()\n",
    "        \n",
    "        # Configure HTML to Markdown converter\n",
    "        h = html2text.HTML2Text()\n",
    "        h.ignore_links = True\n",
    "        h.ignore_images = True\n",
    "        h.ignore_tables = True\n",
    "        h.body_width = 0  # Disable line wrapping\n",
    "        \n",
    "        # Convert HTML to Markdown\n",
    "        markdown_content = h.handle(str(soup))\n",
    "        \n",
    "        # Clean up the Markdown content\n",
    "        # cleaned_content = clean_markdown(markdown_content)\n",
    "        # return cleaned_content, soup\n",
    "\n",
    "        return markdown_content, soup\n",
    "    except SSLError as e:\n",
    "        print(f\"Error processing URL {url}: {str(e)}\")\n",
    "        return \"\", None\n",
    "\n",
    "    except RequestException as e:\n",
    "        print(f\"Error processing URL {url}: {str(e)}\")\n",
    "        return \"\", None\n",
    "\n",
    "def clean_markdown(content):\n",
    "\n",
    "    # Remove [![Rubin Observatory logo](...)](/)\n",
    "    content = re.sub(r'\\[!\\[Rubin Observatory logo\\]\\(.*?\\)\\]\\(.*?\\)', '', content)\n",
    "\n",
    "    # Remove: ## Footer, ### Footer navigation, and the subsequent content\n",
    "    content = re.sub(r'## Footer.*', '', content, flags=re.DOTALL) # Remove Footer, Footer navigation, and the subsequent content\n",
    "\n",
    "    # Remove: ## Navigation, ### Navigation, and the subsequent content \n",
    "    content = re.sub(r'## Navigation.*', '', content, flags=re.DOTALL) # Remove Navigation, Navigation, and the subsequent content  \n",
    "\n",
    "    # Remove: ## Sidebar, ### Sidebar, and the subsequent content\n",
    "    content = re.sub(r'## Sidebar.*', '', content, flags=re.DOTALL) # Remove Sidebar, Sidebar, and the subsequent content\n",
    "\n",
    "    # Remove: ## Navigation Menu\n",
    "    content = re.sub(r'## Navigation Menu.*', '', content, flags=re.DOTALL) # Remove Navigation Menu and the subsequent content\n",
    "\n",
    "    # Remove: ## History\n",
    "    content = re.sub(r'## History.*', '', content, flags=re.DOTALL) # Remove History and the subsequent content                                         \n",
    "\n",
    "    # Remove: ## Have feedback?\n",
    "    content = re.sub(r'## Have feedback\\?.*', '', content, flags=re.DOTALL) # Remove Have feedback? and the subsequent content\n",
    "\n",
    "    # Remove: images, links \n",
    "    content = re.sub(r'!\\[.*?\\]\\(.*?\\)', '', content) # Remove images\n",
    "    content = re.sub(r'\\[.*?\\]\\(.*?\\)', '', content) # Remove links\n",
    "\n",
    "    # Remove: unnecessary whitespace\n",
    "    content = content.strip()\n",
    "\n",
    "    # Remove: redundant links\n",
    "    content = re.sub(r'\\[(.*?)\\]\\(.*?\\)', r'\\1', content)\n",
    "\n",
    "    # Remove: the line with less than 5 characters\n",
    "    content = re.sub(r'^.{1,5}$\\n?', '', content, flags=re.MULTILINE)\n",
    "\n",
    "\n",
    "    \n",
    "    # Remove: empty lines\n",
    "    content = re.sub(r'\\n\\s*\\n', '\\n\\n', content)\n",
    "    # Remove empty line\n",
    "    content = re.sub(r'^\\s*\\n', '', content)\n",
    "    # Remove empty line\n",
    "    content = re.sub(r'\\n\\s*$', '', content)\n",
    "\n",
    "\n",
    "    # # Remove \"Skip to content\" and navigation menu\n",
    "    # content = re.sub(r'Skip to content.*?##', '##', content, flags=re.DOTALL)\n",
    "    \n",
    "    # # Remove base64-encoded images\n",
    "    # content = re.sub(r'!\\[.*?\\]\\(data:image/[^;]+;base64,[^\\)]+\\)', '', content)\n",
    "    \n",
    "    # # Remove empty lines\n",
    "    # content = re.sub(r'\\n\\s*\\n', '\\n\\n', content)\n",
    "    \n",
    "    # # Remove unnecessary whitespace\n",
    "    # content = content.strip()\n",
    "    \n",
    "    # # Simplify headers (remove extra #)\n",
    "    # content = re.sub(r'#{3,}', '##', content)\n",
    "    \n",
    "    # # Remove redundant links\n",
    "    # content = re.sub(r'\\[(.*?)\\]\\(.*?\\)', r'\\1', content)\n",
    "    \n",
    "    # # Remove navigation menu items\n",
    "    # content = re.sub(r'^\\s*\\*\\s.*$\\n?', '', content, flags=re.MULTILINE)\n",
    "    \n",
    "    # # Remove \"You must be signed in\" messages\n",
    "    # content = re.sub(r'You must be signed in.*$\\n?', '', content, flags=re.MULTILINE)\n",
    "    \n",
    "    # # Remove GitHub-specific elements\n",
    "    # content = re.sub(r'(Notifications|Fork \\d+|Star \\d+|Branches|Tags|Activity)', '', content)\n",
    "    \n",
    "    # # Remove empty bullet points\n",
    "    # content = re.sub(r'^\\s*\\*\\s*$\\n?', '', content, flags=re.MULTILINE)\n",
    "    \n",
    "    # # Remove lines with just symbols\n",
    "    # content = re.sub(r'^\\s*[#\\*\\-]+\\s*$\\n?', '', content, flags=re.MULTILINE)\n",
    "    \n",
    "    # # Remove repeated newlines\n",
    "    # content = re.sub(r'\\n{3,}', '\\n\\n', content)\n",
    "    \n",
    "    return content\n",
    "\n",
    "# def html_to_markdown(url):\n",
    "#     # Fetch HTML content from the URL\n",
    "#     response = requests.get(url)\n",
    "#     html_content = response.text\n",
    "    \n",
    "#     # Parse HTML content\n",
    "#     soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "#     # Remove script and style elements\n",
    "#     for script in soup([\"script\", \"style\"]):\n",
    "#         script.decompose()\n",
    "    \n",
    "#     # Configure HTML to Markdown converter\n",
    "#     h = html2text.HTML2Text()\n",
    "#     h.ignore_links = False\n",
    "#     h.ignore_images = False\n",
    "#     h.ignore_tables = False\n",
    "#     h.body_width = 0  # Disable line wrapping\n",
    "    \n",
    "#     # Convert HTML to Markdown\n",
    "#     markdown_content = h.handle(str(soup))\n",
    "#     return markdown_content, soup\n",
    "\n",
    "def extract_links(soup, base_url):\n",
    "    # Extract all links from the page\n",
    "    links = {}\n",
    "    for a in soup.find_all('a', href=True):\n",
    "        href = a['href']\n",
    "        # Handle relative URLs\n",
    "        if href.startswith('/'):\n",
    "            href = base_url + href\n",
    "        if href.startswith('http'):\n",
    "            file_name = generate_unique_filename(href)\n",
    "            links[file_name] = href\n",
    "    return links\n",
    "\n",
    "def generate_unique_filename(url):\n",
    "    # Generate a unique filename based on URL hash\n",
    "    hash_object = hashlib.md5(url.encode())\n",
    "    return hash_object.hexdigest()[:10] + '.md'\n",
    "\n",
    "def jupyter_to_markdown(jupyter_path):\n",
    "    # Convert Jupyter notebook to Markdown\n",
    "    with open(jupyter_path, \"r\") as file:\n",
    "        notebook = nbformat.read(file, as_version=4)\n",
    "    exporter = MarkdownExporter()           \n",
    "    markdown, _ = exporter.from_notebook_node(notebook)\n",
    "    return markdown\n",
    "\n",
    "def save_markdown(content, file_path):\n",
    "    # Save Markdown content to file\n",
    "    with open(file_path, \"w\", encoding='utf-8') as file:\n",
    "        file.write(content)\n",
    "\n",
    "def load_processed_links(output_dir):\n",
    "    # Load previously processed links from JSON file\n",
    "    processed_links_file = os.path.join(output_dir, \"processed_links.json\")\n",
    "    if os.path.exists(processed_links_file):\n",
    "        with open(processed_links_file, 'r') as f:\n",
    "            return json.load(f)\n",
    "    return {}\n",
    "\n",
    "def save_processed_links(processed_links, output_dir):\n",
    "    # Save processed links to JSON file\n",
    "    processed_links_file = os.path.join(output_dir, \"processed_links.json\")\n",
    "    with open(processed_links_file, 'w') as f:\n",
    "        json.dump(processed_links, f, indent=2)\n",
    "\n",
    "def process_url(url, output_dir, processed_links, depth=0, max_depth=100, link_count=0, jupyter_count=0):\n",
    "    # Recursive function to process URLs and their links\n",
    "    if depth > max_depth:\n",
    "        return processed_links, link_count, jupyter_count\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Skip if URL has already been processed\n",
    "    if url in processed_links.values():\n",
    "        print(f\"Skipping already processed URL: {url}\")\n",
    "        return processed_links, link_count, jupyter_count\n",
    "\n",
    "    print(f\"Processing URL: {url}\")\n",
    "    \n",
    "    # Convert page to Markdown\n",
    "    markdown_content, soup = html_to_markdown(url)\n",
    "    file_name = generate_unique_filename(url)\n",
    "    file_path = os.path.join(output_dir, file_name)\n",
    "    save_markdown(markdown_content, file_path)\n",
    "    processed_links[file_name] = url\n",
    "    print(f\"Content saved to {file_path}\")\n",
    "\n",
    "    # Extract and process links\n",
    "    base_url = '/'.join(url.split('/')[:3])\n",
    "    links = extract_links(soup, base_url)\n",
    "\n",
    "    # Process each extracted link\n",
    "    for file_name, link in links.items():\n",
    "        if link in processed_links.values():\n",
    "            print(f\"Skipping already processed link: {link}\")\n",
    "            continue\n",
    "\n",
    "        file_path = os.path.join(output_dir, file_name)\n",
    "        if link.endswith('.ipynb'):\n",
    "            # Handle Jupyter notebooks\n",
    "            try:\n",
    "                jupyter_content = requests.get(link).text\n",
    "                jupyter_file = file_path.replace('.md', '.ipynb')\n",
    "                with open(jupyter_file, 'w') as f:\n",
    "                    f.write(jupyter_content)\n",
    "                markdown = jupyter_to_markdown(jupyter_file)\n",
    "                save_markdown(markdown, file_path)\n",
    "                processed_links[file_name] = link\n",
    "                print(f\"Jupyter notebook converted and saved to {file_path}\")\n",
    "                jupyter_count += 1\n",
    "                \n",
    "                # Delete the Jupyter notebook file after conversion\n",
    "                os.remove(jupyter_file)\n",
    "                print(f\"Deleted Jupyter notebook: {jupyter_file}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing Jupyter notebook {link}: {str(e)}\")\n",
    "        else:\n",
    "            # Handle other links\n",
    "            try:\n",
    "                content, _ = html_to_markdown(link)\n",
    "                save_markdown(content, file_path)\n",
    "                processed_links[file_name] = link\n",
    "                print(f\"Content from {link} saved to {file_path}\")\n",
    "                link_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing link {link}: {str(e)}\")\n",
    "\n",
    "        # Save progress after each processed link\n",
    "        save_processed_links(processed_links, output_dir)\n",
    "\n",
    "        # Recursively process the new link\n",
    "        processed_links, link_count, jupyter_count = process_url(link, output_dir, processed_links, depth + 1, max_depth, link_count, jupyter_count)\n",
    "\n",
    "        # Delay to avoid overwhelming the server\n",
    "        time.sleep(1)\n",
    "\n",
    "    return processed_links, link_count, jupyter_count\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    url = \"https://www.lsst.io/\"\n",
    "    output_dir = \"extracted_content\"\n",
    "    processed_links = load_processed_links(output_dir)\n",
    "    \n",
    "    processed_links, link_count, jupyter_count = process_url(url, output_dir, processed_links)\n",
    "\n",
    "    # # remove markdown file if it is less than 20 lines \n",
    "    # for file in os.listdir(output_dir):\n",
    "    #     file_path = os.path.join(output_dir, file)\n",
    "    #     with open(file_path, 'r') as f:\n",
    "    #         lines = f.readlines()\n",
    "    #         if len(lines) < 20:\n",
    "    #             os.remove(file_path)\n",
    "    #             print(f\"Deleted {file_path}\")       \n",
    "    \n",
    "    print(f\"Total links processed: {link_count}\")\n",
    "    print(f\"Total Jupyter notebooks processed: {jupyter_count}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/home/david/Desktop/scraping-rubin-links/extracted_content/processed_links.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load filename and URL mapping\n",
    "# file_path = \"processed_links.json\"\n",
    "with open(file_path, 'r') as f:\n",
    "    processed_links = json.load(f)\n",
    "\n",
    "# Extract URLs from processed_links\n",
    "urls = list(processed_links.values())\n",
    "\n",
    "# Process URLs\n",
    "for file_name, url in processed_links.items():\n",
    "    print(f\"Processing: {url}\")\n",
    "    markdown_content, soup = html_to_markdown(url)\n",
    "    if markdown_content:\n",
    "        base_url = '/'.join(url.split('/')[:3])\n",
    "        links = extract_links(soup, base_url)\n",
    "        for link_file_name, link in links.items():\n",
    "            if link not in urls:\n",
    "                print(f\"New link found: {link}\")\n",
    "                content, _ = html_to_markdown(link)\n",
    "                if content:\n",
    "                    file_path = os.path.join(\"extracted_content\", generate_unique_filename(link))\n",
    "                    save_markdown(content, file_path)\n",
    "                    urls.append(link)\n",
    "                    print(f\"Content from {link} saved to {file_path}.md\")\n",
    "            else:\n",
    "                print(f\"Skipping already processed link: {link}\")\n",
    "\n",
    "print(\"Processing completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(urls)\n",
    "\n",
    "# check the number of links which include ipynb files\n",
    "ipynb_links = [url for url in urls if url.endswith('.ipynb')]\n",
    "len(ipynb_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://www.lsst.io/\"\n",
    "url = \"https://github.com/lsst\"\n",
    "output_dir = \"extracted_content\"\n",
    "processed_links = load_processed_links(output_dir)\n",
    "\n",
    "processed_links, link_count, jupyter_count = process_url(url, output_dir, processed_links)\n",
    "\n",
    "# # remove markdown file if it is less than 20 lines \n",
    "# for file in os.listdir(output_dir):\n",
    "#     file_path = os.path.join(output_dir, file)\n",
    "#     with open(file_path, 'r') as f:\n",
    "#         lines = f.readlines()\n",
    "#         if len(lines) < 20:\n",
    "#             os.remove(file_path)\n",
    "#             print(f\"Deleted {file_path}\")       \n",
    "\n",
    "print(f\"Total links processed: {link_count}\")\n",
    "print(f\"Total Jupyter notebooks processed: {jupyter_count }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = \"https://www.lsst.io/\"\n",
    "# url = \"https://github.com/lsst\"\n",
    "url = \"https://github.com/rubin-dp0/tutorial-notebooks\"\n",
    "output_dir = \"extracted_content\"\n",
    "processed_links = load_processed_links(output_dir)\n",
    "\n",
    "processed_links, link_count, jupyter_count = process_url(url, output_dir, processed_links)\n",
    "\n",
    "# # remove markdown file if it is less than 20 lines \n",
    "# for file in os.listdir(output_dir):\n",
    "#     file_path = os.path.join(output_dir, file)\n",
    "#     with open(file_path, 'r') as f:\n",
    "#         lines = f.readlines()\n",
    "#         if len(lines) < 20:\n",
    "#             os.remove(file_path)\n",
    "#             print(f\"Deleted {file_path}\")       \n",
    "\n",
    "print(f\"Total links processed: {link_count}\")\n",
    "print(f\"Total Jupyter notebooks processed: {jupyter_count }\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a file with the urls\n",
    "with open(\"extracted_content/urls.txt\", \"w\") as file:\n",
    "    for url in urls:\n",
    "        file.write(url + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new folder, and copy  the extracted content\n",
    "import shutil\n",
    "\n",
    "shutil.copytree(\"extracted_content\", \"extracted_content_v2\")                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep the .md files in the extracted_content_v2 folder\n",
    "for file in os.listdir(\"extracted_content_v2\"):\n",
    "    if not file.endswith(\".md\"):\n",
    "        os.remove(os.path.join(\"extracted_content_v2\", file)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "/home/david/Desktop/scraping-rubin-links/prompt_caching.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the .md file which include dictionary -> {\"payload\":\n",
    "for file in os.listdir(\"extracted_content_v2\"):\n",
    "    file_path = os.path.join(\"extracted_content_v2\", file)\n",
    "    with open(file_path, 'r') as f:\n",
    "        content = f.read()\n",
    "        if content.startswith('{\"payload\":'):\n",
    "            os.remove(file_path)\n",
    "            print(f\"Deleted {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove .md files which has the same content\n",
    "content_files = {}        \n",
    "for file in os.listdir(\"extracted_content_v2\"):\n",
    "    file_path = os.path.join(\"extracted_content_v2\", file)\n",
    "    with open(file_path, 'r') as f:\n",
    "        content = f.read()\n",
    "        if content in content_files:\n",
    "            os.remove(file_path)\n",
    "            print(f\"Deleted {file_path}\")\n",
    "        else:\n",
    "            content_files[content] = file_path        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new folder, transfer .md files to .mdx file \n",
    "os.makedirs(\"extracted_content_mdx\", exist_ok=True)\n",
    "\n",
    "for file in os.listdir(\"extracted_content_v2\"):\n",
    "    file_path = os.path.join(\"extracted_content_v2\", file)\n",
    "    new_file_path = os.path.join(\"extracted_content_mdx\", file.replace(\".md\", \".mdx\"))\n",
    "    shutil.move(file_path, new_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import html2text\n",
    "import os\n",
    "import json\n",
    "import urllib3\n",
    "\n",
    "# Disable SSL warnings (use with caution)\n",
    "urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)\n",
    "\n",
    "def html_to_markdown(url):\n",
    "    try:\n",
    "        # Fetch HTML content from the URL, ignoring SSL verification\n",
    "        response = requests.get(url, verify=False, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        html_content = response.text\n",
    "        \n",
    "        # Parse HTML content\n",
    "        soup = BeautifulSoup(html_content, 'html.parser')\n",
    "        \n",
    "        # Configure HTML to Markdown converter\n",
    "        h = html2text.HTML2Text()\n",
    "        h.ignore_links = False\n",
    "        h.ignore_images = False\n",
    "        h.ignore_tables = False\n",
    "        h.body_width = 0  # Disable line wrapping\n",
    "        \n",
    "        # Convert HTML to Markdown\n",
    "        markdown_content = h.handle(str(soup))\n",
    "        \n",
    "        # Clean up the Markdown content\n",
    "        cleaned_content = clean_markdown(markdown_content)\n",
    "        \n",
    "        return cleaned_content, soup\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "def extract_links(soup, base_url):\n",
    "    links = {}\n",
    "    if soup:\n",
    "        for a in soup.find_all('a', href=True):\n",
    "            href = a['href']\n",
    "            if href.startswith('/'):\n",
    "                href = base_url + href\n",
    "            if href.startswith('http'):\n",
    "                file_name = generate_unique_filename(href)\n",
    "                links[file_name] = href\n",
    "    return links\n",
    "\n",
    "def generate_unique_filename(url):\n",
    "    # Generate a unique filename based on URL\n",
    "    return url.split('/')[-1] or 'index'\n",
    "\n",
    "def save_markdown(content, file_path):\n",
    "    os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "    with open(file_path + '.md', \"w\", encoding='utf-8') as file:\n",
    "        file.write(content)\n",
    "\n",
    "# Load filename and URL mapping\n",
    "# file_path = \"processed_links.json\"\n",
    "with open(file_path, 'r') as f:\n",
    "    processed_links = json.load(f)\n",
    "\n",
    "# Extract URLs from processed_links\n",
    "urls = list(processed_links.values())\n",
    "\n",
    "# Process URLs\n",
    "for file_name, url in processed_links.items():\n",
    "    print(f\"Processing: {url}\")\n",
    "    markdown_content, soup = html_to_markdown(url)\n",
    "    if markdown_content:\n",
    "        base_url = '/'.join(url.split('/')[:3])\n",
    "        links = extract_links(soup, base_url)\n",
    "        for link_file_name, link in links.items():\n",
    "            if link not in urls:\n",
    "                print(f\"New link found: {link}\")\n",
    "                content, _ = html_to_markdown(link)\n",
    "                if content:\n",
    "                    file_path = os.path.join(\"extracted_content\", generate_unique_filename(link))\n",
    "                    save_markdown(content, file_path)\n",
    "                    urls.append(link)\n",
    "                    print(f\"Content from {link} saved to {file_path}.md\")\n",
    "            else:\n",
    "                print(f\"Skipping already processed link: {link}\")\n",
    "\n",
    "print(\"Processing completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load filename and URL mapping\n",
    "# with open(file_path, 'r') as f:\n",
    "#     processed_links = json.load(f)\n",
    "\n",
    "# # Display the processed links\n",
    "# for file_name, url in processed_links.items():\n",
    "#     print(f\"{file_name}: {url}\")\n",
    "\n",
    "# # extract the url from the processed_links and save to a list\n",
    "# urls = []                       \n",
    "# for file_name, url in processed_links.items():\n",
    "#     urls.append(url)                                        \n",
    "                   \n",
    "# # iterate through the urls, extract the links in url, if the link is not in the urls, extract the content and save to a file, if the link is in the urls, skip, add the link to the urls list\n",
    "# # interatvie through the urls list\n",
    "# for file_name, url in processed_links.items():\n",
    "#     # extract the links in the url\n",
    "#     markdown_content, soup = html_to_markdown(url)\n",
    "#     base_url = '/'.join(url.split('/')[:3])\n",
    "#     links = extract_links(soup, base_url)\n",
    "#     for file_name, link in links.items():\n",
    "#         # if the link is not in the urls list, extract the content and save to a file\n",
    "#         if link not in urls:\n",
    "#             # create a new folder using the file_name, file_name doesn't include .md\n",
    "#             file_name = file_name.replace('.md', '')\n",
    "#             file_path = os.path.join(\"extracted_content\", file_name)\n",
    "#             # file_path = os.path.join(output_dir, file_name)\n",
    "#             content, _ = html_to_markdown(link)\n",
    "#             save_markdown(content, file_path)\n",
    "#             urls.append(link)\n",
    "#             print(f\"Content from {link} saved to {file_path}\")\n",
    "#         else:\n",
    "#             # if the link is in the urls list, skip\n",
    "#             print(f\"Skipping already processed link: {link}\")         \n",
    "# # extract the links in the url\n",
    "# # if the link is not in the urls list, extract the content and save to a file\n",
    "# # if the link is in the urls list, skip\n",
    "# # add the link to the urls list\n",
    "\n",
    "\n",
    "\n",
    "# # create a folder using the name of the URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import html2text\n",
    "import os\n",
    "import json\n",
    "import hashlib\n",
    "import time\n",
    "from urllib.parse import urljoin, urlparse\n",
    "\n",
    "class LinkCounter:\n",
    "    def __init__(self):\n",
    "        self.jupyter_count = 0\n",
    "        self.other_count = 0\n",
    "\n",
    "    def increment(self, is_jupyter):\n",
    "        if is_jupyter:\n",
    "            self.jupyter_count += 1\n",
    "        else:\n",
    "            self.other_count += 1\n",
    "\n",
    "    def __str__(self):\n",
    "        return f\"Jupyter notebooks: {self.jupyter_count}, Other links: {self.other_count}\"\n",
    "\n",
    "def html_to_markdown(url):\n",
    "    response = requests.get(url)\n",
    "    html_content = response.text\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.decompose()\n",
    "    h = html2text.HTML2Text()\n",
    "    h.ignore_links = False\n",
    "    h.ignore_images = False\n",
    "    h.ignore_tables = False\n",
    "    h.body_width = 0\n",
    "    markdown_content = h.handle(str(soup))\n",
    "    return markdown_content, soup\n",
    "\n",
    "def extract_links(soup, base_url):\n",
    "    links = set()\n",
    "    for a in soup.find_all('a', href=True):\n",
    "        href = a['href']\n",
    "        full_url = urljoin(base_url, href)\n",
    "        if full_url.startswith('http'):\n",
    "            links.add(full_url)\n",
    "    return links\n",
    "\n",
    "def generate_unique_filename(url):\n",
    "    parsed_url = urlparse(url)\n",
    "    domain = parsed_url.netloc\n",
    "    path = parsed_url.path.strip('/')\n",
    "    hash_object = hashlib.md5(url.encode())\n",
    "    hash_str = hash_object.hexdigest()[:10]\n",
    "    return f\"{domain}_{path}_{hash_str}.md\".replace('/', '_')\n",
    "\n",
    "def save_markdown(content, file_path):\n",
    "    with open(file_path, \"w\", encoding='utf-8') as file:\n",
    "        file.write(content)\n",
    "\n",
    "def load_processed_links(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        with open(file_path, 'r') as f:\n",
    "            return json.load(f)\n",
    "    return {}\n",
    "\n",
    "def save_processed_links(processed_links, file_path):\n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(processed_links, f, indent=2)\n",
    "\n",
    "def process_url(url, output_dir, processed_links, link_counter, depth=0, max_depth=100):\n",
    "    if depth >= max_depth or url in processed_links:\n",
    "        return processed_links, set()\n",
    "\n",
    "    print(f\"Processing URL: {url} (Depth: {depth})\")\n",
    "    \n",
    "    try:\n",
    "        markdown_content, soup = html_to_markdown(url)\n",
    "        file_name = generate_unique_filename(url)\n",
    "        file_path = os.path.join(output_dir, file_name)\n",
    "        save_markdown(markdown_content, file_path)\n",
    "        processed_links[url] = file_name\n",
    "        print(f\"Content saved to {file_path}\")\n",
    "\n",
    "        if url.endswith('.ipynb'):\n",
    "            link_counter.increment(True)\n",
    "        else:\n",
    "            link_counter.increment(False)\n",
    "\n",
    "        new_links = extract_links(soup, url)\n",
    "        return processed_links, new_links\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing URL {url}: {str(e)}\")\n",
    "        return processed_links, set()\n",
    "\n",
    "def main():\n",
    "    output_dir = \"extracted_content\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    processed_links_file = \"/home/david/Desktop/scraping-rubin-links/extracted_content/processed_links.json\"\n",
    "    processed_links = load_processed_links(processed_links_file)\n",
    "    link_counter = LinkCounter()\n",
    "\n",
    "    # Start with the links from processed_links.json\n",
    "    links_to_process = set(processed_links.keys())\n",
    "\n",
    "    iteration = 0\n",
    "    while iteration < 100 and links_to_process:\n",
    "        iteration += 1\n",
    "        print(f\"\\nIteration {iteration}\")\n",
    "        \n",
    "        new_links = set()\n",
    "        for url in list(links_to_process):\n",
    "            processed_links, extracted_links = process_url(url, output_dir, processed_links, link_counter)\n",
    "            new_links.update(extracted_links)\n",
    "            links_to_process.remove(url)\n",
    "            time.sleep(1)  # Be respectful to the server\n",
    "\n",
    "        links_to_process.update(new_links - set(processed_links.keys()))\n",
    "        save_processed_links(processed_links, processed_links_file)\n",
    "        \n",
    "        print(f\"Links remaining to process: {len(links_to_process)}\")\n",
    "        if not links_to_process:\n",
    "            print(\"All links processed.\")\n",
    "            break\n",
    "\n",
    "    print(\"\\nLink Processing Summary:\")\n",
    "    print(link_counter)\n",
    "    print(f\"Total links processed: {len(processed_links)}\")\n",
    "    print(f\"Total iterations: {iteration}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import html2text\n",
    "import os\n",
    "import nbformat\n",
    "from nbconvert import MarkdownExporter\n",
    "import json\n",
    "import hashlib\n",
    "import time\n",
    "\n",
    "def html_to_markdown(url):\n",
    "    # Fetch HTML content from the URL\n",
    "    response = requests.get(url)\n",
    "    html_content = response.text\n",
    "    \n",
    "    # Parse HTML content\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    \n",
    "    # Remove script and style elements\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.decompose()\n",
    "    \n",
    "    # Configure HTML to Markdown converter\n",
    "    h = html2text.HTML2Text()\n",
    "    h.ignore_links = False\n",
    "    h.ignore_images = False\n",
    "    h.ignore_tables = False\n",
    "    h.body_width = 0  # Disable line wrapping\n",
    "    \n",
    "    # Convert HTML to Markdown\n",
    "    markdown_content = h.handle(str(soup))\n",
    "    return markdown_content, soup\n",
    "\n",
    "def extract_links(soup, base_url):\n",
    "    # Extract all links from the page\n",
    "    links = {}\n",
    "    for a in soup.find_all('a', href=True):\n",
    "        href = a['href']\n",
    "        # Handle relative URLs\n",
    "        if href.startswith('/'):\n",
    "            href = base_url + href\n",
    "        if href.startswith('http'):\n",
    "            file_name = generate_unique_filename(href)\n",
    "            links[file_name] = href\n",
    "    return links\n",
    "\n",
    "def generate_unique_filename(url):\n",
    "    # Generate a unique filename based on URL hash\n",
    "    hash_object = hashlib.md5(url.encode())\n",
    "    return hash_object.hexdigest()[:10] + '.md'\n",
    "\n",
    "def jupyter_to_markdown(jupyter_path):\n",
    "    # Convert Jupyter notebook to Markdown\n",
    "    with open(jupyter_path, \"r\") as file:\n",
    "        notebook = nbformat.read(file, as_version=4)\n",
    "    exporter = MarkdownExporter()           \n",
    "    markdown, _ = exporter.from_notebook_node(notebook)\n",
    "    return markdown\n",
    "\n",
    "def save_markdown(content, file_path):\n",
    "    # Save Markdown content to file\n",
    "    with open(file_path, \"w\", encoding='utf-8') as file:\n",
    "        file.write(content)\n",
    "\n",
    "def load_processed_links(output_dir):\n",
    "    # Load previously processed links from JSON file\n",
    "    processed_links_file = os.path.join(output_dir, \"processed_links.json\")\n",
    "    if os.path.exists(processed_links_file):\n",
    "        with open(processed_links_file, 'r') as f:\n",
    "            return json.load(f)\n",
    "    return {}\n",
    "\n",
    "def save_processed_links(processed_links, output_dir):\n",
    "    # Save processed links to JSON file\n",
    "    processed_links_file = os.path.join(output_dir, \"processed_links.json\")\n",
    "    with open(processed_links_file, 'w') as f:\n",
    "        json.dump(processed_links, f, indent=2)\n",
    "\n",
    "def process_url(url, output_dir, processed_links, depth=0, max_depth=100):\n",
    "    # Recursive function to process URLs and their links\n",
    "    if depth > max_depth:\n",
    "        return processed_links\n",
    "\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Skip if URL has already been processed\n",
    "    if url in processed_links.values():\n",
    "        print(f\"Skipping already processed URL: {url}\")\n",
    "        return processed_links\n",
    "\n",
    "    print(f\"Processing URL: {url}\")\n",
    "    \n",
    "    # Convert page to Markdown\n",
    "    markdown_content, soup = html_to_markdown(url)\n",
    "    file_name = generate_unique_filename(url)\n",
    "    file_path = os.path.join(output_dir, file_name)\n",
    "    save_markdown(markdown_content, file_path)\n",
    "    processed_links[file_name] = url\n",
    "    print(f\"Content saved to {file_path}\")\n",
    "\n",
    "    # Extract and process links\n",
    "    base_url = '/'.join(url.split('/')[:3])\n",
    "    links = extract_links(soup, base_url)\n",
    "\n",
    "    # Process each extracted link\n",
    "    for file_name, link in links.items():\n",
    "        if link in processed_links.values():\n",
    "            print(f\"Skipping already processed link: {link}\")\n",
    "            continue\n",
    "\n",
    "        file_path = os.path.join(output_dir, file_name)\n",
    "        if link.endswith('.ipynb'):\n",
    "            # Handle Jupyter notebooks\n",
    "            try:\n",
    "                jupyter_content = requests.get(link).text\n",
    "                jupyter_file = file_path.replace('.md', '.ipynb')\n",
    "                with open(jupyter_file, 'w') as f:\n",
    "                    f.write(jupyter_content)\n",
    "                markdown = jupyter_to_markdown(jupyter_file)\n",
    "                save_markdown(markdown, file_path)\n",
    "                processed_links[file_name] = link\n",
    "                print(f\"Jupyter notebook converted and saved to {file_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing Jupyter notebook {link}: {str(e)}\")\n",
    "        else:\n",
    "            # Handle other links\n",
    "            try:\n",
    "                content, _ = html_to_markdown(link)\n",
    "                save_markdown(content, file_path)\n",
    "                processed_links[file_name] = link\n",
    "                print(f\"Content from {link} saved to {file_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing link {link}: {str(e)}\")\n",
    "\n",
    "        # Save progress after each processed link\n",
    "        save_processed_links(processed_links, output_dir)\n",
    "\n",
    "        # Recursively process the new link\n",
    "        processed_links = process_url(link, output_dir, processed_links, depth + 1, max_depth)\n",
    "\n",
    "        # Delay to avoid overwhelming the server\n",
    "        time.sleep(1)\n",
    "\n",
    "    return processed_links\n",
    "\n",
    "# Main execution\n",
    "# url = \"https://dp0-3.lsst.io/tutorials-dp0-3/index.html#dp0-3-tutorials-contributed\"\n",
    "url = \"https://www.lsst.io/\"\n",
    "\n",
    "output_dir = \"extracted_content\"\n",
    "processed_links = load_processed_links(output_dir)\n",
    "process_url(url, output_dir, processed_links)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import html2text\n",
    "import os\n",
    "import nbformat\n",
    "from nbconvert import MarkdownExporter\n",
    "import json\n",
    "\n",
    "def html_to_markdown(url):\n",
    "    response = requests.get(url)\n",
    "    html_content = response.text\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.decompose()\n",
    "    h = html2text.HTML2Text()\n",
    "    h.ignore_links = False\n",
    "    h.ignore_images = False\n",
    "    h.ignore_tables = False\n",
    "    h.body_width = 0\n",
    "    markdown_content = h.handle(str(soup))\n",
    "    return markdown_content, soup\n",
    "\n",
    "def extract_links(soup, base_url):\n",
    "    links = {}\n",
    "    for a in soup.find_all('a', href=True):\n",
    "        href = a['href']\n",
    "        if href.startswith('/'):\n",
    "            href = base_url + href\n",
    "        if href.startswith('http'):\n",
    "            file_name = a.text.strip().replace(' ', '_') + '.md'\n",
    "            links[file_name] = href\n",
    "    return links\n",
    "\n",
    "def jupyter_to_markdown(jupyter_path):\n",
    "    with open(jupyter_path, \"r\") as file:\n",
    "        notebook = nbformat.read(file, as_version=4)\n",
    "    exporter = MarkdownExporter()           \n",
    "    markdown, _ = exporter.from_notebook_node(notebook)\n",
    "    return markdown\n",
    "\n",
    "def save_markdown(content, file_path):\n",
    "    with open(file_path, \"w\", encoding='utf-8') as file:\n",
    "        file.write(content)\n",
    "\n",
    "def process_url(url, output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Convert main page to Markdown\n",
    "    markdown_content, soup = html_to_markdown(url)\n",
    "    main_file = os.path.join(output_dir, \"main_page.md\")\n",
    "    save_markdown(markdown_content, main_file)\n",
    "    print(f\"Main content saved to {main_file}\")\n",
    "\n",
    "    # Extract and process links\n",
    "    base_url = '/'.join(url.split('/')[:3])\n",
    "    links = extract_links(soup, base_url)\n",
    "    links_file = os.path.join(output_dir, \"links.json\")\n",
    "    with open(links_file, 'w') as f:\n",
    "        json.dump(links, f, indent=2)\n",
    "    print(f\"Links saved to {links_file}\")\n",
    "\n",
    "    # Process each link\n",
    "    for file_name, link in links.items():\n",
    "        file_path = os.path.join(output_dir, file_name)\n",
    "        if link.endswith('.ipynb'):\n",
    "            try:\n",
    "                jupyter_content = requests.get(link).text\n",
    "                with open(file_path.replace('.md', '.ipynb'), 'w') as f:\n",
    "                    f.write(jupyter_content)\n",
    "                markdown = jupyter_to_markdown(file_path.replace('.md', '.ipynb'))\n",
    "                save_markdown(markdown, file_path)\n",
    "                print(f\"Jupyter notebook converted and saved to {file_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing Jupyter notebook {link}: {str(e)}\")\n",
    "        else:\n",
    "            try:\n",
    "                content, _ = html_to_markdown(link)\n",
    "                save_markdown(content, file_path)\n",
    "                print(f\"Content from {link} saved to {file_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing link {link}: {str(e)}\")\n",
    "\n",
    "# Main execution\n",
    "# url = \"https://github.com/lsst\"\n",
    "url = \"https://github.com/rubin-dp0/tutorial-notebooks\"\n",
    "output_dir = \"extracted_content\"\n",
    "process_url(url, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jupyter_path = \"/home/david/Desktop/scraping-rubin-links/dist/all-links/DP02_01_Introduction_to_DP02.ipynb\"\n",
    "\n",
    "# Read the Jupyter notebook\n",
    "import nbformat\n",
    "from nbconvert import MarkdownExporter\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with open(jupyter_path, \"r\") as file:\n",
    "    notebook = nbformat.read(file, as_version=4)\n",
    "\n",
    "# convert the notebook to markdown\n",
    "exporter = MarkdownExporter()           \n",
    "markdown, _ = exporter.from_notebook_node(notebook)\n",
    "\n",
    "# Save the markdown\n",
    "markdown_path = jupyter_path.replace(\".ipynb\", \".md\")\n",
    "with open(markdown_path, \"w\") as file:\n",
    "    file.write(markdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import html2text\n",
    "\n",
    "def html_to_markdown(url):\n",
    "    # Fetch the HTML content\n",
    "    response = requests.get(url)\n",
    "    html_content = response.text\n",
    "\n",
    "    # Parse the HTML content\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Remove script and style elements\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.decompose()\n",
    "\n",
    "    # Initialize html2text\n",
    "    h = html2text.HTML2Text()\n",
    "    h.ignore_links = False\n",
    "    h.ignore_images = False\n",
    "    h.ignore_tables = False\n",
    "    h.body_width = 0  # Disable line wrapping\n",
    "\n",
    "    # Convert to Markdown\n",
    "    markdown_content = h.handle(str(soup))\n",
    "\n",
    "    return markdown_content\n",
    "\n",
    "# Specify the URL of the HTML page\n",
    "# url = \"https://dp0-3.lsst.io/tutorials-dp0-3/index.html#dp0-3-tutorials-contributed\"\n",
    "url = \"https://dp0-3.lsst.io/\"\n",
    "\n",
    "# Convert HTML to Markdown\n",
    "markdown_content = html_to_markdown(url)\n",
    "\n",
    "# Write the Markdown content to a file\n",
    "markdown_file = \"page.md\"\n",
    "with open(markdown_file, \"w\", encoding='utf-8') as file:\n",
    "    file.write(markdown_content)\n",
    "\n",
    "print(f\"Content extracted and saved to {markdown_file}\")\n",
    "\n",
    "# Optionally, print the first 500 characters of the Markdown content\n",
    "print(\"\\nFirst 500 characters of Markdown content:\")\n",
    "print(markdown_content[:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install anthropic bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import anthropic\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# client = anthropic.Anthropic()\n",
    "# MODEL_NAME = \"claude-3-5-sonnet-20240620\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fetch some text content to use in our examples. We'll use the text from Pride and Prejudice by Jane Austen which is around ~187,000 tokens long."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "\n",
    "def fetch_article_content(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Remove script and style elements\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.decompose()\n",
    "    \n",
    "    # Get text\n",
    "    text = soup.get_text()\n",
    "    \n",
    "    # Break into lines and remove leading and trailing space on each\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    # Break multi-headlines into a line each\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    # Drop blank lines\n",
    "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def extract_linked_url(content):\n",
    "    # Regular expression to find URLs\n",
    "    url_pattern = re.compile(r'https?://\\S+')\n",
    "    match = url_pattern.search(content)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read links and filenames from allLinks.json\n",
    "with open('/home/david/Desktop/scraping-rubin-links/dist/all-links/allLinks.json', 'r') as file:\n",
    "    links_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.parse\n",
    "\n",
    "# print(len(links_data))\n",
    "count = 0\n",
    "urls = []\n",
    "links_clean = {}\n",
    "for i, links in enumerate(links_data):\n",
    "  for j, link in enumerate(links):\n",
    "    # print(i, j)\n",
    "    # print(link)\n",
    "    filename = link['filename']    \n",
    "    url = link['links'][0]\n",
    "    urls.append(url)\n",
    "    # use the filename and url  create a new file name \n",
    "    domain = urllib.parse.urlparse(url).netloc\n",
    "    new_filename = f\"{domain}-{filename}\"\n",
    "\n",
    "    # add the new filename and url to the links_clean dictionary  \n",
    "    links_clean[new_filename] = url\n",
    "    count += 1\n",
    "\n",
    "\n",
    "# check if there any duplicate links in the links_clean dictionary\n",
    "# if there are duplicates, then remove the duplicates\n",
    "seen_urls = set()\n",
    "unique_links_names = {}\n",
    "\n",
    "for new_filename, url in links_clean.items():\n",
    "  # find unique links\n",
    "    if url not in seen_urls:\n",
    "        seen_urls.add(url)\n",
    "        unique_links_names[new_filename] = url\n",
    "\n",
    "\n",
    "    # # print(f\"filename: {filename}\")               \n",
    "    # url = link[\"links\"][0]\n",
    "    # urls.append(url)\n",
    "    # links_clean[filename] = url\n",
    "    # count += 1\n",
    "    # print(f\"url: {url}\")\n",
    "\n",
    "    # if len(url) > 1:\n",
    "    #   print(f\"url: {url}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(unique_links_names))\n",
    "print(unique_links_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read unique_links_names, fetech the content of the article from url and save it to a file use the filename\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Ensure the directory exists\n",
    "output_dir = \"/home/david/Desktop/scraping-rubin-links/dist/all-links/articles/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Read unique_links_names, fetch the content of the article from url and save it to a file using the filename\n",
    "for filename, url in unique_links_names.items():\n",
    "    print(f\"filename: {filename}\")\n",
    "    print(f\"url: {url}\")\n",
    "    content = fetch_article_content(url)\n",
    "    with open(os.path.join(output_dir, filename), 'w') as file:\n",
    "        file.write(content)\n",
    "    time.sleep(1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read links and filenames from allLinks.json\n",
    "with open('/home/david/Desktop/scraping-rubin-links/dist/all-links/allLinks.json', 'r') as file:\n",
    "    links_data = json.load(file)\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(\"data/papers\", exist_ok=True)\n",
    "\n",
    "for entrys in links_data:\n",
    "    # print(f\"entry: {entrys}\")\n",
    "    for entry in entrys:\n",
    "        print(f\"entry: {entry}\")\n",
    "        if isinstance(entry, dict) and 'links' in entry and 'filename' in entry:\n",
    "            \n",
    "            file_name = entry['filename']\n",
    "            \n",
    "            # Get the URL of the book\n",
    "            for link in entry['links']:\n",
    "                book_url = link['url']\n",
    "                    # break\n",
    "            \n",
    "                print(f\"Fetching {book_url}...\")\n",
    "                # Fetch the content of the article\n",
    "                book_content = fetch_article_content(book_url)\n",
    "\n",
    "                \n",
    "                # Extract the linked URL from the content\n",
    "                linked_url = extract_linked_url(book_content)\n",
    "                \n",
    "                # Store the book content\n",
    "                file_path = f\"data/papers/{file_name}.md\"\n",
    "                with open(file_path, \"w\") as file:\n",
    "                    file.write(book_content)\n",
    "                \n",
    "                # Store the linked URL\n",
    "                if linked_url:\n",
    "                    linked_url_path = f\"data/papers/{file_name}_linked_url.txt\"\n",
    "                    with open(linked_url_path, \"w\") as file:\n",
    "                        file.write(linked_url)\n",
    "                \n",
    "                print(f\"Fetched {len(book_content)} characters from {book_url}.\")\n",
    "                print(\"First 500 characters:\")\n",
    "                print(book_content[:500])\n",
    "                if linked_url:\n",
    "                    print(f\"Extracted linked URL: {linked_url}\")\n",
    "                else:\n",
    "                    print(\"No linked URL found.\")\n",
    "        else:\n",
    "            print(f\"Invalid entry: {entry}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pwd\n",
    "# %pip install nbformat nbconvert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jupyter_path = \"/home/david/Desktop/scraping-rubin-links/dist/all-links/DP02_01_Introduction_to_DP02.ipynb\"\n",
    "\n",
    "# Read the Jupyter notebook\n",
    "import nbformat\n",
    "from nbconvert import MarkdownExporter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jupyter_path = \"/home/david/Desktop/scraping-rubin-links/dist/all-links/DP02_01_Introduction_to_DP02.ipynb\"\n",
    "\n",
    "# Read the Jupyter notebook\n",
    "import nbformat\n",
    "from nbconvert import MarkdownExporter\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "with open(jupyter_path, \"r\") as file:\n",
    "    notebook = nbformat.read(file, as_version=4)\n",
    "\n",
    "# convert the notebook to markdown\n",
    "exporter = MarkdownExporter()           \n",
    "markdown, _ = exporter.from_notebook_node(notebook)\n",
    "\n",
    "# Save the markdown\n",
    "markdown_path = jupyter_path.replace(\".ipynb\", \".md\")\n",
    "with open(markdown_path, \"w\") as file:\n",
    "    file.write(markdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# featch the content from the url and extract the linked url from the content, save the content to a markdown file and the linked url to another file\n",
    "# create fetch_link_content() function to do this\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "\n",
    "def fetch_link_content(url, content_filename, links_filename):\n",
    "    # Fetch the content from the URL\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "    content = response.text\n",
    "\n",
    "    # Parse the content to extract linked URLs\n",
    "    soup = BeautifulSoup(content, 'html.parser')\n",
    "    links = [a['href'] for a in soup.find_all('a', href=True)]\n",
    "\n",
    "    # Save the content to a Markdown file\n",
    "    with open(content_filename, 'w') as content_file:\n",
    "        content_file.write(content)\n",
    "\n",
    "    # Save the linked URLs to another file\n",
    "    with open(links_filename, 'w') as links_file:\n",
    "        for link in links:\n",
    "            links_file.write(link + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_link = \"https://dp0-2.lsst.io/\"\n",
    "\n",
    "# Read content from the link and save it to a markdown file: create a file name from the link     \n",
    "# content = fetch_link_content(original_link)\n",
    "content_filename = original_link.replace(\"https://\", \"\").replace(\"http://\", \"\").replace(\"/\", \"-\") + \".md\"\n",
    "links_filename = original_link.replace(\"https://\", \"\").replace(\"http://\", \"\").replace(\"/\", \"-\") + \"_links.txt\"\n",
    "\n",
    "# # Ensure the directory exists\n",
    "# os.makedirs(os.path.dirname(content_filename), exist_ok=True)\n",
    "# os.makedirs(os.path.dirname(links_filename), exist_ok=True)\n",
    "\n",
    "fetch_link_content(original_link, content_filename, links_filename)\n",
    "                       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import os\n",
    "\n",
    "def fetch_article_content(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Remove script and style elements\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.decompose()\n",
    "    \n",
    "    # Get text\n",
    "    text = soup.get_text()\n",
    "    \n",
    "    # Break into lines and remove leading and trailing space on each\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    # Break multi-headlines into a line each\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    # Drop blank lines\n",
    "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def extract_linked_url(content):\n",
    "    # Regular expression to find URLs\n",
    "    url_pattern = re.compile(r'https?://\\S+')\n",
    "    match = url_pattern.search(content)\n",
    "    if match:\n",
    "        return match.group(0)\n",
    "    return None\n",
    "\n",
    "# Fetch the content of the article\n",
    "book_url = \"https://github.com/rubin-dp0/tutorial-notebooks/blob/main/DP02_01_Introduction_to_DP02.ipynb\"\n",
    "book_content = fetch_article_content(book_url)\n",
    "\n",
    "# Extract the linked URL from the content\n",
    "linked_url = extract_linked_url(book_content)\n",
    "\n",
    "# Ensure the directory exists\n",
    "os.makedirs(\"data/papers\", exist_ok=True)\n",
    "\n",
    "# Store the book content\n",
    "file_name = book_url.split(\"/\")[-2]\n",
    "file_path = f\"data/papers/{file_name}.md\"\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.write(book_content)\n",
    "\n",
    "# Store the linked URL\n",
    "if linked_url:\n",
    "    linked_url_path = f\"data/papers/{file_name}_linked_url.txt\"\n",
    "    with open(linked_url_path, \"w\") as file:\n",
    "        file.write(linked_url)\n",
    "\n",
    "print(f\"Fetched {len(book_content)} characters from the book.\")\n",
    "print(\"First 500 characters:\")\n",
    "print(book_content)\n",
    "# save the content to a md file\n",
    "with open(\"data/papers/book.md\", \"w\") as file:\n",
    "    file.write(book_content)\n",
    "if linked_url:\n",
    "    print(f\"Extracted linked URL: {linked_url}\")\n",
    "else:\n",
    "    print(\"No linked URL found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_article_content(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Remove script and style elements\n",
    "    for script in soup([\"script\", \"style\"]):\n",
    "        script.decompose()\n",
    "    \n",
    "    # Get text\n",
    "    text = soup.get_text()\n",
    "    \n",
    "    # Break into lines and remove leading and trailing space on each\n",
    "    lines = (line.strip() for line in text.splitlines())\n",
    "    # Break multi-headlines into a line each\n",
    "    chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
    "    # Drop blank lines\n",
    "    text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Fetch the content of the article\n",
    "# book_url = \"https://www.gutenberg.org/cache/epub/1342/pg1342.txt\"\n",
    "book_url = \"https://www.lsst.io/\"\n",
    "book_content = fetch_article_content(book_url)\n",
    "\n",
    "print(f\"Fetched {len(book_content)} characters from the book.\")\n",
    "print(\"First 500 characters:\")\n",
    "print(book_content[:])\n",
    "\n",
    "#and create a file name from the book_url, transfer book_content to markdown file, store the file to folder data/papers, \n",
    "\n",
    "file_name = book_url.split(\"/\")[-2]\n",
    "file_path = f\"data/papers/{file_name}.md\"\n",
    "with open(file_path, \"w\") as file:\n",
    "    file.write(book_content)\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Single turn\n",
    "\n",
    "Let's demonstrate prompt caching with a large document, comparing the performance and cost between cached and non-cached API calls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Non-cached API Call\n",
    "\n",
    "First, let's make a non-cached API call. This will load the prompt into the cache so that our subsequent cached API calls can benefit from the prompt caching.\n",
    "\n",
    "We will ask for a short output string to keep the output response time low since the benefit of prompt caching applies only to the input processing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_non_cached_api_call():\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"<book>\" + book_content + \"</book>\",\n",
    "                    \"cache_control\": {\"type\": \"ephemeral\"}\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"What is the title of this book? Only output the title.\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    start_time = time.time()\n",
    "    response = client.messages.create(\n",
    "        model=MODEL_NAME,\n",
    "        max_tokens=300,\n",
    "        messages=messages,\n",
    "        extra_headers={\"anthropic-beta\": \"prompt-caching-2024-07-31\"}\n",
    "\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    return response, end_time - start_time\n",
    "\n",
    "non_cached_response, non_cached_time = make_non_cached_api_call()\n",
    "\n",
    "print(f\"Non-cached API call time: {non_cached_time:.2f} seconds\")\n",
    "print(f\"Non-cached API call input tokens: {non_cached_response.usage.input_tokens}\")\n",
    "print(f\"Non-cached API call output tokens: {non_cached_response.usage.output_tokens}\")\n",
    "\n",
    "print(\"\\nSummary (non-cached):\")\n",
    "print(non_cached_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Cached API Call\n",
    "\n",
    "Now, let's make a cached API call. I'll add in the \"cache_control\": {\"type\": \"ephemeral\"} attribute to the content object and add the \"prompt-caching-2024-07-31\" beta header to the request. This will enable prompt caching for this API call.\n",
    "\n",
    "To keep the output latency constant, we will ask Claude the same question as before. Note that this question is not part of the cached content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_cached_api_call():\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"<book>\" + book_content + \"</book>\",\n",
    "                    \"cache_control\": {\"type\": \"ephemeral\"}\n",
    "                },\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \"What is the title of this book? Only output the title.\"\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    start_time = time.time()\n",
    "    response = client.messages.create(\n",
    "        model=MODEL_NAME,\n",
    "        max_tokens=300,\n",
    "        messages=messages,\n",
    "        extra_headers={\"anthropic-beta\": \"prompt-caching-2024-07-31\"}\n",
    "    )\n",
    "    end_time = time.time()\n",
    "\n",
    "    return response, end_time - start_time\n",
    "\n",
    "cached_response, cached_time = make_cached_api_call()\n",
    "\n",
    "print(f\"Cached API call time: {cached_time:.2f} seconds\")\n",
    "print(f\"Cached API call input tokens: {cached_response.usage.input_tokens}\")\n",
    "print(f\"Cached API call output tokens: {cached_response.usage.output_tokens}\")\n",
    "\n",
    "print(\"\\nSummary (cached):\")\n",
    "print(cached_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the cached API call only took 3.64 seconds total compared to 21.44 seconds for the non-cached API call. This is a significant improvement in overall latency due to caching."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2: Multi-turn Conversation with Incremental Caching\n",
    "\n",
    "Now, let's look at a multi-turn conversation where we add cache breakpoints as the conversation progresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationHistory:\n",
    "    def __init__(self):\n",
    "        # Initialize an empty list to store conversation turns\n",
    "        self.turns = []\n",
    "\n",
    "    def add_turn_assistant(self, content):\n",
    "        # Add an assistant's turn to the conversation history\n",
    "        self.turns.append({\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": content\n",
    "                }\n",
    "            ]\n",
    "        })\n",
    "\n",
    "    def add_turn_user(self, content):\n",
    "        # Add a user's turn to the conversation history\n",
    "        self.turns.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [\n",
    "                {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": content\n",
    "                }\n",
    "            ]\n",
    "        })\n",
    "\n",
    "    def get_turns(self):\n",
    "        # Retrieve conversation turns with specific formatting\n",
    "        result = []\n",
    "        user_turns_processed = 0\n",
    "        # Iterate through turns in reverse order\n",
    "        for turn in reversed(self.turns):\n",
    "            if turn[\"role\"] == \"user\" and user_turns_processed < 2:\n",
    "                # Add the last two user turns with ephemeral cache control\n",
    "                result.append({\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\n",
    "                            \"type\": \"text\",\n",
    "                            \"text\": turn[\"content\"][0][\"text\"],\n",
    "                            \"cache_control\": {\"type\": \"ephemeral\"}\n",
    "                        }\n",
    "                    ]\n",
    "                })\n",
    "                user_turns_processed += 1\n",
    "            else:\n",
    "                # Add other turns as they are\n",
    "                result.append(turn)\n",
    "        # Return the turns in the original order\n",
    "        return list(reversed(result))\n",
    "\n",
    "# Initialize the conversation history\n",
    "conversation_history = ConversationHistory()\n",
    "\n",
    "# System message containing the book content\n",
    "# Note: 'book_content' should be defined elsewhere in the code\n",
    "system_message = f\"<file_contents> {book_content} </file_contents>\"\n",
    "\n",
    "# Predefined questions for our simulation\n",
    "questions = [\n",
    "    \"What is the title of this novel?\",\n",
    "    \"Who are Mr. and Mrs. Bennet?\",\n",
    "    \"What is Netherfield Park?\",\n",
    "    \"What is the main theme of this novel?\"\n",
    "]\n",
    "\n",
    "def simulate_conversation():\n",
    "    for i, question in enumerate(questions, 1):\n",
    "        print(f\"\\nTurn {i}:\")\n",
    "        print(f\"User: {question}\")\n",
    "        \n",
    "        # Add user input to conversation history\n",
    "        conversation_history.add_turn_user(question)\n",
    "\n",
    "        # Record the start time for performance measurement\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Make an API call to the assistant\n",
    "        response = client.messages.create(\n",
    "            model=MODEL_NAME,\n",
    "            extra_headers={\n",
    "              \"anthropic-beta\": \"prompt-caching-2024-07-31\"\n",
    "            },\n",
    "            max_tokens=300,\n",
    "            system=[\n",
    "                {\"type\": \"text\", \"text\": system_message, \"cache_control\": {\"type\": \"ephemeral\"}},\n",
    "            ],\n",
    "            messages=conversation_history.get_turns(),\n",
    "        )\n",
    "\n",
    "        # Record the end time\n",
    "        end_time = time.time()\n",
    "\n",
    "        # Extract the assistant's reply\n",
    "        assistant_reply = response.content[0].text\n",
    "        print(f\"Assistant: {assistant_reply}\")\n",
    "\n",
    "        # Print token usage information\n",
    "        input_tokens = response.usage.input_tokens\n",
    "        output_tokens = response.usage.output_tokens\n",
    "        input_tokens_cache_read = getattr(response.usage, 'cache_read_input_tokens', '---')\n",
    "        input_tokens_cache_create = getattr(response.usage, 'cache_creation_input_tokens', '---')\n",
    "        print(f\"User input tokens: {input_tokens}\")\n",
    "        print(f\"Output tokens: {output_tokens}\")\n",
    "        print(f\"Input tokens (cache read): {input_tokens_cache_read}\")\n",
    "        print(f\"Input tokens (cache write): {input_tokens_cache_create}\")\n",
    "\n",
    "        # Calculate and print the elapsed time\n",
    "        elapsed_time = end_time - start_time\n",
    "\n",
    "        # Calculate the percentage of input prompt cached\n",
    "        total_input_tokens = input_tokens + (int(input_tokens_cache_read) if input_tokens_cache_read != '---' else 0)\n",
    "        percentage_cached = (int(input_tokens_cache_read) / total_input_tokens * 100 if input_tokens_cache_read != '---' and total_input_tokens > 0 else 0)\n",
    "\n",
    "        print(f\"{percentage_cached:.1f}% of input prompt cached ({total_input_tokens} tokens)\")\n",
    "        print(f\"Time taken: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "        # Add assistant's reply to conversation history\n",
    "        conversation_history.add_turn_assistant(assistant_reply)\n",
    "\n",
    "# Run the simulated conversation\n",
    "simulate_conversation()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see in this example, response times decreased from nearly 24 seconds to just 8-9 seconds after the initial cache setup, while maintaining the same level of quality across the answers. Most of this remaining latency is due to the time it takes to generate the response, which is not affected by prompt caching.\n",
    "\n",
    "And since nearly 100% of input tokens were cached in subsequent turns as we kept adjusting the cache breakpoints, we were able to read the next user message nearly instantly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
