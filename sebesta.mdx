CONCEPTS OF 
PROGRAMMING LANGUAGES
TENTH EDITIONThis page intentionally left blank CONCEPTS OF
PROGRAMMING LANGUAGES
TENTH EDITION
ROBERT W. SEBESTA
University of Colorado at Colorado Springs
 
Boston Columbus Indianapolis New York San Francisco Upper Saddle River 
Amsterdam Cape T own Dubai London Madrid Milan Munich Paris Montreal T oronto
Delhi Mexico City Sao Paulo Sydney Hong Kong Seoul Singapore T aipei T okyoVice President and Editorial Director, ECS: 
Marcia Horton
Editor in Chief: Michael HirschExecutive Editor: Matt GoldsteinEditorial Assistant: Chelsea KharakozovaVice President Marketing: Patrice JonesMarketing Manager: Yez AlayanMarketing Coordinator: Kathryn FerrantiMarketing Assistant: Emma SniderVice President and Director of Production: 
Vince O’Brien
Managing Editor: Jeff HolcombSenior Production Project Manager: Marilyn Lloyd
Manufacturing Manager: Nick SklitsisOperations Specialist: Lisa McDowellCover Designer: Anthony GemmellaroT ext Designer: Gillian HallCover Image: Mountain near Pisac, Peru; 
Photo by author
Media Editor: Dan SandinFull\-Service Vendor: LaserwordsProject Management: Gillian HallPrinter/Binder: Courier WestfordCover Printer: Lehigh\-Phoenix Color
This book was composed in InDesign. Basal font is Janson T ext. Display font is ITC Franklin Gothic.
Copyright © 2012, 2010, 2008, 2006, 2004 by Pearson Education, Inc., publishing as Addison\-Wesley. 
All rights reserved. Manufactured in the United States of America. This publication is protected by Copy\-right, and permission should be obtained from the publisher prior to any prohibited reproduction, storage in a retrieval system, or transmission in any form or by any means, electronic, mechanical, photocopying, recording, or likewise. T o obtain permission(s) to use material from this work, please submit a written request to Pearson Education, Inc., Permissions Department, One Lake Street, Upper Saddle River, New Jersey 07458, or you may fax your request to 201\-236\-3290\.
Many of the designations by manufacturers and sellers to distinguish their products are claimed as trade\-
marks. Where those designations appear in this book, and the publisher was aware of a trademark claim, the designations have been printed in initial caps or all caps.
Library of Congress Cataloging\-in\-Publication Data
Sebesta, Robert W.
 Concepts of programming languages / Robert W. Sebesta.—10th ed. p. cm. Includes bibliographical references and index. ISBN 978\-0\-13\-139531\-2 (alk. paper)1\. Programming languages (Electronic computers) I. Title. QA76\.7\.S43 2009 005\.13—dc22 2008055702
10 9 8 7 6 5 4 3 2 1
ISBN 10: 0\-13\-139531\-9
ISBN 13: 978\-0\-13\-139531\-2New to the Tenth Ed ition
Chapter 5: a new section on the let construct in functional pro\-
gramming languages was added
Chapter 6: the section on COBOL's record operations was removed;
new sections on lists, tuples, and unions in F\# were added
Chapter 8: discussions of Fortran's Do statement and Ada's case
statement were removed; descriptions of the control statements in
functional programming languages were moved to this chapter from
Chapter 15
Chapter 9: a new section on closures, a new section on calling sub\-
programs indirectly, and a new section on generic functions in F\# were
added; the description of Ada's generic subprograms was removed
Chapter 11: a new section on Objective\-C was added, the chapter
was substantially revised
Chapter 12: a new section on Objective\-C was added, five new fig\-
ures were added
Chapter 13: a section on concurrency in functional programming
languages was added; the discussion of Ada's asynchronous message
passing was removed
Chapter 14: a section on C\# event handling was added
Chapter 15: a new section on F\# and a new section on support for
functional programming in primarily imperative languages were added;
discussions of several different constructs in functional programming
languages were moved from Chapter 15 to earlier chaptersvi Preface
Changes for the Tenth Edition
The goals, overall structure, and approach of this tenth edition of Concepts 
of Programming Languages remain the same as those of the nine ear\-
lier editions. The principal goals are to introduce the main constructs 
of contemporary programming languages and to provide the reader with the tools necessary for the critical evaluation of existing and future programming languages. A secondary goal is to prepare the reader for the study of com\-piler design, by providing an in\-depth discussion of programming language structures, presenting a formal method of describing syntax and introducing approaches to lexical and syntatic analysis.
The tenth edition evolved from the ninth through several different kinds 
of changes. T o maintain the currency of the material, some of the discussion of older programming languages has been removed. For example, the descrip\-tion of COBOL ’s record operations was removed from Chapter 6 and that of Fortran’s 
Do statement was removed from Chapter 8\. Likewise, the description 
of Ada’s generic subprograms was removed from Chapter 9 and the discussion of Ada’s asynchronous message passing was removed from Chapter 13\.
On the other hand, a section on closures, a section on calling subprograms 
indirectly, and a section on generic functions in F\# were added to Chapter 9; sections on Objective\-C were added to Chapters 11 and 12; a section on con\-currency in functional programming languages was added to Chapter 13; a section on C\# event handling was added to Chapter 14; a section on F\# and a section on support for functional programming in primarily imperative lan\-guages were added to Chapter 15\.
In some cases, material has been moved. For example, several different 
discussions of constructs in functional programming languages were moved from Chapter 15 to earlier chapters. Among these were the descriptions of the control statements in functional programming languages to Chapter 8 and the lists and list operations of Scheme and ML to Chapter 6\. These moves indicate a significant shift in the philosophy of the book—in a sense, the mainstreaming of some of the constructs of functional programming languages. In previous editions, all discussions of functional programming language constructs were segregated in Chapter 15\.
Chapters 11, 12, and 15 were substantially revised, with five figures being 
added to Chapter 12\.
Finally, numerous minor changes were made to a large number of sections 
of the book, primarily to improve clarity.The Vision
This book describes the fundamental concepts of programming languages by 
discussing the design issues of the various language constructs, examining the design choices for these constructs in some of the most common languages, and critically comparing design alternatives.
Any serious study of programming languages requires an examination of 
some related topics, among which are formal methods of describing the syntax and semantics of programming languages, which are covered in Chapter 3\. Also, implementation techniques for various language constructs must be con\-sidered: Lexical and syntax analysis are discussed in Chapter 4, and implemen\-tation of subprogram linkage is covered in Chapter 10\. Implementation of some other language constructs is discussed in various other parts of the book.
The following paragraphs outline the contents of the tenth edition.
Chapter Outlines
Chapter 1 begins with a rationale for studying programming languages. It then discusses the criteria used for evaluating programming languages and language constructs. The primary influences on language design, common design trade\-offs, and the basic approaches to implementation are also examined.
Chapter 2 outlines the evolution of most of the important languages dis\-
cussed in this book. Although no language is described completely, the origins, purposes, and contributions of each are discussed. This historical overview is valuable, because it provides the background necessary to understanding the practical and theoretical basis for contemporary language design. It also moti\-vates further study of language design and evaluation. In addition, because none of the remainder of the book depends on Chapter 2, it can be read on its own, independent of the other chapters.
Chapter 3 describes the primary formal method for describing the syntax 
of programming language—BNF . This is followed by a description of attribute grammars, which describe both the syntax and static semantics of languages. The difficult task of semantic description is then explored, including brief introductions to the three most common methods: operational, denotational, and axiomatic semantics.
Chapter 4 introduces lexical and syntax analysis. This chapter is targeted to 
those colleges that no longer require a compiler design course in their curricula. Like Chapter 2, this chapter stands alone and can be read independently of the rest of the book.
Chapters 5 through 14 describe in detail the design issues for the primary 
constructs of programming languages. In each case, the design choices for several example languages are presented and evaluated. Specifically, Chapter 5 covers the many characteristics of variables, Chapter 6 covers data types, and Chapter 7 explains expressions and assignment statements. Chapter 8 describes control Preface viiviii Preface
statements, and Chapters 9 and 10 discuss subprograms and their implementa\-
tion. Chapter 11 examines data abstraction facilities. Chapter 12 provides an in\-depth discussion of language features that support object\-oriented programming (inheritance and dynamic method binding), Chapter 13 discusses concurrent program units, and Chapter 14 is about exception handling, along with a brief discussion of event handling.
The last two chapters (15 and 16\) describe two of the most important alterna\-
tive programming paradigms: functional programming and logic programming. However, some of the data structures and control constructs of functional pro\-gramming languages are discussed in Chapters 6 and 8\. Chapter 15 presents an introduction to Scheme, including descriptions of some of its primitive functions, special forms, and functional forms, as well as some examples of simple func\-tions written in Scheme. Brief introductions to ML, Haskell, and F\# are given to illustrate some different directions in functional language design. Chapter 16 introduces logic programming and the logic programming language, Prolog.
To the Instructor
In the junior\-level programming language course at the University of Colorado at Colorado Springs, the book is used as follows: We typically cover Chapters 1 and 3 in detail, and though students find it interesting and beneficial reading, Chapter 2 receives little lecture time due to its lack of hard technical content. Because no material in subsequent chapters depends on Chapter 2, as noted earlier, it can be skipped entirely, and because we require a course in compiler design, Chapter 4 is not covered.
Chapters 5 through 9 should be relatively easy for students with extensive 
programming experience in C\+\+, Java, or C\#. Chapters 10 through 14 are more challenging and require more detailed lectures.
Chapters 15 and 16 are entirely new to most students at the junior level. 
Ideally, language processors for Scheme and Prolog should be available for students required to learn the material in these chapters. Sufficient material is included to allow students to dabble with some simple programs.
Undergraduate courses will probably not be able to cover all of the mate\-
rial in the last two chapters. Graduate courses, however, should be able to completely discuss the material in those chapters by skipping over parts of the early chapters on imperative languages.
Supplemental Materials
The following supplements are available to all readers of this book at www
.pearsonhighered.com/cssupport .
• A set of lecture note slides. PowerPoint slides are available for each chapter 
in the book.
• PowerPoint slides containing all the figures in the book.A companion Website to the book is available at www.pearsonhighered.com/sebe\-
sta. This site contains mini\-manuals (approximately 100\-page tutorials) on a 
handful of languages. These proceed on the assumption that the student knows how to program in some other language, giving the student enough informa\-tion to complete the chapter materials in each language. Currently the site includes manuals for C\+\+, C, Java, and Smalltalk.
Solutions to many of the problem sets are available to qualified instruc\-
tors in our Instructor Resource Center at www.pearsonhighered.com/irc . 
Please contact your school’s Pearson Education representative or visit 
www.pearsonhighered.com/irc to register.
Language Processor Availability
Processors for and information about some of the programming languages 
discussed in this book can be found at the following Websites:
C, C\+\+, Fortran, and Ada gcc.gnu.org
C\# and F\# microsoft.com
Java java.sun.com
Haskell haskell.org
Lua www.lua.org
Scheme www.plt\-scheme.org/software/drscheme
Perl www.perl.com
Python www.python.org
Ruby www.ruby\-lang.org
JavaScript is included in virtually all browsers; PHP is included in virtually all 
Web servers.
All this information is also included on the companion Website.
Acknowledgments
The suggestions from outstanding reviewers contributed greatly to this book’s present form. In alphabetical order, they are:
Matthew Michael Burke
I\-ping Chu DePaul University
T eresa Cole Boise State University
Pamela Cutter Kalamazoo College
Amer Diwan University of Colorado
Stephen Edwards Virginia T ech
David E. GoldschmidtNigel Gwee Southern University–Baton RougePreface ixx Preface
Timothy Henry University of Rhode Island
Paul M. Jackowitz University of Scranton
Duane J. Jarc University of Maryland, University College
K. N. King Georgia State University
Donald Kraft Louisiana State University
Simon H. Lin California State University–Northridge
Mark Llewellyn University of Central Florida
Bruce R. Maxim University of Michigan–Dearborn
Robert McCloskey University of Scranton
Curtis Meadow University of Maine
Gloria Melara California State University–Northridge
Frank J. Mitropoulos Nova Southeastern University
Euripides Montagne University of Central Florida
Serita Nelesen Calvin College
Bob Neufeld Wichita State University
Charles Nicholas University of Maryland\-Baltimore County
Tim R. Norton University of Colorado\-Colorado Springs
Richard M. Osborne University of Colorado\-Denver
Saverio Perugini University of Dayton
Walter Pharr College of Charleston
Michael Prentice SUNY Buffalo
Amar Raheja California State Polytechnic University–Pomona
Hossein Saiedian University of Kansas
Stuart C. Shapiro SUNY Buffalo
Neelam Soundarajan Ohio State University
Ryan Stansifer Florida Institute of T echnology
Nancy Tinkham Rowan University
Paul T ymann Rochester Institute of T echnology
Cristian Videira Lopes University of California–Irvine
Sumanth Yenduri University of Southern Mississippi
Salih Yurttas T exas A\&M University
Numerous other people provided input for the previous editions of 
Concepts of Programming Languages at various stages of its development. All 
of their comments were useful and greatly appreciated. In alphabetical order, they are: Vicki Allan, Henry Bauer, Carter Bays, Manuel E. Bermudez, Peter Brouwer, Margaret Burnett, Paosheng Chang, Liang Cheng, John Crenshaw, Charles Dana, Barbara Ann Griem, Mary Lou Haag, John V . Harrison, Eileen Head, Ralph C. Hilzer, Eric Joanis, Leon Jololian, Hikyoo Koh, Jiang B. Liu, Meiliu Lu, Jon Mauney, Robert McCoard, Dennis L. Mumaugh, Michael G. Murphy, Andrew Oldroyd, Young Park, Rebecca Parsons, Steve J. Phelps, 
Jeffery Popyack, Raghvinder Sangwan, Steven Rapkin, Hamilton Richard, T om Sager, Joseph Schell, Sibylle Schupp, Mary Louise Soffa, Neelam Soundarajan, Ryan Stansifer, Steve Stevenson, Virginia T eller, Yang Wang, John M. Weiss, Franck Xia, and Salih Yurnas.Matt Goldstein, editor; Chelsea Kharakozova, editorial assistant; and, 
Marilyn Lloyd, senior production manager of Addison\-Wesley, and Gillian Hall of The Aardvark Group Publishing Services, all deserve my gratitude for their efforts to produce the tenth edition both quickly and carefully.
About the Author
Robert Sebesta is an Associate Professor Emeritus in the Computer Science Department at the University of Colorado–Colorado Springs. Professor Sebesta received a BS in applied mathematics from the University of Colorado in Boulder and MS and PhD degrees in computer science from Pennsylvania State University. He has taught computer science for more than 38 years. His professional interests are the design and evaluation of programming languages. Preface xixiiContents
 Chapter 1 Preliminaries 1
 1\.1 Reasons for Studying Concepts of Programming Languages ............... 2
 1\.2 Programming Domains ..................................................................... 5
 1\.3 Language Evaluation Criteria ........................................................... 7
 1\.4 Influences on Language Design ....................................................... 18
 1\.5 Language Categories ...................................................................... 21
 1\.6 Language Design Trade\-Offs ........................................................... 23
 1\.7 Implementation Methods ................................................................ 23
 1\.8 Programming Environments ........................................................... 31
 Summary • Review Questions • Problem Set .............................................. 31
 Chapter 2 Evolution of the Major Programming Languages 35
 2\.1 Zuse’s Plankalkül .......................................................................... 38
 2\.2 Pseudocodes .................................................................................. 39
 2\.3 The IBM 704 and Fortran .............................................................. 42
 2\.4 Functional Programming: LISP ...................................................... 47
 2\.5 The First Step Toward Sophistication: ALGOL 60 ........................... 52
 2\.6 Computerizing Business Records: COBOL ........................................ 58
 2\.7 The Beginnings of Timesharing: BASIC ........................................... 63
 Interview: ALAN COOPER—User Design and Language Design ................. 66
 2\.8 Everything for Everybody: PL/I ...................................................... 68
 2\.9 Two Early Dynamic Languages: APL and SNOBOL ......................... 71
 2\.10 The Beginnings of Data Abstraction: SIMULA 67 ........................... 72
 2\.11 Orthogonal Design: ALGOL 68 ....................................................... 73
 2\.12 Some Early Descendants of the ALGOLs ......................................... 75 Contents xiii
 2\.13 Programming Based on Logic: Prolog ............................................. 79
 2\.14 History’s Largest Design Effort: Ada .............................................. 81
 2\.15 Object\-Oriented Programming: Smalltalk ........................................ 85
 2\.16 Combining Imperative and Object\-Oriented Features: C\+\+................ 88
 2\.17 An Imperative\-Based Object\-Oriented Language: Java ..................... 91
 2\.18 Scripting Languages ....................................................................... 95
 2\.19 The Flagship .NET Language: C\# ................................................. 101
 2\.20 Markup/Programming Hybrid Languages ...................................... 104
 Summary • Bibliographic Notes • Review Questions • Problem Set • 
Programming Exercises ........................................................................... 106
 Chapter 3 Describing Syntax and Semantics 113
 3\.1 Introduction ................................................................................. 114
 3\.2 The General Problem of Describing Syntax .................................... 115
 3\.3 Formal Methods of Describing Syntax ........................................... 117
 3\.4 Attribute Grammars ..................................................................... 132
 History Note ..................................................................................... 133
 3\.5 Describing the Meanings of Programs: Dynamic Semantics ............ 139
 History Note ..................................................................................... 154
 Summary • Bibliographic Notes • Review Questions • Problem Set ........... 161
 Chapter 4 Lexical and Syntax Analysis 167
 4\.1 Introduction ................................................................................. 168
 4\.2 Lexical Analysis ........................................................................... 169
 4\.3 The Parsing Problem .................................................................... 177
 4\.4 Recursive\-Descent Parsing ............................................................ 181
 4\.5 Bottom\-Up Parsing ...................................................................... 190
 Summary • Review Questions • Problem Set • Programming Ex ercises ..... 197
 Chapter 5 Names, Bindings, and Scopes 203
 5\.1 Introduction ................................................................................. 204
 5\.2 Names ......................................................................................... 205
 History Note ..................................................................................... 205xiv Contents
 5\.3 Variables ..................................................................................... 207
 5\.4 The Concept of Binding ................................................................ 209
 5\.5 Scope .......................................................................................... 218
 5\.6 Scope and Lifetime ...................................................................... 229
 5\.7 Referencing Environments ............................................................ 230
 5\.8 Named Constants ......................................................................... 232
 Summary • Review Questions • Problem Set • Programming Ex ercises ..... 234
 Chapter 6 Data Types 243
 6\.1 Introduction ................................................................................. 244
 6\.2 Primitive Data Types .................................................................... 246
 6\.3 Character String Types ................................................................. 250
 History Note ..................................................................................... 251
 6\.4 User\-Defined Ordinal Types ........................................................... 255
 6\.5 Array Types .................................................................................. 259
 History Note ..................................................................................... 260
 History Note ..................................................................................... 261
 6\.6 Associative Arrays ........................................................................ 272
 Interview: ROBERTO IERUSALIMSCHY—Lua ........................... 274
 6\.7 Record Types ................................................................................ 276
 6\.8 Tuple Types .................................................................................. 280
 6\.9 List Types .................................................................................... 281
 6\.10 Union Types ................................................................................. 284
 6\.11 Pointer and Reference Types ......................................................... 289
 History Note ..................................................................................... 293
 6\.12 Type Checking .............................................................................. 302
 6\.13 Strong Typing ............................................................................... 303
 6\.14 Type Equivalence.......................................................................... 304
 6\.15 Theory and Data Types ................................................................. 308
 Summary • Bibliographic Notes • Review Questions • Problem Set • 
Programming Exercises ........................................................................... 310 Contents xv
 Chapter 7 Expressions and Assignment Statements 317
 7\.1 Introduction ................................................................................. 318
 7\.2 Arithmetic Expressions ................................................................ 318
 7\.3 Overloaded Operators ................................................................... 328
 7\.4 Type Conversions .......................................................................... 329
 History Note ..................................................................................... 332
 7\.5 Relational and Boolean Expressions .............................................. 332
 History Note ..................................................................................... 333
 7\.6 Short\-Circuit Evaluation .............................................................. 335
 7\.7 Assignment Statements ................................................................ 336
 History Note ..................................................................................... 340
 7\.8 Mixed\-Mode Assignment .............................................................. 341
 Summary • Review Questions • Problem Set • Programming Ex ercises ..... 341
 Chapter 8 Statement\-Level Control Structures 347
 8\.1 Introduction ................................................................................. 348
 8\.2 Selection Statements .................................................................... 350
 8\.3 Iterative Statements ..................................................................... 362
 8\.4 Unconditional Branching .............................................................. 375
 History Note ..................................................................................... 376
 8\.5 Guarded Commands ..................................................................... 376
 8\.6 Conclusions .................................................................................. 379
 Summary • Review Questions • Problem Set • Programming Ex ercises ..... 380
 Chapter 9 Subprograms 387
 9\.1 Introduction ................................................................................. 388
 9\.2 Fundamentals of Subprograms ..................................................... 388
 9\.3 Design Issues for Subprograms ..................................................... 396
 9\.4 Local Referencing Environments ................................................... 397
 9\.5 Parameter\-Passing Methods ......................................................... 399
 History Note ..................................................................................... 407
 History Note ..................................................................................... 407xvi Contents
 9\.6 Parameters That Are Subprograms ............................................... 417
 9\.7 Calling Subprograms Indirectly ..................................................... 419
 History Note ..................................................................................... 419
 9\.8 Overloaded Subprograms .............................................................. 421
 9\.9 Generic Subprograms ................................................................... 422
 9\.10 Design Issues for Functions .......................................................... 428
 9\.11 User\-Defined Overloaded Operators ............................................... 430
 9\.12 Closures ...................................................................................... 430
 9\.13 Coroutines ................................................................................... 432
 Summary • Review Questions • Problem Set • Programming Ex ercises ..... 435
 Chapter 10 Implementing Subprograms 441
 10\.1 The General Semantics of Calls and Returns.................................. 442
 10\.2 Implementing “Simple” Subprograms ........................................... 443
 10\.3 Implementing Subprograms with Stack\-Dynamic Local Variables ... 445
 10\.4 Nested Subprograms .................................................................... 454
 10\.5 Blocks ......................................................................................... 460
 10\.6 Implementing Dynamic Scoping .................................................... 462
 Summary • Review Questions • Problem Set • Programming Ex ercises ..... 466
 Chapter 11 Abstract Data Types and Encapsulation Constructs 473
 11\.1 The Concept of Abstraction .......................................................... 474
 11\.2 Introduction to Data Abstraction .................................................. 475
 11\.3 Design Issues for Abstract Data Types ........................................... 478
 11\.4 Language Examples ..................................................................... 479
 Interview: BJARNE STROUSTRUP—C\+\+: Its Birth, 
Its Ubiquitousness, and Common Criticisms ............................................. 480
 11\.5 Parameterized Abstract Data Types ............................................... 503
 11\.6 Encapsulation Constructs ............................................................. 509
 11\.7 Naming Encapsulations ................................................................ 513
 Summary • Review Questions • Problem Set • Programming Ex ercises ..... 517 Contents xvii
 Chapter 12 Support for Object\-Oriented Programming 523
 12\.1 Introduction ................................................................................. 524
 12\.2 Object\-Oriented Programming ...................................................... 525
 12\.3 Design Issues for Object\-Oriented Languages ................................. 529
 12\.4 Support for Object\-Oriented Programming in Smalltalk ................. 534
 Interview: BJARNE STROUSTRUP—On Paradigms and Better 
Programming ......................................................................................... 536
 12\.5 Support for Object\-Oriented Programming in C\+\+ ......................... 538
 12\.6 Support for Object\-Oriented Programming in Objective\-C .............. 549
 12\.7 Support for Object\-Oriented Programming in Java ......................... 552
 12\.8 Support for Object\-Oriented Programming in C\# ........................... 556
 12\.9 Support for Object\-Oriented Programming in Ada 95 .................... 558
 12\.10 Support for Object\-Oriented Programming in Ruby ........................ 563
 12\.11 Implementation of Object\-Oriented Constructs ............................... 566
 Summary • Review Questions • Problem Set • Programming Ex ercises .... 569
 Chapter 13 Concurrency 575
 13\.1 Introduction ................................................................................. 576
 13\.2 Introduction to Subprogram\-Level Concurrency ............................. 581
 13\.3 Semaphores ................................................................................. 586
 13\.4 Monitors ...................................................................................... 591
 13\.5 Message Passing .......................................................................... 593
 13\.6 Ada Support for Concurrency ....................................................... 594
 13\.7 Java Threads ................................................................................ 603
 13\.8 C\# Threads .................................................................................. 613
 13\.9 Concurrency in Functional Languages ........................................... 618
 13\.10 Statement\-Level Concurrency ....................................................... 621
 Summary • Bibliographic Notes • Review Questions • Problem Set • 
 Programming Exercises ........................................................................... 623xviii Contents
 Chapter 14 Exception Handling and Event Handling 629
 14\.1 Introduction to Exception Handling .............................................. 630
 History Note ..................................................................................... 634
 14\.2 Exception Handling in Ada ........................................................... 636
 14\.3 Exception Handling in C\+\+ ........................................................... 643
 14\.4 Exception Handling in Java .......................................................... 647
 14\.5 Introduction to Event Handling ..................................................... 655
 14\.6 Event Handling with Java ............................................................. 656
 14\.7 Event Handling in C\# ................................................................... 661
 Summary • Bibliographic Notes • Review Questions • Problem Set • 
Programming Exercises ........................................................................... 664
 Chapter 15 Functional Programming Languages 671
 15\.1 Introduction ................................................................................. 672
 15\.2 Mathematical Functions ............................................................... 673
 15\.3 Fundamentals of Functional Programming Languages ................... 676
 15\.4 The First Functional Programming Language: LISP ..................... 677
 15\.5 An Introduction to Scheme ........................................................... 681
 15\.6 Common LISP ............................................................................. 699
 15\.7 ML .............................................................................................. 701
 15\.8 Haskell ........................................................................................ 707
 15\.9 F\# ............................................................................................... 712
 15\.10 Support for Functional Programming in Primarily 
 Imperative Languages .................................................................. 715
 15\.11 A Comparison of Functional and Imperative Languages ................. 717
 Summary • Bibliographic Notes • Review Questions • Problem Set • 
 Programming Exercises ........................................................................... 720
 Chapter 16 Logic Programming Languages 727
 16\.1 Introduction ................................................................................. 728
 16\.2 A Brief Introduction to Predicate Calculus .................................... 728
 16\.3 Predicate Calculus and Proving Theorems ..................................... 732 Contents xix
 16\.4 An Overview of Logic Programming .............................................. 734
 16\.5 The Origins of Prolog ................................................................... 736
 16\.6 The Basic Elements of Prolog ....................................................... 736
 16\.7 Deficiencies of Prolog .................................................................. 751
 16\.8 Applications of Logic Programming .............................................. 757
 Summary • Bibliographic Notes • Review Questions • Problem Set • 
Programming Exercises ........................................................................... 758
 Bibliography ................................................................................ 763
 Index ........................................................................................... 773This page intentionally left blank 1 1\.1 Reasons for Studying Concepts of Programming Languages
 1\.2 Programming Domains
 1\.3 Language Evaluation Criteria
 1\.4 Influences on Language Design
 1\.5 Language Categories
 1\.6 Language Design Trade\-Offs
 1\.7 Implementation Methods
 1\.8 Programming Environments1
Preliminaries2 Chapter 1 Preliminaries
Before we begin discussing the concepts of programming languages, we must 
consider a few preliminaries. First, we explain some reasons why computer science students and professional software developers should study general 
concepts of language design and evaluation. This discussion is especially valu\-
able for those who believe that a working knowledge of one or two programming 
languages is sufficient for computer scientists. Then, we briefly describe the major programming domains. Next, because the book evaluates language constructs and 
features, we present a list of criteria that can serve as a basis for such judgments. 
Then, we discuss the two major influences on language design: machine architecture 
and program design methodologies. After that, we introduce the various categories 
of programming languages. Next, we describe a few of the major trade\-offs that must be considered during language design.
Because this book is also about the implementation of programming languages, 
this chapter includes an overview of the most common general approaches to imple\-mentation. Finally, we briefly describe a few examples of programming environments 
and discuss their impact on software production.
1\.1 Reasons for Studying Concepts of Programming Languages
It is natural for students to wonder how they will benefit from the study of pro\-
gramming language concepts. After all, many other topics in computer science are worthy of serious study. The following is what we believe to be a compel\-ling list of potential benefits of studying concepts of programming languages:
• Increased capacity to express ideas. It is widely believed that the depth at 
which people can think is influenced by the expressive power of the lan\-guage in which they communicate their thoughts. Those with only a weak understanding of natural language are limited in the complexity of their thoughts, particularly in depth of abstraction. In other words, it is difficult for people to conceptualize structures they cannot describe, verbally or in writing.
Programmers, in the process of developing software, are similarly con\-
strained. The language in which they develop software places limits on the kinds of control structures, data structures, and abstractions they can use; thus, the forms of algorithms they can construct are likewise limited. Awareness of a wider variety of programming language features can reduce such limitations in software development. Programmers can increase the range of their software development thought processes by learning new language constructs.
It might be argued that learning the capabilities of other languages does 
not help a programmer who is forced to use a language that lacks those capabilities. That argument does not hold up, however, because often, lan\-guage constructs can be simulated in other languages that do not support those constructs directly. For example, a C programmer who had learned the structure and uses of associative arrays in Perl (Wall et al., 2000\) might design structures that simulate associative arrays in that language. In other 1\.1 Reasons for Studying Concepts of Programming Languages 3
words, the study of programming language concepts builds an appreciation 
for valuable language features and constructs and encourages programmers to use them, even when the language they are using does not directly sup\-port such features and constructs.
• Improved background for choosing appropriate languages. Many professional 
programmers have had little formal education in computer science; rather, they have developed their programming skills independently or through in\-house training programs. Such training programs often limit instruction to one or two languages that are directly relevant to the current projects of the organization. Many other programmers received their formal training years ago. The languages they learned then are no longer used, and many features now available in programming languages were not widely known at the time. The result is that many programmers, when given a choice of languages for a new project, use the language with which they are most familiar, even if it is poorly suited for the project at hand. If these programmers were familiar with a wider range of languages and language constructs, they would be better able to choose the language with the features that best address the problem.
Some of the features of one language often can be simulated in another 
language. However, it is preferable to use a feature whose design has been integrated into a language than to use a simulation of that feature, which is often less elegant, more cumbersome, and less safe.
• Increased ability to learn new languages. Computer programming is still a rela\-
tively young discipline, and design methodologies, software development tools, and programming languages are still in a state of continuous evolu\-tion. This makes software development an exciting profession, but it also means that continuous learning is essential. The process of learning a new programming language can be lengthy and difficult, especially for someone who is comfortable with only one or two languages and has never examined programming language concepts in general. Once a thorough understanding of the fundamental concepts of languages is acquired, it becomes far easier to see how these concepts are incorporated into the design of the language being learned. For example, programmers who understand the concepts of object\-oriented programming will have a much easier time learning Java (Arnold et al., 2006\) than those who have never used those concepts.
The same phenomenon occurs in natural languages. The better you 
know the grammar of your native language, the easier it is to learn a sec\-ond language. Furthermore, learning a second language has the benefit of teaching you more about your first language.
The TIOBE Programming Community issues an index ( http://www
.tiobe.com/tiobe\_index/index.htm ) that is an indicator of the 
relative popularity of programming languages. For example, according to the index, Java, C, and C\+\+ were the three most popular languages in use in August 2011\.
1 However, dozens of other languages were widely used at 
 1\. Note that this index is only one measure of the popularity of programming languages, and 
its accuracy is not universally accepted.4 Chapter 1 Preliminaries
the time. The index data also show that the distribution of usage of pro\-
gramming languages is always changing. The number of languages in use and the dynamic nature of the statistics imply that every software developer must be prepared to learn different languages.
Finally, it is essential that practicing programmers know the vocabulary 
and fundamental concepts of programming languages so they can read and understand programming language descriptions and evaluations, as well as promotional literature for languages and compilers. These are the sources of information needed in order to choose and learn a language.
• Better understanding of the significance of implementation. In learning the con\-
cepts of programming languages, it is both interesting and necessary to touch on the implementation issues that affect those concepts. In some cases, an understanding of implementation issues leads to an understanding of why languages are designed the way they are. In turn, this knowledge leads to the ability to use a language more intelligently, as it was designed to be used. We can become better programmers by understanding the choices among programming language constructs and the consequences of those choices.
Certain kinds of program bugs can be found and fixed only by a pro\-
grammer who knows some related implementation details. Another ben\-efit of understanding implementation issues is that it allows us to visualize how a computer executes various language constructs. In some cases, some knowledge of implementation issues provides hints about the relative effi\-ciency of alternative constructs that may be chosen for a program. For example, programmers who know little about the complexity of the imple\-mentation of subprogram calls often do not realize that a small subprogram that is frequently called can be a highly inefficient design choice.
Because this book touches on only a few of the issues of implementa\-
tion, the previous two paragraphs also serve well as rationale for studying compiler design.
• Better use of languages that are already known. Many contemporary program\-
ming languages are large and complex. Accordingly, it is uncommon for a programmer to be familiar with and use all of the features of a language he or she uses. By studying the concepts of programming languages, pro\-grammers can learn about previously unknown and unused parts of the languages they already use and begin to use those features.
• Overall advancement of computing. Finally, there is a global view of comput\-
ing that can justify the study of programming language concepts. Although it is usually possible to determine why a particular programming language became popular, many believe, at least in retrospect, that the most popu\-lar languages are not always the best available. In some cases, it might be concluded that a language became widely used, at least in part, because those in positions to choose languages were not sufficiently familiar with programming language concepts.
For example, many people believe it would have been better if ALGOL 
60 (Backus et al., 1963\) had displaced Fortran (Metcalf et al., 2004\) in the 1\.2 Programming Domains 5
early 1960s, because it was more elegant and had much better control state\-
ments, among other reasons. That it did not, is due partly to the program\-mers and software development managers of that time, many of whom did not clearly understand the conceptual design of ALGOL 60\. They found its description difficult to read (which it was) and even more difficult to under\-stand. They did not appreciate the benefits of block structure, recursion, and well\-structured control statements, so they failed to see the benefits of ALGOL 60 over Fortran.
Of course, many other factors contributed to the lack of acceptance of 
ALGOL 60, as we will see in Chapter 2\. However, the fact that computer users were generally unaware of the benefits of the language played a sig\-nificant role.
In general, if those who choose languages were well informed, perhaps 
better languages would eventually squeeze out poorer ones.
1\.2 Programming Domains
Computers have been applied to a myriad of different areas, from controlling 
nuclear power plants to providing video games in mobile phones. Because of this great diversity in computer use, programming languages with very different goals have been developed. In this section, we briefly discuss a few of the areas of computer applications and their associated languages.
1\.2\.1 Scientific Applications
The first digital computers, which appeared in the late 1940s and early 1950s, were invented and used for scientific applications. T ypically, the scientific appli\-cations of that time used relatively simple data structures, but required large numbers of floating\-point arithmetic computations. The most common data structures were arrays and matrices; the most common control structures were counting loops and selections. The early high\-level programming languages invented for scientific applications were designed to provide for those needs. Their competition was assembly language, so efficiency was a primary concern. The first language for scientific applications was Fortran. ALGOL 60 and most of its descendants were also intended to be used in this area, although they were designed to be used in related areas as well. For some scientific applications where efficiency is the primary concern, such as those that were common in the 1950s and 1960s, no subsequent language is significantly better than Fortran, which explains why Fortran is still used.
1\.2\.2 Business Applications
The use of computers for business applications began in the 1950s. Special computers were developed for this purpose, along with special languages. The first successful high\-level language for business was COBOL (ISO/IEC, 2002\), 6 Chapter 1 Preliminaries
the initial version of which appeared in 1960\. It is still the most commonly 
used language for these applications. Business languages are characterized by facilities for producing elaborate reports, precise ways of describing and stor\-ing decimal numbers and character data, and the ability to specify decimal arithmetic operations.
There have been few developments in business application languages out\-
side the development and evolution of COBOL. Therefore, this book includes only limited discussions of the structures in COBOL.
1\.2\.3 Artificial Intelligence
Artificial intelligence (AI) is a broad area of computer applications charac\-terized by the use of symbolic rather than numeric computations. Symbolic computation means that symbols, consisting of names rather than numbers, are manipulated. Also, symbolic computation is more conveniently done with linked lists of data rather than arrays. This kind of programming sometimes requires more flexibility than other programming domains. For example, in some AI applications the ability to create and execute code segments during execution is convenient.
The first widely used programming language developed for AI applications 
was the functional language LISP (McCarthy et al., 1965\), which appeared in 1959\. Most AI applications developed prior to 1990 were written in LISP or one of its close relatives. During the early 1970s, however, an alternative approach to some of these applications appeared—logic programming using the Prolog (Clocksin and Mellish, 2003\) language. More recently, some AI applications have been written in systems languages such as C. Scheme (Dybvig, 2003\), a dialect of LISP , and Prolog are introduced in Chapters 15 
and 16, respectively.
1\.2\.4 Systems Programming 
The operating system and the programming support tools of a computer sys\-tem are collectively known as its systems software . Systems software is used 
almost continuously and so it must be efficient. Furthermore, it must have low\-level features that allow the software interfaces to external devices to be written.
In the 1960s and 1970s, some computer manufacturers, such as IBM, 
Digital, and Burroughs (now UNISYS), developed special machine\-oriented high\-level languages for systems software on their machines. For IBM main\-frame computers, the language was PL/S, a dialect of PL/I; for Digital, it was BLISS, a language at a level just above assembly language; for Burroughs, it was Extended ALGOL. However, most system software is now written in more general programming languages, such as C and C\+\+.
The UNIX operating system is written almost entirely in C (ISO, 1999\), 
which has made it relatively easy to port, or move, to different machines. Some of the characteristics of C make it a good choice for systems programming. It is low level, execution efficient, and does not burden the user with many 1\.3 Language Evaluation Criteria 7
safety restrictions. Systems programmers are often excellent programmers 
who believe they do not need such restrictions. Some nonsystems program\-mers, however, find C to be too dangerous to use on large, important software systems.
1\.2\.5 Web Software
The World Wide Web is supported by an eclectic collection of languages, ranging from markup languages, such as HTML, which is not a programming language, to general\-purpose programming languages, such as Java. Because of the pervasive need for dynamic Web content, some computation capability is often included in the technology of content presentation. This functionality can be provided by embedding programming code in an HTML document. Such code is often in the form of a scripting language, such as JavaScript or PHP . There are also some markup\-like languages that have been extended to include constructs that control document processing, which are discussed in Section 1\.5 and in Chapter 2\.
1\.3 Language Evaluation Criteria
As noted previously, the purpose of this book is to examine carefully the under\-
lying concepts of the various constructs and capabilities of programming lan\-guages. We will also evaluate these features, focusing on their impact on the software development process, including maintenance. T o do this, we need a set of evaluation criteria. Such a list of criteria is necessarily controversial, because it is difficult to get even two computer scientists to agree on the value of some given language characteristic relative to others. In spite of these differences, most would agree that the criteria discussed in the following subsections are important.
Some of the characteristics that influence three of the four most impor\-
tant of these criteria are shown in T able 1\.1, and the criteria themselves are discussed in the following sections.
2 Note that only the most impor\-
tant characteristics are included in the table, mirroring the discussion in the following subsections. One could probably make the case that if one considered less important characteristics, virtually all table positions could include “bullets.”
Note that some of these characteristics are broad and somewhat vague, 
such as writability, whereas others are specific language constructs, such as exception handling. Furthermore, although the discussion might seem to imply that the criteria have equal importance, that implication is not intended, and it is clearly not the case.
 2\. The fourth primary criterion is cost, which is not included in the table because it is only 
slightly related to the other criteria and the characteristics that influence them.8 Chapter 1 Preliminaries
1\.3\.1 Readability
One of the most important criteria for judging a programming language is the 
ease with which programs can be read and understood. Before 1970, software development was largely thought of in terms of writing code. The primary positive characteristic of programming languages was efficiency. Language constructs were designed more from the point of view of the computer than of the computer users. In the 1970s, however, the software life\-cycle concept (Booch, 1987\) was developed; coding was relegated to a much smaller role, and maintenance was recognized as a major part of the cycle, particularly in terms of cost. Because ease of maintenance is determined in large part by the read\-ability of programs, readability became an important measure of the quality of programs and programming languages. This was an important juncture in the evolution of programming languages. There was a distinct crossover from a focus on machine orientation to a focus on human orientation.
Readability must be considered in the context of the problem domain. For 
example, if a program that describes a computation is written in a language not designed for such use, the program may be unnatural and convoluted, making it unusually difficult to read.
The following subsections describe characteristics that contribute to the 
readability of a programming language.
1\.3\.1\.1 Overall Simplicity 
The overall simplicity of a programming language strongly affects its readabil\-ity. A language with a large number of basic constructs is more difficult to learn than one with a smaller number. Programmers who must use a large language often learn a subset of the language and ignore its other features. This learning pattern is sometimes used to excuse the large number of language constructs, Table 1\.1 
Language evaluation criteria and the characteristics that affect them
CRITERIA
Characteristic READABILITY WRITABILITY RELIABILITY
Simplicity • • •
Orthogonality • • •
Data types • • •
Syntax design • • •Support for abstraction • •Expressivity • •
T ype checking •
Exception handling •
Restricted aliasing •1\.3 Language Evaluation Criteria 9
but that argument is not valid. Readability problems occur whenever the pro\-
gram’s author has learned a different subset from that subset with which the reader is familiar.
A second complicating characteristic of a programming language is feature 
multiplicity —that is, having more than one way to accomplish a particular 
operation. For example, in Java, a user can increment a simple integer variable in four different ways:
count \= count \+ 1
count \+\= 1count\+\+\+\+count
Although the last two statements have slightly different meanings from each 
other and from the others in some contexts, all of them have the same mean\-ing when used as stand\-alone expressions. These variations are discussed in Chapter 7\.
A third potential problem is operator overloading , in which a single oper\-
ator symbol has more than one meaning. Although this is often useful, it can lead to reduced readability if users are allowed to create their own overloading and do not do it sensibly. For example, it is clearly acceptable to overload \+ to use it for both integer and floating\-point addition. In fact, this overloading simplifies a language by reducing the number of operators. However, suppose the programmer defined \+ used between single\-dimensioned array operands to mean the sum of all elements of both arrays. Because the usual meaning of vector addition is quite different from this, it would make the program more confusing for both the author and the program’s readers. An even more extreme example of program confusion would be a user defining \+ between two vector operands to mean the difference between their respective first elements. Opera\-tor overloading is further discussed in Chapter 7\.
Simplicity in languages can, of course, be carried too far. For example, 
the form and meaning of most assembly language statements are models of simplicity, as you can see when you consider the statements that appear in the next section. This very simplicity, however, makes assembly language programs less readable. Because they lack more complex control statements, program structure is less obvious; because the statements are simple, far more of them are required than in equivalent programs in a high\-level language. These same arguments apply to the less extreme case of high\-level languages with inad\-equate control and data\-structuring constructs.
1\.3\.1\.2 Orthogonality 
Orthogonality in a programming language means that a relatively small set of 
primitive constructs can be combined in a relatively small number of ways to build the control and data structures of the language. Furthermore, every pos\-sible combination of primitives is legal and meaningful. For example, consider 10 Chapter 1 Preliminaries
data types. Suppose a language has four primitive data types (integer, float, 
double, and character) and two type operators (array and pointer). If the two type operators can be applied to themselves and the four primitive data types, a large number of data structures can be defined.
The meaning of an orthogonal language feature is independent of the 
context of its appearance in a program. (the word orthogonal comes from the 
mathematical concept of orthogonal vectors, which are independent of each other.) Orthogonality follows from a symmetry of relationships among primi\-tives. A lack of orthogonality leads to exceptions to the rules of the language. For example, in a programming language that supports pointers, it should be possible to define a pointer to point to any specific type defined in the language. However, if pointers are not allowed to point to arrays, many potentially useful user\-defined data structures cannot be defined.
We can illustrate the use of orthogonality as a design concept by compar\-
ing one aspect of the assembly languages of the IBM mainframe computers and the VAX series of minicomputers. We consider a single simple situation: adding two 32\-bit integer values that reside in either memory or registers and replacing one of the two values with the sum. The IBM mainframes have two instructions for this purpose, which have the forms
A Reg1, memory\_cell
AR Reg1, Reg2
where Reg1 and Reg2 represent registers. The semantics of these are
Reg1 ← contents(Reg1\) \+ contents(memory\_cell)
Reg1 ← contents(Reg1\) \+ contents(Reg2\)
The VAX addition instruction for 32\-bit integer values is
ADDL operand\_1, operand\_2
whose semantics is
operand\_2 ← contents(operand\_1\) \+ contents(operand\_2\)
In this case, either operand can be a register or a memory cell.
The VAX instruction design is orthogonal in that a single instruction can 
use either registers or memory cells as the operands. There are two ways to specify operands, which can be combined in all possible ways. The IBM design is not orthogonal. Only two out of four operand combinations possibilities are legal, and the two require different instructions, 
A and AR. The IBM design 
is more restricted and therefore less writable. For example, you cannot add two values and store the sum in a memory location. Furthermore, the IBM design is more difficult to learn because of the restrictions and the additional instruction.1\.3 Language Evaluation Criteria 11
Orthogonality is closely related to simplicity: The more orthogonal the 
design of a language, the fewer exceptions the language rules require. Fewer exceptions mean a higher degree of regularity in the design, which makes the language easier to learn, read, and understand. Anyone who has learned a sig\-nificant part of the English language can testify to the difficulty of learning its many rule exceptions (for example, i before e except after c).
As examples of the lack of orthogonality in a high\-level language, consider 
the following rules and exceptions in C. Although C has two kinds of struc\-tured data types, arrays and records ( struct s), records can be returned from 
functions but arrays cannot. A member of a structure can be any data type except void or a structure of the same type. An array element can be any data 
type except void or a function. Parameters are passed by value, unless they 
are arrays, in which case they are, in effect, passed by reference (because the appearance of an array name without a subscript in a C program is interpreted to be the address of the array’s first element).
As an example of context dependence, consider the C expression
a \+ b
This expression often means that the values of a and b are fetched and added 
together. However, if a happens to be a pointer, it affects the value of b. For 
example, if a points to a float value that occupies four bytes, then the value of b 
must be scaled—in this case multiplied by 4—before it is added to a. Therefore, 
the type of a affects the treatment of the value of b. The context of b affects 
its meaning.
T oo much orthogonality can also cause problems. Perhaps the most 
orthogonal programming language is ALGOL 68 (van Wijngaarden et al., 1969\). Every language construct in ALGOL 68 has a type, and there are no restrictions on those types. In addition, most constructs produce values. This combinational freedom allows extremely complex constructs. For example, a conditional can appear as the left side of an assignment, along with declarations and other assorted statements, as long as the result is an address. This extreme form of orthogonality leads to unnecessary complexity. Furthermore, because languages require a large number of primitives, a high degree of orthogonality results in an explosion of combinations. So, even if the combinations are simple, their sheer numbers lead to complexity.
Simplicity in a language, therefore, is at least in part the result of a com\-
bination of a relatively small number of primitive constructs and a limited use of the concept of orthogonality.
Some believe that functional languages offer a good combination of sim\-
plicity and orthogonality. A functional language, such as LISP , is one in which computations are made primarily by applying functions to given parameters. In contrast, in imperative languages such as C, C\+\+, and Java, computations are usually specified with variables and assignment statements. Functional languages offer potentially the greatest overall simplicity, because they can accomplish everything with a single construct, the function call, which can be 12 Chapter 1 Preliminaries
combined simply with other function calls. This simple elegance is the reason 
why some language researchers are attracted to functional languages as the primary alternative to complex nonfunctional languages such as C\+\+. Other factors, such as efficiency, however, have prevented functional languages from becoming more widely used.
1\.3\.1\.3 Data Types
The presence of adequate facilities for defining data types and data structures in a language is another significant aid to readability. For example, suppose a numeric type is used for an indicator flag because there is no Boolean type in the language. In such a language, we might have an assignment such as the following:
timeOut \= 1
The meaning of this statement is unclear, whereas in a language that includes Boolean types, we would have the following:
timeOut \= true
The meaning of this statement is perfectly clear.
1\.3\.1\.4 Syntax Design
The syntax, or form, of the elements of a language has a significant effect on 
the readability of programs. Following are some examples of syntactic design choices that affect readability:
• Special words. Program appearance and thus program readability are strongly 
influenced by the forms of a language’s special words (for example, 
while , 
class , and for). Especially important is the method of forming compound 
statements, or statement groups, primarily in control constructs. Some lan\-guages have used matching pairs of special words or symbols to form groups. C and its descendants use braces to specify compound statements. All of these languages suffer because statement groups are always terminated in the same way, which makes it difficult to determine which group is being ended when an 
end or a right brace appears. Fortran 95 and Ada make this clearer 
by using a distinct closing syntax for each type of statement group. For example, Ada uses 
end if to terminate a selection construct and end loop 
to terminate a loop construct. This is an example of the conflict between simplicity that results in fewer reserved words, as in C\+\+, and the greater readability that can result from using more reserved words, as in Ada.
Another important issue is whether the special words of a language can 
be used as names for program variables. If so, the resulting programs can be very confusing. For example, in Fortran 95, special words, such as 
Do 
and End, are legal variable names, so the appearance of these words in a 
program may or may not connote something special.1\.3 Language Evaluation Criteria 13
• Form and meaning. Designing statements so that their appearance at least 
partially indicates their purpose is an obvious aid to readability. Semantics, 
or meaning, should follow directly from syntax, or form. In some cases, this 
principle is violated by two language constructs that are identical or similar 
in appearance but have different meanings, depending perhaps on context. In 
C, for example, the meaning of the reserved word static depends on the 
context of its appearance. If used on the definition of a variable inside a func\-
tion, it means the variable is created at compile time. If used on the definition 
of a variable that is outside all functions, it means the variable is visible only in 
the file in which its definition appears; that is, it is not exported from that file.
One of the primary complaints about the shell commands of UNIX 
(Raymond, 2004\) is that their appearance does not always suggest their 
function. For example, the meaning of the UNIX command grep can be 
deciphered only through prior knowledge, or perhaps cleverness and famil\-
iarity with the UNIX editor, ed. The appearance of grep connotes nothing 
to UNIX beginners. (In ed, the command /regular\_expression / searches for a 
substring that matches the regular expression. Preceding this with g makes 
it a global command, specifying that the scope of the search is the whole file being edited. Following the command with 
p specifies that lines with 
the matching substring are to be printed. So g/regular\_expression /p, which 
can obviously be abbreviated as grep , prints all lines in a file that contain 
substrings that match the regular expression.)
1\.3\.2 Writability
Writability is a measure of how easily a language can be used to create programs 
for a chosen problem domain. Most of the language characteristics that affect 
readability also affect writability. This follows directly from the fact that the process of writing a program requires the programmer frequently to reread the 
part of the program that is already written.
As is the case with readability, writability must be considered in the con\-
text of the target problem domain of a language. It is simply not reasonable to 
compare the writability of two languages in the realm of a particular application 
when one was designed for that application and the other was not. For example, 
the writabilities of Visual BASIC (VB) and C are dramatically different for 
creating a program that has a graphical user interface, for which VB is ideal. 
Their writabilities are also quite different for writing systems programs, such 
as an operation system, for which C was designed.
The following subsections describe the most important characteristics 
influencing the writability of a language.
1\.3\.2\.1 Simplicity and Orthogonality 
If a language has a large number of different constructs, some programmers 
might not be familiar with all of them. This situation can lead to a misuse of 
some features and a disuse of others that may be either more elegant or more 14 Chapter 1 Preliminaries
efficient, or both, than those that are used. It may even be possible, as noted 
by Hoare (1973\), to use unknown features accidentally, with bizarre results. Therefore, a smaller number of primitive constructs and a consistent set of rules for combining them (that is, orthogonality) is much better than simply having a large number of primitives. A programmer can design a solution to a complex problem after learning only a simple set of primitive constructs.
On the other hand, too much orthogonality can be a detriment to writ\-
ability. Errors in programs can go undetected when nearly any combination of primitives is legal. This can lead to code absurdities that cannot be discovered by the compiler.
1\.3\.2\.2 Support for Abstraction
Briefly, abstraction means the ability to define and then use complicated 
structures or operations in ways that allow many of the details to be ignored. Abstraction is a key concept in contemporary programming language design. This is a reflection of the central role that abstraction plays in modern pro\-gram design methodologies. The degree of abstraction allowed by a program\-ming language and the naturalness of its expression are therefore important to its writability. Programming languages can support two distinct categories of abstraction, process and data.
A simple example of process abstraction is the use of a subprogram to 
implement a sort algorithm that is required several times in a program. With\-out the subprogram, the sort code would need to be replicated in all places where it was needed, which would make the program much longer and more tedious to write. Perhaps more important, if the subprogram were not used, the code that used the sort subprogram would be cluttered with the sort algorithm details, greatly obscuring the flow and overall intent of that code.
As an example of data abstraction, consider a binary tree that stores integer 
data in its nodes. Such a binary tree would usually be implemented in a language that does not support pointers and dynamic storage management with a heap, such as Fortran 77, as three parallel integer arrays, where two of the integers are used as subscripts to specify offspring nodes. In C\+\+ and Java, these trees can be implemented by using an abstraction of a tree node in the form of a simple class with two pointers (or references) and an integer. The naturalness of the latter representation makes it much easier to write a program that uses binary trees in these languages than to write one in Fortran 77\. It is a simple matter of the problem solution domain of the language being closer to the problem domain.
The overall support for abstraction is clearly an important factor in the 
writability of a language.
1\.3\.2\.3 Expressivity
Expressivity in a language can refer to several different characteristics. In a language such as APL (Gilman and Rose, 1976\), it means that there are very powerful operators that allow a great deal of computation to be accomplished 1\.3 Language Evaluation Criteria 15
with a very small program. More commonly, it means that a language has 
relatively convenient, rather than cumbersome, ways of specifying computa\-tions. For example, in C, the notation 
count\+\+ is more convenient and shorter 
than count \= count \+ 1 . Also, the and then Boolean operator in Ada is a 
convenient way of specifying short\-circuit evaluation of a Boolean expression. The inclusion of the 
for statement in Java makes writing counting loops easier 
than with the use of while , which is also possible. All of these increase the 
writability of a language.
1\.3\.3 Reliability
A program is said to be reliable if it performs to its specifications under all conditions. The following subsections describe several language fea\-tures that have a significant effect on the reliability of programs in a given language.
1\.3\.3\.1 Type Checking
Type checking is simply testing for type errors in a given program, either 
by the compiler or during program execution. T ype checking is an impor\-tant factor in language reliability. Because run\-time type checking is expen\-sive, compile\-time type checking is more desirable. Furthermore, the earlier errors in programs are detected, the less expensive it is to make the required repairs. The design of Java requires checks of the types of nearly all variables and expressions at compile time. This virtually eliminates type errors at run time in Java programs. T ypes and type checking are discussed in depth in Chapter 6\.
One example of how failure to type check, at either compile time or run 
time, has led to countless program errors is the use of subprogram parameters in the original C language (Kernighan and Ritchie, 1978\). In this language, the type of an actual parameter in a function call was not checked to determine whether its type matched that of the corresponding formal parameter in the function. An 
int type variable could be used as an actual parameter in a call to 
a function that expected a float type as its formal parameter, and neither the 
compiler nor the run\-time system would detect the inconsistency. For example, because the bit string that represents the integer 23 is essentially unrelated to the bit string that represents a floating\-point 23, if an integer 23 is sent to a function that expects a floating\-point parameter, any uses of the parameter in the function will produce nonsense. Furthermore, such problems are often difficult to diagnose.
3 The current version of C has eliminated this problem 
by requiring all parameters to be type checked. Subprograms and parameter\-passing techniques are discussed in Chapter 9\.
 3\. In response to this and other similar problems, UNIX systems include a utility program 
named lint that checks C programs for such problems.16 Chapter 1 Preliminaries
1\.3\.3\.2 Exception Handling
The ability of a program to intercept run\-time errors (as well as other unusual 
conditions detectable by the program), take corrective measures, and then continue is an obvious aid to reliability. This language facility is called excep\-
tion handling . Ada, C\+\+, Java, and C\# include extensive capabilities for 
exception handling, but such facilities are practically nonexistent in many widely used languages, including C and Fortran. Exception handling is dis\-cussed in Chapter 14\.
1\.3\.3\.3 Aliasing
Loosely defined, aliasing is having two or more distinct names that can be 
used to access the same memory cell. It is now widely accepted that aliasing is a dangerous feature in a programming language. Most programming lan\-guages allow some kind of aliasing—for example, two pointers set to point to the same variable, which is possible in most languages. In such a program, the programmer must always remember that changing the value pointed to by one of the two changes the value referenced by the other. Some kinds of aliasing, as described in Chapters 5 and 9 can be prohibited by the design of a language.
In some languages, aliasing is used to overcome deficiencies in the lan\-
guage’s data abstraction facilities. Other languages greatly restrict aliasing to increase their reliability.
1\.3\.3\.4 Readability and Writability
Both readability and writability influence reliability. A program written in a language that does not support natural ways to express the required algorithms will necessarily use unnatural approaches. Unnatural approaches are less likely to be correct for all possible situations. The easier a program is to write, the more likely it is to be correct.
Readability affects reliability in both the writing and maintenance phases 
of the life cycle. Programs that are difficult to read are difficult both to write and to modify.
1\.3\.4 Cost
The total cost of a programming language is a function of many of its characteristics.
First, there is the cost of training programmers to use the language, which 
is a function of the simplicity and orthogonality of the language and the experi\-ence of the programmers. Although more powerful languages are not neces\-sarily more difficult to learn, they often are.
Second, there is the cost of writing programs in the language. This is a 
function of the writability of the language, which depends in part on its close\-ness in purpose to the particular application. The original efforts to design and 1\.3 Language Evaluation Criteria 17
implement high\-level languages were driven by the desire to lower the costs 
of creating software.
Both the cost of training programmers and the cost of writing programs in 
a language can be significantly reduced in a good programming environment. Programming environments are discussed in Section 1\.8\.
Third, there is the cost of compiling programs in the language. A major 
impediment to the early use of Ada was the prohibitively high cost of run\-ning the first\-generation Ada compilers. This problem was diminished by the appearance of improved Ada compilers.
Fourth, the cost of executing programs written in a language is greatly 
influenced by that language’s design. A language that requires many run\-time type checks will prohibit fast code execution, regardless of the quality of the compiler. Although execution efficiency was the foremost concern in the design of early languages, it is now considered to be less important.
A simple trade\-off can be made between compilation cost and execution 
speed of the compiled code. Optimization is the name given to the collection of 
techniques that compilers may use to decrease the size and/or increase the execu\-tion speed of the code they produce. If little or no optimization is done, com\-pilation can be done much faster than if a significant effort is made to produce optimized code. The choice between the two alternatives is influenced by the environment in which the compiler will be used. In a laboratory for beginning programming students, who often compile their programs many times during development but use little code at execution time (their programs are small and they must execute correctly only once), little or no optimization should be done. In a production environment, where compiled programs are executed many times after development, it is better to pay the extra cost to optimize the code.
The fifth factor in the cost of a language is the cost of the language imple\-
mentation system. One of the factors that explains the rapid acceptance of Java is that free compiler/interpreter systems became available for it soon after its design was released. A language whose implementation system is either expensive or runs only on expensive hardware will have a much smaller chance of becoming widely used. For example, the high cost of first\-generation Ada compilers helped prevent Ada from becoming popular in its early days.
Sixth, there is the cost of poor reliability. If the software fails in a critical sys\-
tem, such as a nuclear power plant or an X\-ray machine for medical use, the cost could be very high. The failures of noncritical systems can also be very expensive in terms of lost future business or lawsuits over defective software systems.
The final consideration is the cost of maintaining programs, which includes 
both corrections and modifications to add new functionality. The cost of software maintenance depends on a number of language characteristics, primarily read\-ability. Because maintenance is often done by individuals other than the original author of the software, poor readability can make the task extremely challenging.
The importance of software maintainability cannot be overstated. It has 
been estimated that for large software systems with relatively long lifetimes, maintenance costs can be as high as two to four times as much as development costs (Sommerville, 2005\).18 Chapter 1 Preliminaries
Of all the contributors to language costs, three are most important: program 
development, maintenance, and reliability. Because these are functions of writabil\-ity and readability, these two evaluation criteria are, in turn, the most important.
Of course, a number of other criteria could be used for evaluating program\-
ming languages. One example is portability , or the ease with which programs 
can be moved from one implementation to another. Portability is most strongly influenced by the degree of standardization of the language. Some languages, such as BASIC, are not standardized at all, making programs in these languages very difficult to move from one implementation to another. Standardization is a time\-consuming and difficult process. A committee began work on producing a standard version of C\+\+ in 1989\. It was approved in 1998\.
Generality (the applicability to a wide range of applications) and well\-
definedness (the completeness and precision of the language’s official defining 
document) are two other criteria.
Most criteria, particularly readability, writability, and reliability, are neither 
precisely defined nor exactly measurable. Nevertheless, they are useful concepts and they provide valuable insight into the design and evaluation of program\-ming languages.
A final note on evaluation criteria: language design criteria are weighed 
differently from different perspectives. Language implementors are concerned primarily with the difficulty of implementing the constructs and features of the language. Language users are worried about writability first and readability later. Language designers are likely to emphasize elegance and the ability to attract widespread use. These characteristics often conflict with one another.
1\.4 Influences on Language Design
In addition to those factors described in Section 1\.3, several other factors influ\-
ence the basic design of programming languages. The most important of these are computer architecture and programming design methodologies.
1\.4\.1 Computer Architecture
The basic architecture of computers has had a profound effect on language design. Most of the popular languages of the past 50 years have been designed around the prevalent computer architecture, called the von Neumann archi\-
tecture , after one of its originators, John von Neumann (pronounced “von 
Noyman”). These languages are called imperative languages. In a von Neu\-
mann computer, both data and programs are stored in the same memory. The central processing unit (CPU), which executes instructions, is separate from the memory. Therefore, instructions and data must be transmitted, or piped, from memory to the CPU. Results of operations in the CPU must be moved back to memory. Nearly all digital computers built since the 1940s have been based on the von Neumann architecture. The overall structure of a von Neumann computer is shown in Figure 1\.1\.1\.4 Influences on Language Design 19
Because of the von Neumann architecture, the central features of impera\-
tive languages are variables, which model the memory cells; assignment state\-ments, which are based on the piping operation; and the iterative form of repetition, which is the most efficient way to implement repetition on this architecture. Operands in expressions are piped from memory to the CPU, and the result of evaluating the expression is piped back to the memory cell represented by the left side of the assignment. Iteration is fast on von Neumann computers because instructions are stored in adjacent cells of memory and repeating the execution of a section of code requires only a branch instruction. This efficiency discourages the use of recursion for repetition, although recur\-sion is sometimes more natural.
The execution of a machine code program on a von Neumann architecture 
computer occurs in a process called the fetch\-execute cycle . As stated earlier, 
programs reside in memory but are executed in the CPU. Each instruction to be executed must be moved from memory to the processor. The address of the next instruction to be executed is maintained in a register called the program 
counter . The fetch\-execute cycle can be simply described by the following 
algorithm:
initialize the program counter
repeat forever
 fetch the instruction pointed to by the program counter increment the program counter to point at the next instruction decode the instruction execute the instruction
end repeatArithmetic and 
logic unitControl
unitMemory (stores both instructions and data)
Instructions and data
Input and output devicesResults of
operations
Central processing unitFigure 1\.1
The von Neumann 
computer architecture20 Chapter 1 Preliminaries
The “decode the instruction” step in the algorithm means the instruction is 
examined to determine what action it specifies. Program execution terminates when a stop instruction is encountered, although on an actual computer a stop instruction is rarely executed. Rather, control transfers from the operating sys\-tem to a user program for its execution and then back to the operating system when the user program execution is complete. In a computer system in which more than one user program may be in memory at a given time, this process is far more complex.
As stated earlier, a functional, or applicative, language is one in which 
the primary means of computation is applying functions to given parameters. Programming can be done in a functional language without the kind of vari\-ables that are used in imperative languages, without assignment statements, and without iteration. Although many computer scientists have expounded on the myriad benefits of functional languages, such as Scheme, it is unlikely that they will displace the imperative languages until a non–von Neumann computer is designed that allows efficient execution of programs in functional languages. Among those who have bemoaned this fact, the most eloquent is John Backus (1978\), the principal designer of the original version of Fortran.
In spite of the fact that the structure of imperative programming languages 
is modeled on a machine architecture, rather than on the abilities and inclina\-tions of the users of programming languages, some believe that using imperative languages is somehow more natural than using a functional language. So, these people believe that even if functional programs were as efficient as imperative programs, the use of imperative programming languages would still dominate.
1\.4\.2 Programming Design Methodologies
The late 1960s and early 1970s brought an intense analysis, begun in large part by the structured\-programming movement, of both the software development process and programming language design.
An important reason for this research was the shift in the major cost of 
computing from hardware to software, as hardware costs decreased and pro\-grammer costs increased. Increases in programmer productivity were relatively small. In addition, progressively larger and more complex problems were being solved by computers. Rather than simply solving sets of equations to simulate satellite tracks, as in the early 1960s, programs were being written for large and complex tasks, such as controlling large petroleum\-refining facilities and providing worldwide airline reservation systems.
The new software development methodologies that emerged as a result 
of the research of the 1970s were called top\-down design and stepwise refine\-ment. The primary programming language deficiencies that were discovered were incompleteness of type checking and inadequacy of control statements (requiring the extensive use of gotos).
In the late 1970s, a shift from procedure\-oriented to data\-oriented pro\-
gram design methodologies began. Simply put, data\-oriented methods empha\-size data design, focusing on the use of abstract data types to solve problems.1\.5 Language Categories 21
For data abstraction to be used effectively in software system design, it 
must be supported by the languages used for implementation. The first lan\-guage to provide even limited support for data abstraction was SIMULA 67 (Birtwistle et al., 1973\), although that language certainly was not propelled to popularity because of it. The benefits of data abstraction were not widely recognized until the early 1970s. However, most languages designed since the late 1970s support data abstraction, which is discussed in detail in Chapter 11\.
The latest step in the evolution of data\-oriented software development, 
which began in the early 1980s, is object\-oriented design. Object\-oriented methodology begins with data abstraction, which encapsulates processing with data objects and controls access to data, and adds inheritance and dynamic method binding. Inheritance is a powerful concept that greatly enhances the potential reuse of existing software, thereby providing the possibility of signifi\-cant increases in software development productivity. This is an important factor in the increase in popularity of object\-oriented languages. Dynamic (run\-time) method binding allows more flexible use of inheritance.
Object\-oriented programming developed along with a language that 
supported its concepts: Smalltalk (Goldberg and Robson, 1989\). Although Smalltalk never became as widely used as many other languages, support for object\-oriented programming is now part of most popular imperative lan\-guages, including Ada 95 (ARM, 1995\), Java, C\+\+, and C\#. Object\-oriented concepts have also found their way into functional programming in CLOS (Bobrow et al., 1988\) and F\# (Syme, et al., 2010\), as well as logic programming in Prolog\+\+ (Moss, 1994\). Language support for object\-oriented programming is discussed in detail in Chapter 12\.
Procedure\-oriented programming is, in a sense, the opposite of data\-
oriented programming. Although data\-oriented methods now dominate soft\-ware development, procedure\-oriented methods have not been abandoned. On the contrary, in recent years, a good deal of research has occurred in procedure\-oriented programming, especially in the area of concurrency. These research efforts brought with them the need for language facilities for creating and controlling concurrent program units. Ada, Java, and C\# include such capabilities. Concurrency is discussed in detail in Chapter 13\.
All of these evolutionary steps in software development methodologies led 
to new language constructs to support them.
1\.5 Language Categories
Programming languages are often categorized into four bins: imperative, 
functional, logic, and object oriented. However, we do not consider languages that support object\-oriented programming to form a separate category of languages. We have described how the most popular languages that support object\-oriented programming grew out of imperative languages. Although the object\-oriented software development paradigm differs significantly from the procedure\-oriented paradigm usually used with imperative languages, the 22 Chapter 1 Preliminaries
extensions to an imperative language required to support object\-oriented pro\-
gramming are not intensive. For example, the expressions, assignment state\-ments, and control statements of C and Java are nearly identical. (On the other hand, the arrays, subprograms, and semantics of Java are very different from those of C.) Similar statements can be made for functional languages that sup\-port object\-oriented programming.
Another kind of language, the visual language, is a subcategory of the impera\-
tive languages. The most popular visual languages are the .NET languages. These languages (or their implementations) include capabilities for drag\-and\-drop gen\-eration of code segments. Such languages were once called fourth\-generation languages, although that name has fallen out of use. The visual languages provide a simple way to generate graphical user interfaces to programs. For example, using Visual Studio to develop software in the .NET languages, the code to produce a display of a form control, such as a button or text box, can be created with a single keystroke. These capabilities are now available in all of the .NET languages.
Some authors refer to scripting languages as a separate category of pro\-
gramming languages. However, languages in this category are bound together more by their implementation method, partial or full interpretation, than by a common language design. The languages that are typically called scripting languages, among them Perl, JavaScript, and Ruby, are imperative languages in every sense.
A logic programming language is an example of a rule\-based language. 
In an imperative language, an algorithm is specified in great detail, and the specific order of execution of the instructions or statements must be included. In a rule\-based language, however, rules are specified in no particular order, and the language implementation system must choose an order in which the rules are used to produce the desired result. This approach to software devel\-opment is radically different from those used with the other two categories of languages and clearly requires a completely different kind of language. Prolog, the most commonly used logic programming language, and logic programming are discussed in Chapter 16\.
In recent years, a new category of languages has emerged, the markup/
programming hybrid languages. Markup languages are not programming languages. For instance, HTML, the most widely used markup language, is used to specify the layout of information in Web documents. However, some programming capability has crept into some extensions to HTML and XML. Among these are the Java Server Pages Standard T ag Library ( JSTL) and eXtensible Stylesheet Language T ransformations (XSL T). Both of these are briefly introduced in Chapter 2\.Those languages cannot be compared to any of the complete programming languages and therefore will not be discussed after Chapter 2\.
A host of special\-purpose languages have appeared over the past 50 years. 
These range from Report Program Generator (RPG), which is used to produce business reports; to Automatically Programmed T ools (APT), which is used for instructing programmable machine tools; to General Purpose Simulation Sys\-tem (GPSS), which is used for systems simulation. This book does not discuss 1\.7 Implementation Methods 23
special\-purpose languages, primarily because of their narrow applicability and 
the difficulty of comparing them with other languages.
1\.6 Language Design Trade\-Offs
The programming language evaluation criteria described in Section 1\.3 
provide a framework for language design. Unfortunately, that framework is self\-contradictory. In his insightful paper on language design, Hoare (1973\) stated that “there are so many important but conflicting criteria, that their reconciliation and satisfaction is a major engineering task.”
T wo criteria that conflict are reliability and cost of execution. For example, the 
Java language definition demands that all references to array elements be checked to ensure that the index or indices are in their legal ranges. This step adds a great deal to the cost of execution of Java programs that contain large numbers of refer\-ences to array elements. C does not require index range checking, so C programs execute faster than semantically equivalent Java programs, although Java programs are more reliable. The designers of Java traded execution efficiency for reliability.
As another example of conflicting criteria that leads directly to design 
trade\-offs, consider the case of APL. APL includes a powerful set of operators for array operands. Because of the large number of operators, a significant number of new symbols had to be included in APL to represent the operators. Also, many APL operators can be used in a single, long, complex expression. One result of this high degree of expressivity is that, for applications involv\-ing many array operations, APL is very writable. Indeed, a huge amount of computation can be specified in a very small program. Another result is that APL programs have very poor readability. A compact and concise expression has a certain mathematical beauty but it is difficult for anyone other than the programmer to understand. Well\-known author Daniel McCracken (1970\) once noted that it took him four hours to read and understand a four\-line APL program. The designer of APL traded readability for writability.
The conflict between writability and reliability is a common one in lan\-
guage design. The pointers of C\+\+ can be manipulated in a variety of ways, which supports highly flexible addressing of data. Because of the potential reli\-ability problems with pointers, they are not included in Java.
Examples of conflicts among language design (and evaluation) criteria 
abound; some are subtle, others are obvious. It is therefore clear that the task of choosing constructs and features when designing a programming language requires many compromises and trade\-offs.
1\.7 Implementation Methods
As described in Section 1\.4\.1, two of the primary components of a computer 
are its internal memory and its processor. The internal memory is used to store programs and data. The processor is a collection of circuits that provides 24 Chapter 1 Preliminaries
a realization of a set of primitive operations, or machine instructions, such as 
those for arithmetic and logic operations. In most computers, some of these instructions, which are sometimes called macroinstructions, are actually imple\-mented with a set of instructions called microinstructions, which are defined at an even lower level. Because microinstructions are never seen by software, they will not be discussed further here.
The machine language of the computer is its set of instructions. In the 
absence of other supporting software, its own machine language is the only language that most hardware computers “understand.” Theoretically, a com\-puter could be designed and built with a particular high\-level language as its machine language, but it would be very complex and expensive. Furthermore, it would be highly inflexible, because it would be difficult (but not impossible) to use it with other high\-level languages. The more practical machine design choice implements in hardware a very low\-level language that provides the most commonly needed primitive operations and requires system software to create an interface to programs in higher\-level languages.
A language implementation system cannot be the only software on a com\-
puter. Also required is a large collection of programs, called the operating sys\-tem, which supplies higher\-level primitives than those of the machine language. These primitives provide system resource management, input and output oper\-ations, a file management system, text and/or program editors, and a variety of other commonly needed functions. Because language implementation systems need many of the operating system facilities, they interface with the operating system rather than directly with the processor (in machine language).
The operating system and language implementations are layered over the 
machine language interface of a computer. These layers can be thought of as virtual computers, providing interfaces to the user at higher levels. For exam\-ple, an operating system and a C compiler provide a virtual C computer. With other compilers, a machine can become other kinds of virtual computers. Most computer systems provide several different virtual computers. User programs form another layer over the top of the layer of virtual computers. The layered view of a computer is shown in Figure 1\.2\.
The implementation systems of the first high\-level programming lan\-
guages, constructed in the late 1950s, were among the most complex software systems of that time. In the 1960s, intensive research efforts were made to understand and formalize the process of constructing these high\-level language implementations. The greatest success of those efforts was in the area of syn\-tax analysis, primarily because that part of the implementation process is an application of parts of automata theory and formal language theory that were then well understood.
1\.7\.1 Compilation
Programming languages can be implemented by any of three general methods. At one extreme, programs can be translated into machine language, which can be executed directly on the computer. This method is called a compiler 1\.7 Implementation Methods 25
implementation and has the advantage of very fast program execution, once 
the translation process is complete. Most production implementations of lan\-guages, such as C, COBOL, C\+\+, and Ada, are by compilers.
The language that a compiler translates is called the source language . The 
process of compilation and program execution takes place in several phases, the most important of which are shown in Figure 1\.3\.
The lexical analyzer gathers the characters of the source program into lexi\-
cal units. The lexical units of a program are identifiers, special words, operators, and punctuation symbols. The lexical analyzer ignores comments in the source program because the compiler has no use for them.
The syntax analyzer takes the lexical units from the lexical analyzer and uses 
them to construct hierarchical structures called parse trees . These parse trees 
represent the syntactic structure of the program. In many cases, no actual parse tree structure is constructed; rather, the information that would be required to build a tree is generated and used directly. Both lexical units and parse trees are further discussed in Chapter 3\. Lexical analysis and syntax analysis, or parsing, are discussed in Chapter 4\.OperatingsystemcommandinterpreterScheme
interpreter
C
compilerVirtual
Ccomputer
Virtual
AdacomputerAda
compiler
. . .. . .
Assembler
Virtual
assemblylanguagecomputerJava Virtual MachineJava 
compiler.NETcommo n
language
run timeVB.NET
compilerC\#
compilerVirtualVB .NETcomputerVirtual C\# 
computer
Bare 
machineMacroinstruction
interpreterOperating system
Virtual Java
computerVirtual Scheme computerFigure 1\.2
Layered interface of 
virtual computers, provided by a typical computer system26 Chapter 1 Preliminaries
Source
program
Lexical
analyzer
Syntax
analyzer
Intermediate
code generator
and semantic
analyzerOptimization
(optional)Symbol
table
Code
generator
Computer
ResultsInput dataMachine
languageIntermediate
codeParse treesLexical unitsFigure 1\.3
The compilation process
The intermediate code generator produces a program in a different lan\-
guage, at an intermediate level between the source program and the final out\-put of the compiler: the machine language program.
4 Intermediate languages 
sometimes look very much like assembly languages, and in fact, sometimes are actual assembly languages. In other cases, the intermediate code is at a level 
 4\. Note that the words program and code are often used interchangeably.1\.7 Implementation Methods 27
somewhat higher than an assembly language. The semantic analyzer is an inte\-
gral part of the intermediate code generator. The semantic analyzer checks for errors, such as type errors, that are difficult, if not impossible, to detect during syntax analysis.
Optimization, which improves programs (usually in their intermediate 
code version) by making them smaller or faster or both, is often an optional part of compilation. In fact, some compilers are incapable of doing any significant optimization. This type of compiler would be used in situations where execu\-tion speed of the translated program is far less important than compilation speed. An example of such a situation is a computing laboratory for beginning programmers. In most commercial and industrial situations, execution speed is more important than compilation speed, so optimization is routinely desirable. Because many kinds of optimization are difficult to do on machine language, most optimization is done on the intermediate code.
The code generator translates the optimized intermediate code version of 
the program into an equivalent machine language program.
The symbol table serves as a database for the compilation process. The 
primary contents of the symbol table are the type and attribute information of each user\-defined name in the program. This information is placed in the symbol table by the lexical and syntax analyzers and is used by the semantic analyzer and the code generator.
As stated previously, although the machine language generated by a com\-
piler can be executed directly on the hardware, it must nearly always be run along with some other code. Most user programs also require programs from the operating system. Among the most common of these are programs for input and output. The compiler builds calls to required system programs when they are needed by the user program. Before the machine language programs pro\-duced by a compiler can be executed, the required programs from the operating system must be found and linked to the user program. The linking operation connects the user program to the system programs by placing the addresses of the entry points of the system programs in the calls to them in the user pro\-gram. The user and system code together are sometimes called a load module , 
or executable image . The process of collecting system programs and linking 
them to user programs is called linking and loading , or sometimes just link\-
ing. It is accomplished by a systems program called a linker .
In addition to systems programs, user programs must often be linked to 
previously compiled user programs that reside in libraries. So the linker not only links a given program to system programs, but also it may link it to other user programs.
The speed of the connection between a computer’s memory and its proces\-
sor usually determines the speed of the computer, because instructions often can be executed faster than they can be moved to the processor for execution. This connection is called the von Neumann bottleneck ; it is the primary 
limiting factor in the speed of von Neumann architecture computers. The von Neumann bottleneck has been one of the primary motivations for the research and development of parallel computers.28 Chapter 1 Preliminaries
1\.7\.2 Pure Interpretation
Pure interpretation lies at the opposite end (from compilation) of implementa\-
tion methods. With this approach, programs are interpreted by another program called an interpreter, with no translation whatever. The interpreter program acts as a software simulation of a machine whose fetch\-execute cycle deals with high\-level language program statements rather than machine instructions. This software simulation obviously provides a virtual machine for the language.
Pure interpretation has the advantage of allowing easy implementation of 
many source\-level debugging operations, because all run\-time error messages can refer to source\-level units. For example, if an array index is found to be out of range, the error message can easily indicate the source line and the name of the array. On the other hand, this method has the serious disadvantage that execution is 10 to 100 times slower than in compiled systems. The primary source of this slowness is the decoding of the high\-level language statements, which are far more complex than machine language instructions (although there may be fewer statements than instructions in equivalent machine code). Furthermore, regardless of how many times a statement is executed, it must be decoded every time. Therefore, statement decoding, rather than the connec\-tion between the processor and memory, is the bottleneck of a pure interpreter.
Another disadvantage of pure interpretation is that it often requires more 
space. In addition to the source program, the symbol table must be present during interpretation. Furthermore, the source program may be stored in a form designed for easy access and modification rather than one that provides for minimal size.
Although some simple early languages of the 1960s (APL, SNOBOL, and 
LISP) were purely interpreted, by the 1980s, the approach was rarely used on high\-level languages. However, in recent years, pure interpretation has made a significant comeback with some Web scripting languages, such as JavaScript and PHP , which are now widely used. The process of pure interpretation is shown in Figure 1\.4\.
Source
program
Interpreter
ResultsInput dataFigure 1\.4
Pure interpretation1\.7 Implementation Methods 29
1\.7\.3 Hybrid Implementation Systems
Some language implementation systems are a compromise between compilers 
and pure interpreters; they translate high\-level language programs to an inter\-mediate language designed to allow easy interpretation. This method is faster than pure interpretation because the source language statements are decoded only once. Such implementations are called hybrid implementation systems .
The process used in a hybrid implementation system is shown in 
Figure 1\.5\. Instead of translating intermediate language code to machine code, it simply interprets the intermediate code.
Source
program
Interpreter
ResultsInput dataLexical
analyzer
Syntax
analyzer
Intermediate
code generatorParse treesLexical units
Intermediate
code Figure 1\.5
Hybrid implementation 
system30 Chapter 1 Preliminaries
Perl is implemented with a hybrid system. Perl programs are partially com\-
piled to detect errors before interpretation and to simplify the interpreter.
Initial implementations of Java were all hybrid. Its intermediate form, 
called byte code , provides portability to any machine that has a byte code 
interpreter and an associated run\-time system. T ogether, these are called the Java Virtual Machine. There are now systems that translate Java byte code into machine code for faster execution.
A Just\-in\-Time ( JIT) implementation system initially translates programs 
to an intermediate language. Then, during execution, it compiles intermediate language methods into machine code when they are called. The machine code version is kept for subsequent calls. JIT systems are now widely used for Java programs. Also, the .NET languages are all implemented with a JIT system.
Sometimes an implementor may provide both compiled and interpreted 
implementations for a language. In these cases, the interpreter is used to develop and debug programs. Then, after a (relatively) bug\-free state is reached, the programs are compiled to increase their execution speed.
1\.7\.4 Preprocessors
A preprocessor is a program that processes a program immediately before the 
program is compiled. Preprocessor instructions are embedded in programs. The preprocessor is essentially a macro expander. Preprocessor instructions are commonly used to specify that the code from another file is to be included. For example, the C preprocessor instruction
\#include "myLib.h"
causes the preprocessor to copy the contents of myLib.h into the program at 
the position of the \#include .
Other preprocessor instructions are used to define symbols to represent 
expressions. For example, one could use
\#define max(A, B) ((A) \> (B) ? (A) : (B))
to determine the largest of two given expressions. For example, the expression
x \= max(2 \* y, z / 1\.73\);
would be expanded by the preprocessor to
x \= ((2 \* y) \> (z / 1\.73\) ? (2 \* y) : (z / 1\.73\);
Notice that this is one of those cases where expression side effects can cause trouble. For example, if either of the expressions given to the 
max macro have 
side effects—such as z\+\+—it could cause a problem. Because one of the two 
expression parameters is evaluated twice, this could result in z being incre\-
mented twice by the code produced by the macro expansion.Summary 31
1\.8 Programming Environments
A programming environment is the collection of tools used in the development of 
software. This collection may consist of only a file system, a text editor, a linker, and a compiler. Or it may include a large collection of integrated tools, each accessed through a uniform user interface. In the latter case, the development and mainte\-nance of software is greatly enhanced. Therefore, the characteristics of a program\-ming language are not the only measure of the software development capability of a system. We now briefly describe several programming environments.
UNIX is an older programming environment, first distributed in the middle 
1970s, built around a portable multiprogramming operating system. It provides a wide array of powerful support tools for software production and maintenance in a variety of languages. In the past, the most important feature absent from UNIX was a uniform interface among its tools. This made it more difficult to learn and to use. However, UNIX is now often used through a graphical user interface (GUI) that runs on top of UNIX. Examples of UNIX GUIs are the Solaris Com\-mon Desktop Environment (CDE), GNOME, and KDE. These GUIs make the interface to UNIX appear similar to that of Windows and Macintosh systems.
Borland JBuilder is a programming environment that provides an inte\-
grated compiler, editor, debugger, and file system for Java development, where all four are accessed through a graphical interface. JBuilder is a complex and powerful system for creating Java software.
Microsoft Visual Studio .NET is a relatively recent step in the evolution 
of software development environments. It is a large and elaborate collection of software development tools, all used through a windowed interface. This system can be used to develop software in any one of the five .NET languages: C\#, Visual BASIC .NET, JScript (Microsoft’s version of JavaScript), F\# (a func\-tional language), and C\+\+/CLI.
NetBeans is a development environment that is primarily used for Java 
application development but also supports JavaScript, Ruby, and PHP . Both Visual Studio and NetBeans are more than development environments—they are also frameworks, which means they actually provide common parts of the code of the application.
SUMMARY
The study of programming languages is valuable for some important reasons: It increases our capacity to use different constructs in writing programs, enables us to choose languages for projects more intelligently, and makes learning new languages easier.
Computers are used in a wide variety of problem\-solving domains. The 
design and evaluation of a particular programming language is highly depen\-dent on the domain in which it is to be used.32 Chapter 1 Preliminaries
Among the most important criteria for evaluating languages are readability, 
writability, reliability, and overall cost. These will be the basis on which we examine and judge the various language features discussed in the remainder of the book.
The major influences on language design have been machine architecture 
and software design methodologies.
Designing a programming language is primarily an engineering feat, in 
which a long list of trade\-offs must be made among features, constructs, and capabilities.
The major methods of implementing programming languages are compila\-
tion, pure interpretation, and hybrid implementation.
Programming environments have become important parts of software 
development systems, in which the language is just one of the components.
REVIEW QUESTIONS
 1\. Why is it useful for a programmer to have some background in language 
design, even though he or she may never actually design a programming language?
 2\. How can knowledge of programming language characteristics benefit the 
whole computing community?
 3\. What programming language has dominated scientific computing over 
the past 50 years?
 4\. What programming language has dominated business applications over 
the past 50 years?
 5\. What programming language has dominated artificial intelligence over 
the past 50 years?
 6\. In what language is most of UNIX written? 7\. What is the disadvantage of having too many features in a language? 8\. How can user\-defined operator overloading harm the readability of a 
program?
 9\. What is one example of a lack of orthogonality in the design of C? 10\. What language used orthogonality as a primary design criterion? 11\. What primitive control statement is used to build more complicated 
control statements in languages that lack them?
 12\. What construct of a programming language provides process 
abstraction?
 13\. What does it mean for a program to be reliable? 14\. Why is type checking the parameters of a subprogram important? 15\. What is aliasing?Problem Set 33
 16\. What is exception handling?
 17\. Why is readability important to writability? 18\. How is the cost of compilers for a given language related to the design of 
that language?
 19\. What have been the strongest influences on programming language 
design over the past 50 years?
 20\. What is the name of the category of programming languages whose 
structure is dictated by the von Neumann computer architecture?
 21\. What two programming language deficiencies were discovered as a 
result of the research in software development in the 1970s?
 22\. What are the three fundamental features of an object\-oriented program\-
ming language?
 23\. What language was the first to support the three fundamental features of 
object\-oriented programming?
 24\. What is an example of two language design criteria that are in direct 
conflict with each other?
 25\. What are the three general methods of implementing a programming 
language?
 26\. Which produces faster program execution, a compiler or a pure 
interpreter?
 27\. What role does the symbol table play in a compiler? 28\. What does a linker do? 29\. Why is the von Neumann bottleneck important? 30\. What are the advantages in implementing a language with a pure 
interpreter?
PROBLEM SET
 1\. Do you believe our capacity for abstract thought is influenced by our 
language skills? Support your opinion.
 2\. What are some features of specific programming languages you know 
whose rationales are a mystery to you?
 3\. What arguments can you make for the idea of a single language for all 
programming domains?
 4\. What arguments can you make against the idea of a single language for 
all programming domains?
 5\. Name and explain another criterion by which languages can be judged 
(in addition to those discussed in this chapter).34 Chapter 1 Preliminaries
 6\. What common programming language statement, in your opinion, is 
most detrimental to readability?
 7\. Java uses a right brace to mark the end of all compound statements. 
What are the arguments for and against this design?
 8\. Many languages distinguish between uppercase and lowercase letters in 
user\-defined names. What are the pros and cons of this design decision?
 9\. Explain the different aspects of the cost of a programming language. 10\. What are the arguments for writing efficient programs even though 
hardware is relatively inexpensive?
 11\. Describe some design trade\-offs between efficiency and safety in some 
language you know.
 12\. In your opinion, what major features would a perfect programming lan\-
guage include?
 13\. Was the first high\-level programming language you learned imple\-
mented with a pure interpreter, a hybrid implementation system, or a compiler? (You may have to research this.)
 14\. Describe the advantages and disadvantages of some programming envi\-
ronment you have used.
 15\. How do type declaration statements for simple variables affect the read\-
ability of a language, considering that some languages do not require them?
 16\. Write an evaluation of some programming language you know, using the 
criteria described in this chapter.
 17\. Some programming languages—for example, Pascal—have used the 
semicolon to separate statements, while Java uses it to terminate state\-ments. Which of these, in your opinion, is most natural and least likely to result in syntax errors? Support your answer.
 18\. Many contemporary languages allow two kinds of comments: one in 
which delimiters are used on both ends (multiple\-line comments), and one in which a delimiter marks only the beginning of the comment (one\-line comments). Discuss the advantages and disadvantages of each of these with respect to our criteria.35 2\.1 Zuse’s Plankalkül
 2\.2 Pseudocodes
 2\.3 The IBM 704 and Fortran
 2\.4 Functional Programming: LISP
 2\.5 The First Step Toward Sophistication: ALGOL 60
 2\.6 Computerizing Business Records: COBOL
 2\.7 The Beginnings of Timesharing: BASIC
 2\.8 Everything for Everybody: PL/I
 2\.9 Two Early Dynamic Languages: APL and SNOBOL
 2\.10 The Beginnings of Data Abstraction: SIMULA 67
 2\.11 Orthogonal Design: ALGOL 68
 2\.12 Some Early Descendants of the ALGOLs
 2\.13 Programming Based on Logic: Prolog
 2\.14 History’s Largest Design Effort: Ada
 2\.15 Object\-Oriented Programming: Smalltalk
 2\.16 Combining Imperative and Object\-Oriented Features: C\+\+
 2\.17 An Imperative\-Based Object\-Oriented Language: Java
 2\.18 Scripting Languages
 2\.19 The Flagship .NET Language: C\#
 2\.20 Markup/Programming Hybrid Languages2
Evolution of the Major 
Programming Languages36 Chapter 2 Evolution of the Major Programming Languages
This chapter describes the development of a collection of programming lan\-
guages. It explores the environment in which each was designed and focuses on the contributions of the language and the motivation for its development. 
Overall language descriptions are not included; rather, we discuss only some of the 
new features introduced by each language. Of particular interest are the features 
that most influenced subsequent languages or the field of computer science.
This chapter does not include an in\-depth discussion of any language feature or 
concept; that is left for later chapters. Brief, informal explanations of features will suffice for our trek through the development of these languages.
This chapter discusses a wide variety of languages and language concepts that 
will not be familiar to many readers. These topics are discussed in detail only in later chapters. Those who find this unsettling may prefer to delay reading this chap\-ter until the rest of the book has been studied.
The choice as to which languages to discuss here was subjective, and some 
readers will unhappily note the absence of one or more of their favorites. However, 
to keep this historical coverage to a reasonable size, it was necessary to leave out 
some languages that some regard highly. The choices were based on our estimate of 
each language’s importance to language development and the computing world as a 
whole. We also include brief discussions of some other languages that are referenced later in the book.
The organization of this chapter is as follows: The initial versions of languages 
generally are discussed in chronological order. However, subsequent versions of lan\-
guages appear with their initial version, rather than in later sections. For example, Fortran 2003 is discussed in the section with Fortran I (1956\). Also, in some cases, languages of secondary importance that are related to a language that has its own section appear in that section.
This chapter includes listings of 14 complete example programs, each in a 
 different language. These programs are not described in this chapter; they are meant 
simply to illustrate the appearance of programs in these languages. Readers familiar 
with any of the common imperative languages should be able to read and understand 
most of the code in these programs, except those in LISP, COBOL, and Smalltalk. 
(A Scheme function similar to the LISP example is discussed in Chapter 15\.) The same 
problem is solved by the Fortran, ALGOL 60, PL/I, BASIC, Pascal, C, Perl, Ada, Java, 
JavaScript, and C\# programs. Note that most of the contemporary languages in this 
list support dynamic arrays, but because of the simplicity of the example problem, 
we did not use them in the example programs. Also, in the Fortran 95 program, we 
avoided using the features that could have avoided the use of loops altogether, in 
part to keep the program simple and readable and in part just to illustrate the basic 
loop structure of the language.
Figure 2\.1 is a chart of the genealogy of the high\-level languages discussed in 
this chapter.Chapter 2 Evolution of the Major Programming Languages 37
1957
585960
61
626364
65
6667
68
697071
72
737475
76
777879
80
8182
83
848586
87
888990
91
9293
94
959697
98
9900
01
020304
05
06
07
08
09
10
11ALGOL 58
ALGOL 60
ALGOL W
PascalBASIC
OberonMODULA\-3
EiffelANSI C (C89\)
Fortran 90
Fortran 95Fortran 77Fortran IVFortran IIFortran I
Visual BASICQuickBASICCPL
BCPL
CBPL/ICOBOLLISP
SchemeFLOW\-MATIC
C\+\+APL
COMMON LISPMODULA\-2SNOBOL
ICONSIMULA I
SIMULA 67
ALGOL 68
Ada 83Smalltalk 80
Ada 95
Ada 2005Lua Java
JavascriptRuby
Ruby 1\.8
Ruby 1\.9Prolog
Java 5\.0
Java 6\.0
Java 7\.0Miranda
Haskell
Python
Python 2\.0
Python 3\.0ML
Perl
PHP
C99
C\#
C\# 2\.0
C\# 3\.0
C\# 4\.0Visual Basic.NETawk
Fortran 2003
Fortran 2008
Figure 2\.1
Genealogy of common high\-level programming languages38 Chapter 2 Evolution of the Major Programming Languages
2\.1 Zuse’s Plankalkül
The first programming language discussed in this chapter is highly unusual 
in several respects. For one thing, it was never implemented. Furthermore, although developed in 1945, its description was not published until 1972\. Because so few people were familiar with the language, some of its capabilities did not appear in other languages until 15 years after its development.
2\.1\.1 Historical Background
Between 1936 and 1945, German scientist Konrad Zuse (pronounced “T soo\-zuh”) built a series of complex and sophisticated computers from electrome\-chanical relays. By early 1945, Allied bombing had destroyed all but one of his latest models, the Z4, so he moved to a remote Bavarian village, Hinterstein, and his research group members went their separate ways.
Working alone, Zuse embarked on an effort to develop a language for 
expressing computations for the Z4, a project he had begun in 1943 as a pro\-posal for his Ph.D. dissertation. He named this language Plankalkül, which means program calculus. In a lengthy manuscript dated 1945 but not published 
until 1972 (Zuse, 1972\), Zuse defined Plankalkül and wrote algorithms in the language to solve a wide variety of problems.
2\.1\.2 Language Overview
Plankalkül was remarkably complete, with some of its most advanced features in the area of data structures. The simplest data type in Plankalkül was the single bit. Integer and floating\-point numeric types were built from the bit type. The floating\-point type used twos\-complement notation and the “hid\-den bit” scheme currently used to avoid storing the most significant bit of the normalized fraction part of a floating\-point value.
In addition to the usual scalar types, Plankalkül included arrays and records 
(called structs in the C\-based languages). The records could include nested 
records.
Although the language had no explicit goto, it did include an iterative state\-
ment similar to the Ada 
for. It also had the command Fin with a superscript 
that specified an exit out of a given number of iteration loop nestings or to the beginning of a new iteration cycle. Plankalkül included a selection statement, but it did not allow an 
else clause.
One of the most interesting features of Zuse’s programs was the inclusion 
of mathematical expressions showing the current relationships between pro\-gram variables. These expressions stated what would be true during execution at the points in the code where they appeared. These are very similar to the assertions of Java and in those in axiomatic semantics, which is discussed in Chapter 3\.Zuse’s manuscript contained programs of far greater complexity than any 
written prior to 1945\. Included were programs to sort arrays of numbers; test the connectivity of a given graph; carry out integer and floating\-point opera\-tions, including square root; and perform syntax analysis on logic formulas that had parentheses and operators in six different levels of precedence. Perhaps most remarkable were his 49 pages of algorithms for playing chess, a game in which he was not an expert.
If a computer scientist had found Zuse’s description of Plankalkül in the 
early 1950s, the single aspect of the language that would have hindered its implementation as defined would have been the notation. Each statement con\-sisted of either two or three lines of code. The first line was most like the state\-ments of current languages. The second line, which was optional, contained the subscripts of the array references in the first line. The same method of indicating subscripts was used by Charles Babbage in programs for his Ana\-lytical Engine in the middle of the nineteenth century. The last line of each Plankalkül statement contained the type names for the variables mentioned in the first line. This notation is quite intimidating when first seen.
The following example assignment statement, which assigns the value of 
the expression 
A\[4] \+1 to A\[5] , illustrates this notation. The row labeled V is 
for subscripts, and the row labeled S is for the data types. In this example, 1\.n 
means an integer of n bits:
 \| A \+ 1 \=\> A
V \| 4 5
S \| 1\.n 1\.n
We can only speculate on the direction that programming language design 
might have taken if Zuse’s work had been widely known in 1945 or even 1950\. It is also interesting to consider how his work might have been different had he done it in a peaceful environment surrounded by other scientists, rather than in Germany in 1945 in virtual isolation.
2\.2 Pseudocodes
First, note that the word pseudocode is used here in a different sense than its 
contemporary meaning. We call the languages discussed in this section pseudo\-codes because that’s what they were named at the time they were developed and used (the late 1940s and early 1950s). However, they are clearly not pseudo\-codes in the contemporary sense.
The computers that became available in the late 1940s and early 1950s 
were far less usable than those of today. In addition to being slow, unreliable, expensive, and having extremely small memories, the machines of that time were difficult to program because of the lack of supporting software.
There were no high\-level programming languages or even assembly lan\-
guages, so programming was done in machine code, which is both tedious and 2\.2 Pseudocodes 3940 Chapter 2 Evolution of the Major Programming Languages
error prone. Among its problems is the use of numeric codes for specifying 
instructions. For example, an ADD instruction might be specified by the code 14 rather than a connotative textual name, even if only a single letter. This makes programs very difficult to read. A more serious problem is absolute addressing, which makes program modification tedious and error prone. For example, suppose we have a machine language program stored in memory. Many of the instructions in such a program refer to other locations within the program, usually to reference data or to indicate the targets of branch instruc\-tions. Inserting an instruction at any position in the program other than at the end invalidates the correctness of all instructions that refer to addresses beyond the insertion point, because those addresses must be increased to make room for the new instruction. T o make the addition correctly, all instructions that refer to addresses that follow the addition must be found and modified. A similar problem occurs with deletion of an instruction. In this case, however, machine languages often include a “no operation” instruction that can replace deleted instructions, thereby avoiding the problem.
These are standard problems with all machine languages and were the 
primary motivations for inventing assemblers and assembly languages. In addi\-tion, most programming problems of that time were numerical and required floating\-point arithmetic operations and indexing of some sort to allow the convenient use of arrays. Neither of these capabilities, however, was included in the architecture of the computers of the late 1940s and early 1950s. These defi\-ciencies naturally led to the development of somewhat higher\-level languages.
2\.2\.1 Short Code
The first of these new languages, named Short Code, was developed by John Mauchly in 1949 for the BINAC computer, which was one of the first success\-ful stored\-program electronic computers. Short Code was later transferred to a UNIVAC I computer (the first commercial electronic computer sold in the United States) and, for several years, was one of the primary means of pro\-gramming those machines. Although little is known of the original Short Code because its complete description was never published, a programming manual for the UNIVAC I version did survive (Remington\-Rand, 1952\). It is safe to assume that the two versions were very similar.
The words of the UNIVAC I’s memory had 72 bits, grouped as 12 six\-bit 
bytes. Short Code consisted of coded versions of mathematical expressions that were to be evaluated. The codes were byte\-pair values, and many equations could be coded in a word. The following operation codes were included:
01 \- 06 abs value 1n (n\+2\)nd power
02 ) 07 \+ 2n (n\+2\)nd root
03 \= 08 pause 4n if \<\= n
04 / 09 ( 58 print and tabVariables were named with byte\-pair codes, as were locations to be used as 
constants. For example, X0 and Y0 could be variables. The statement
X0 \= SQRT(ABS(Y0\))
would be coded in a word as 00 X0 03 20 06 Y0 . The initial 00 was used 
as padding to fill the word. Interestingly, there was no multiplication code; multiplication was indicated by simply placing the two operands next to each other, as in algebra.
Short Code was not translated to machine code; rather, it was implemented 
with a pure interpreter. At the time, this process was called automatic program\-
ming. It clearly simplified the programming process, but at the expense of 
execution time. Short Code interpretation was approximately 50 times slower than machine code.
2\.2\.2 Speedcoding
In other places, interpretive systems were being developed that extended machine languages to include floating\-point operations. The Speedcoding system developed by John Backus for the IBM 701 is an example of such a system (Backus, 1954\). The Speedcoding interpreter effectively converted the 701 to a virtual three\-address floating\-point calculator. The system included pseudoinstructions for the four arithmetic operations on floating\-point data, as well as operations such as square root, sine, arc tangent, exponent, and logarithm. Conditional and unconditional branches and input/output conversions were also part of the virtual architecture. T o get an idea of the 
limitations of such systems, consider that the remaining usable memory after loading the interpreter was only 700 words and that the add instruction took 4\.2 milliseconds to execute. On the other hand, Speedcoding included the 
novel facility of automatically incrementing address registers. This facility did not appear in hardware until the UNIVAC 1107 computers of 1962\. Because of such features, matrix multiplication could be done in 12 Speedcoding instruc\-tions. Backus claimed that problems that could take two weeks to program in machine code could be programmed in a few hours using Speedcoding.
2\.2\.3 The UNIVAC “Compiling” System
Between 1951 and 1953, a team led by Grace Hopper at UNIVAC developed a series of “compiling” systems named A\-0, A\-1, and A\-2 that expanded a pseudo\-code into machine code subprograms in the same way as macros are expanded into assembly language. The pseudocode source for these “compilers” was still quite primitive, although even this was a great improvement over machine code because it made source programs much shorter. Wilkes (1952\) independently suggested a similar process.2\.2 Pseudocodes 4142 Chapter 2 Evolution of the Major Programming Languages
2\.2\.4 Related Work
Other means of easing the task of programming were being developed at about 
the same time. At Cambridge University, David J. Wheeler (1950\) developed a method of using blocks of relocatable addresses to solve, at least partially, the problem of absolute addressing, and later, Maurice V . Wilkes (also at Cam\-bridge) extended the idea to design an assembly program that could combine chosen subroutines and allocate storage (Wilkes et al., 1951, 1957\). This was indeed an important and fundamental advance.
We should also mention that assembly languages, which are quite different 
from the pseudocodes discussed, evolved during the early 1950s. However, they had little impact on the design of high\-level languages.
2\.3 The IBM 704 and Fortran
Certainly one of the greatest single advances in computing came with the 
introduction of the IBM 704 in 1954, in large measure because its capabilities prompted the development of Fortran. One could argue that if it had not been IBM with the 704 and Fortran, it would soon thereafter have been some other organization with a similar computer and related high\-level language. How\-ever, IBM was the first with both the foresight and the resources to undertake these developments.
2\.3\.1 Historical Background
One of the primary reasons why the slowness of interpretive systems was tol\-erated from the late 1940s to the mid\-1950s was the lack of floating\-point hardware in the available computers. All floating\-point operations had to be simulated in software, a very time\-consuming process. Because so much pro\-cessor time was spent in software floating\-point processing, the overhead of interpretation and the simulation of indexing were relatively insignificant. As long as floating\-point had to be done by software, interpretation was an accept\-able expense. However, many programmers of that time never used interpre\-tive systems, preferring the efficiency of hand\-coded machine (or assembly) language. The announcement of the IBM 704 system, with both indexing and floating\-point instructions in hardware, heralded the end of the interpretive era, at least for scientific computation. The inclusion of floating\-point hard\-ware removed the hiding place for the cost of interpretation.
Although Fortran is often credited with being the first compiled high\-
level language, the question of who deserves credit for implementing the first such language is somewhat open. Knuth and Pardo (1977\) give the credit to Alick E. Glennie for his Autocode compiler for the Manchester Mark I com\-puter. Glennie developed the compiler at Fort Halstead, Royal Armaments Research Establishment, in England. The compiler was operational by Sep\-tember 1952\. However, according to John Backus (Wexelblat, 1981, p. 26\), Glennie’s Autocode was so low level and machine oriented that it should not 
be considered a compiled system. Backus gives the credit to Laning and Zierler at the Massachusetts Institute of T echnology.
The Laning and Zierler system (Laning and Zierler, 1954\) was the first 
algebraic translation system to be implemented. By algebraic, we mean that it translated arithmetic expressions, used separately coded subprograms to com\-pute transcendental functions (e.g., sine and logarithm), and included arrays. The system was implemented on the MIT Whirlwind computer, in experi\-mental prototype form, in the summer of 1952 and in a more usable form by May 1953\. The translator generated a subroutine call to code each formula, or expression, in the program. The source language was easy to read, and the only actual machine instructions included were for branching. Although this work preceded the work on Fortran, it never escaped MIT.
In spite of these earlier works, the first widely accepted compiled high\-
level language was Fortran. The following subsections chronicle this important development.
2\.3\.2 Design Process
Even before the 704 system was announced in May 1954, plans were begun for Fortran. By November 1954, John Backus and his group at IBM had produced the report titled “The IBM Mathematical FORmula TRANslating System: FORTRAN” (IBM, 1954\). This document described the first version of For\-tran, which we refer to as Fortran 0, prior to its implementation. It also boldly stated that Fortran would provide the efficiency of hand\-coded programs and the ease of programming of the interpretive pseudocode systems. In another burst of optimism, the document stated that Fortran would eliminate coding errors and the debugging process. Based on this premise, the first Fortran compiler included little syntax error checking.
The environment in which Fortran was developed was as follows: (1\) Com\-
puters had small memories and were slow and relatively unreliable; (2\) the primary use of computers was for scientific computations; (3\) there were no existing efficient and effective ways to program computers; and (4\) because of the high cost of computers compared to the cost of programmers, speed of the generated object code was the primary goal of the first Fortran compilers. The characteristics of the early versions of Fortran follow directly from this environment.
2\.3\.3 Fortran I Overview
Fortran 0 was modified during the implementation period, which began in January 1955 and continued until the release of the compiler in April 1957\. The implemented language, which we call Fortran I, is described in the first Fortran Programmer’s Reference Manual, published in October 1956 (IBM, 1956\). For\-
tran I included input/output formatting, variable names of up to six characters (it had been just two in Fortran 0\), user\-defined subroutines, although they 2\.3 The IBM 704 and Fortran 4344 Chapter 2 Evolution of the Major Programming Languages
could not be separately compiled, the If selection statement, and the Do loop 
statement.
All of Fortran I’s control statements were based on 704 instructions. It is 
not clear whether the 704 designers dictated the control statement design of Fortran I or whether the designers of Fortran I suggested these instructions to the 704 designers.
There were no data\-typing statements in the Fortran I language. Variables 
whose names began with 
I, J, K, L, M, and N were implicitly integer type, and all 
others were implicitly floating\-point. The choice of the letters for this conven\-tion was based on the fact that at that time scientists and engineers used letters as variable subscripts, usually i, j, and k. In a gesture of generosity, Fortran’s 
designers threw in the three additional letters.
The most audacious claim made by the Fortran development group during 
the design of the language was that the machine code produced by the compiler would be about half as efficient as what could be produced by hand.
1 This, more 
than anything else, made skeptics of potential users and prevented a great deal of interest in Fortran before its actual release. T o almost everyone’s surprise, however, the Fortran development group nearly achieved its goal in efficiency. The largest part of the 18 worker\-years of effort used to construct the first com\-piler had been spent on optimization, and the results were remarkably effective.
The early success of Fortran is shown by the results of a survey made in 
April 1958\. At that time, roughly half of the code being written for 704s was being written in Fortran, in spite of the skepticism of most of the programming world only a year earlier.
2\.3\.4 Fortran II
The Fortran II compiler was distributed in the spring of 1958\. It fixed many of the bugs in the Fortran I compilation system and added some significant features to the language, the most important being the independent com\-pilation of subroutines. Without independent compilation, any change in a program required that the entire program be recompiled. Fortran I’s lack of independent\-compilation capability, coupled with the poor reliability of the 704, placed a practical restriction on the length of programs to about 300 to 400 lines (Wexelblat, 1981, p. 68\). Longer programs had a poor chance of being compiled completely before a machine failure occurred. The capability of including precompiled machine language versions of subprograms shortened the compilation process considerably and made it practical to develop much larger programs.
 1\. In fact, the Fortran team believed that the code generated by their compiler could be no 
less than half as fast as handwritten machine code, or the language would not be adopted by users.2\.3\.5 Fortrans IV, 77, 90, 95, 2003, and 2008
A Fortran III was developed, but it was never widely distributed. Fortran IV , 
however, became one of the most widely used programming languages of its time. It evolved over the period 1960 to 1962 and was standardized as For\-tran 66 (ANSI, 1966\), although that name was rarely used. Fortran IV was an improvement over Fortran II in many ways. Among its most important addi\-tions were explicit type declarations for variables, a logical 
If construct, and 
the capability of passing subprograms as parameters to other subprograms.
Fortran IV was replaced by Fortran 77, which became the new standard 
in 1978 (ANSI, 1978a). Fortran 77 retained most of the features of Fortran IV and added character string handling, logical loop control statements, and an 
If with an optional Else clause.
Fortran 90 (ANSI, 1992\) was dramatically different from Fortran 77\. The 
most significant additions were dynamic arrays, records, pointers, a multiple selection statement, and modules. In addition, Fortran 90 subprograms could be recursively called.
A new concept that was included in the Fortran 90 definition was that of 
removing some language features from earlier versions. While Fortran 90 included all of the features of Fortran 77, the language definition included a list of con\-structs that were recommended for removal in the next version of the language.
Fortran 90 included two simple syntactic changes that altered the appearance 
of both programs and the literature describing the language. First, the required fixed format of code, which required the use of specific character positions for spe\-cific parts of statements, was dropped. For example, statement labels could appear only in the first five positions and statements could not begin before the seventh position. This rigid formatting of code was designed around the use of punch cards. The second change was that the official spelling of FORTRAN became Fortran. This change was accompanied by the change in convention of using all uppercase letters for keywords and identifiers in Fortran programs. The new convention was that only the first letter of keywords and identifiers would be uppercase.
Fortran 95 (INCITS/ISO/IEC, 1997\) continued the evolution of the lan\-
guage, but only a few changes were made. Among other things, a new iteration construct, 
Forall , was added to ease the task of parallelizing Fortran programs.
Fortran 2003 (Metcalf et al., 2004\), added support for object\-oriented pro\-
gramming, parameterized derived types, procedure pointers, and interoper\-ability with the C programming language.
The latest version of Fortran, Fortran 2008 (ISO/IEC 1539\-1, 2010\) added 
support for blocks to define local scopes, co\-arrays, which provide a parallel execution model, and the 
DO CONCURRENT construct, to specify loops without 
interdependencies.
2\.3\.6 Evaluation
The original Fortran design team thought of language design only as a nec\-essary prelude to the critical task of designing the translator. Furthermore, it never occurred to them that Fortran would be used on computers not 2\.3 The IBM 704 and Fortran 4546 Chapter 2 Evolution of the Major Programming Languages
manufactured by IBM. Indeed, they were forced to consider building Fortran 
compilers for other IBM machines only because the successor to the 704, the 709, was announced before the 704 Fortran compiler was released. The effect that Fortran has had on the use of computers, along with the fact that all sub\-sequent programming languages owe a debt to Fortran, is indeed impressive in light of the modest goals of its designers.
One of the features of Fortran I, and all of its successors before 90, that allows 
highly optimizing compilers was that the types and storage for all variables are fixed before run time. No new variables or space could be allocated during execu\-tion time. This was a sacrifice of flexibility to simplicity and efficiency. It elimi\-nated the possibility of recursive subprograms and made it difficult to implement data structures that grow or change shape dynamically. Of course, the kinds of programs that were being built at the time of the development of the early versions of Fortran were primarily numerical in nature and were simple in comparison with more recent software projects. Therefore, the sacrifice was not a great one.
The overall success of Fortran is difficult to overstate: It dramatically 
changed the way computers are used. This is, of course, in large part due to its being the first widely used high\-level language. In comparison with concepts and languages developed later, early versions of Fortran suffer in a variety of ways, as should be expected. After all, it would not be fair to compare the performance and comfort of a 1910 Model T Ford with the performance and comfort of a 2013 Ford Mustang. Nevertheless, in spite of the inadequacies of Fortran, the momentum of the huge investment in Fortran software, among other factors, has kept it in use for more than a half century.
Alan Perlis, one of the designers of ALGOL 60, said of Fortran in 1978, 
“Fortran is the lingua franca of the computing world. It is the language of the 
streets in the best sense of the word, not in the prostitutional sense of the word. And it has survived and will survive because it has turned out to be a remarkably useful part of a very vital commerce” (Wexelblat, 1981, p. 161\).
The following is an example of a Fortran 95 program:
! Fortran 95 Example program
! Input: An integer, List\_Len, where List\_Len is less! than 100, followed by List\_Len\-Integer values! Output: The number of input values that are greater
! than the average of all input values
Implicit none
Integer Dimension(99\) :: Int\_List
Integer :: List\_Len, Counter, Sum, Average, Result
Result\= 0
Sum \= 0
Read \*, List\_Len
If ((List\_Len \> 0\) .AND. (List\_Len \< 100\)) Then
! Read input data into an array and compute its sum
 Do Counter \= 1, List\_Len Read \*, Int\_List(Counter) Sum \= Sum \+ Int\_List(Counter) End Do
! Compute the average Average \= Sum / List\_Len! Count the values that are greater than the average Do Counter \= 1, List\_Len
 If (Int\_List(Counter) \> Average) Then
 Result \= Result \+ 1
 End If
 End Do
! Print the result
 Print \*, 'Number of values \> Average is:', Result 
Else
 Print \*, 'Error \- list length value is not legal'
End IfEnd Program Example
2\.4 Functional Programming: LISP
The first functional programming language was invented to provide language 
features for list processing, the need for which grew out of the first applications in the area of artificial intelligence (AI).
2\.4\.1 The Beginnings of Artificial Intelligence and List Processing
Interest in AI appeared in the mid\-1950s in a number of places. Some of this interest grew out of linguistics, some from psychology, and some from math\-ematics. Linguists were concerned with natural language processing. Psycholo\-gists were interested in modeling human information storage and retrieval, as well as other fundamental processes of the brain. Mathematicians were inter\-ested in mechanizing certain intelligent processes, such as theorem proving. All of these investigations arrived at the same conclusion: Some method must be developed to allow computers to process symbolic data in linked lists. At the time, most computation was on numeric data in arrays.
The concept of list processing was developed by Allen Newell, J. C. Shaw, 
and Herbert Simon at the RAND Corporation. It was first published in a clas\-sic paper that describes one of the first AI programs, the Logic Theorist,
2 and 
a language in which it could be implemented (Newell and Simon, 1956\). The language, named IPL\-I (Information Processing Language I), was never imple\-mented. The next version, IPL\-II, was implemented on a RAND Johnniac computer. Development of IPL continued until 1960, when the description of IPL\-V was published (Newell and T onge, 1960\). The low level of the IPL languages prevented their widespread use. They were actually assembly lan\-guages for a hypothetical computer, implemented with an interpreter, in which 
 2\. Logic Theorist discovered proofs for theorems in propositional calculus.2\.4 Functional Programming: LISP 4748 Chapter 2 Evolution of the Major Programming Languages
list\-processing instructions were included. Another factor that kept the IPL 
languages from becoming popular was their implementation on the obscure Johnniac machine.
The contributions of the IPL languages were in their list design and their 
demonstration that list processing was feasible and useful.
IBM became interested in AI in the mid\-1950s and chose theorem prov\-
ing as a demonstration area. At the time, the Fortran project was still under\-way. The high cost of the Fortran I compiler convinced IBM that their list processing should be attached to Fortran, rather than in the form of a new language. Thus, the Fortran List Processing Language (FLPL) was designed and implemented as an extension to Fortran. FLPL was used to construct a theorem prover for plane geometry, which was then considered the easiest area for mechanical theorem proving.
2\.4\.2 LISP Design Process
John McCarthy of MIT took a summer position at the IBM Information Research Department in 1958\. His goal for the summer was to investigate symbolic computations and to develop a set of requirements for doing such computations. As a pilot example problem area, he chose differentiation of algebraic expressions. From this study came a list of language requirements. Among them were the control flow methods of mathematical functions: recur\-sion and conditional expressions. The only available high\-level language of the time, Fortran I, had neither of these.
Another requirement that grew from the symbolic\-differentiation inves\-
tigation was the need for dynamically allocated linked lists and some kind of implicit deallocation of abandoned lists. McCarthy simply would not allow his elegant algorithm for differentiation to be cluttered with explicit deallocation statements.
Because FLPL did not support recursion, conditional expressions, dynamic 
storage allocation, or implicit deallocation, it was clear to McCarthy that a new language was needed.
When McCarthy returned to MIT in the fall of 1958, he and Marvin 
Minsky formed the MIT AI Project, with funding from the Research Labora\-tory for Electronics. The first important effort of the project was to produce a software system for list processing. It was to be used initially to implement a program proposed by McCarthy called the Advice T aker.
3 This application 
became the impetus for the development of the list\-processing language LISP . The first version of LISP is sometimes called “pure LISP” because it is a purely functional language. In the following section, we describe the development of pure LISP .
 3\. Advice T aker represented information with sentences written in a formal language and used 
a logical inferencing process to decide what to do.2\.4\.3 Language Overview
2\.4\.3\.1 Data Structures
Pure LISP has only two kinds of data structures: atoms and lists. Atoms are 
either symbols, which have the form of identifiers, or numeric literals. The con\-cept of storing symbolic information in linked lists is natural and was used in IPL\-II. Such structures allow insertions and deletions at any point, operations that were then thought to be a necessary part of list processing. It was eventu\-ally determined, however, that LISP programs rarely require these operations.
Lists are specified by delimiting their elements with parentheses. Simple 
lists, in which elements are restricted to atoms, have the form
(A B C D)
Nested list structures are also specified by parentheses. For example, the list
(A (B C) D (E (F G)))
is composed of four elements. The first is the atom A; the second is the sublist 
(B C) ; the third is the atom D; the fourth is the sublist (E (F G)) , which has 
as its second element the sublist (F G) .
Internally, lists are stored as single\-linked list structures, in which each 
node has two pointers and represents a list element. A node containing an atom has its first pointer pointing to some representation of the atom, such as its symbol or numeric value, or a pointer to a sublist. A node for a sublist element has its first pointer pointing to the first node of the sublist. In both cases, the second pointer of a node points to the next element of the list. A list is referenced by a pointer to its first element.
The internal representations of the two lists shown earlier are depicted in 
Figure 2\.2\. Note that the elements of a list are shown horizontally. The last element of a list has no successor, so its link is 
NIL, which is represented in 
Figure 2\.2 as a diagonal line in the element. Sublists are shown with the same structure.
2\.4\.3\.2 Processes in Functional Programming
LISP was designed as a functional programming language. All computation in a purely functional program is accomplished by applying functions to arguments. Neither the assignment statements nor the variables that abound in imperative language programs are necessary in functional language programs. Furthermore, 
repetitive processes can be specified with recursive function calls, making itera\-tion (loops) unnecessary. These basic concepts of functional programming make it significantly different from programming in an imperative language.2\.4 Functional Programming: LISP 4950 Chapter 2 Evolution of the Major Programming Languages
2\.4\.3\.3 The Syntax of LISP
LISP is very different from the imperative languages, both because it is a func\-
tional programming language and because the appearance of LISP programs is so different from those in languages like Java or C\+\+. For example, the syntax of Java is a complicated mixture of English and algebra, while LISP’s syntax is a model of simplicity. Program code and data have exactly the same form: parenthesized lists. Consider again the list
(A B C D)
When interpreted as data, it is a list of four elements. When viewed as code, it is the application of the function named 
A to the three parameters B, C, and D.
2\.4\.4 Evaluation
LISP completely dominated AI applications for a quarter century. Much of the cause of LISP’s reputation for being highly inefficient has been eliminated. Many contemporary implementations are compiled, and the resulting code is much faster than running the source code on an interpreter. In addition to its success in AI, LISP pioneered functional programming, which has proven to be a lively area of research in programming languages. As stated in Chapter 1, many programming language researchers believe functional programming is a much better approach to software development than procedural programming using imperative languages.B CD
FGBC EA DAFigure 2\.2
Internal representation 
of two LISP listsThe following is an example of a LISP program:
; LISP Example function
; The following code defines a LISP predicate function; that takes two lists as arguments and returns True; if the two lists are equal, and NIL (false) otherwise 
 (DEFUN equal\_lists (lis1 lis2\) (COND
 ((ATOM lis1\) (EQ lis1 lis2\))
 ((ATOM lis2\) NIL)
 ((equal\_lists (CAR lis1\) (CAR lis2\)) 
 (equal\_lists (CDR lis1\) (CDR lis2\)))
 (T NIL)
 ))
2\.4\.5 Two Descendants of LISP
T wo dialects of LISP are now widely used, Scheme and Common LISP . These 
are briefly discussed in the following subsections.
2\.4\.5\.1 Scheme
The Scheme language emerged from MIT in the mid\-1970s (Dybvig, 2003\). It is characterized by its small size, its exclusive use of static scoping (discussed in Chapter 5\), and its treatment of functions as first\-class entities. As first\-class entities, Scheme functions can be assigned to variables, passed as parameters, and returned as the values of function applications. They can also be the ele\-ments of lists. Early versions of LISP did not provide all of these capabilities, nor did they use static scoping.
As a small language with simple syntax and semantics, Scheme is well suited 
to educational applications, such as courses in functional programming and general introductions to programming. Scheme is described in some detail in Chapter 15\.
2\.4\.5\.2 Common LISP
During the 1970s and early 1980s, a large number of different dialects of LISP were developed and used. This led to the familiar problem of lack of portabil\-ity among programs written in the various dialects. Common LISP (Graham, 1996\) was created in an effort to rectify this situation. Common LISP was designed by combining the features of several dialects of LISP developed in the early 1980s, including Scheme, into a single language. Being such an amalgam, Common LISP is a relatively large and complex language. Its basis, however, is pure LISP , so its syntax, primitive functions, and fundamental nature come from that language.2\.4 Functional Programming: LISP 5152 Chapter 2 Evolution of the Major Programming Languages
Recognizing the flexibility provided by dynamic scoping as well as the 
simplicity of static scoping, Common LISP allows both. The default scoping for variables is static, but by declaring a variable to be 
special , that variable 
becomes dynamically scoped.
Common LISP has a large number of data types and structures, including 
records, arrays, complex numbers, and character strings. It also has a form of packages for modularizing collections of functions and data providing access control.
Common LISP is further described in Chapter 15\.
2\.4\.6 Related Languages
ML ( MetaLanguage; Ullman, 1998\) was originally designed in the 1980s by 
Robin Milner at the University of Edinburgh as a metalanguage for a program verification system named Logic for Computable Functions (LCF; Milner et al., 1990\). ML is primarily a functional language, but it also supports impera\-tive programming. Unlike LISP and Scheme, the type of every variable and expression in ML can be determined at compile time. T ypes are associated with objects rather than names. T ypes of names and expressions are inferred from their context.
Unlike LISP and Scheme, ML does not use the parenthesized functional 
syntax that originated with lambda expressions. Rather, the syntax of ML resembles that of the imperative languages, such as Java and C\+\+.
Miranda was developed by David T urner (1986\) at the University of Kent 
in Canterbury, England, in the early 1980s. Miranda is based partly on the languages ML, SASL, and KRC. Haskell (Hudak and Fasel, 1992\) is based in large part on Miranda. Like Miranda, it is a purely functional language, having no variables and no assignment statement. Another distinguishing character\-istic of Haskell is its use of lazy evaluation. This means that no expression is evaluated until its value is required. This leads to some surprising capabilities in the language.
Caml (Cousineau et al., 1998\) and its dialect that supports object\-oriented 
programming, OCaml (Smith, 2006\), descended from ML and Haskell. Finally, F\# is a relatively new typed language based directly on OCaml. F\# (Syme et al., 2010\) is a .NET language with direct access to the whole .NET library. Being a .NET language also means it can smoothly interoperate with any other .NET language. F\# supports both functional programming and procedural program\-ming. It also fully supports object\-oriented programming.
ML, Haskell, and F\# are further discussed in Chapter 15\.
2\.5 The First Step Toward Sophistication: ALGOL 60
ALGOL 60 has had much influence on subsequent programming languages 
and is therefore of central importance in any historical study of languages.2\.5\.1 Historical Background
ALGOL 60 was the result of efforts to design a universal programming language 
for scientific applications. By late 1954, the Laning and Zierler algebraic system had been in operation for over a year, and the first report on Fortran had been published. Fortran became a reality in 1957, and several other high\-level languages were being developed. Most notable among them were IT, which was designed by Alan Perlis at Carnegie T ech, and two languages for the UNIVAC computers, MATH\-MATIC and UNICODE. The proliferation of languages made program sharing among users difficult. Furthermore, the new languages were all grow\-ing up around single architectures, some for UNIVAC computers and some for IBM 700\-series machines. In response to this blossoming of machine\-dependent languages, several major computer user groups in the United States, including SHARE (the IBM scientific user group) and USE (UNIVAC Scientific Exchange, the large\-scale UNIVAC scientific user group), submitted a petition to the Asso\-ciation for Computing Machinery (ACM) on May 10, 1957, to form a commit\-tee to study and recommend action to create a machine\-independent scientific programming language. Although Fortran might have been a candidate, it could not become a universal language, because at the time it was solely owned by IBM.
Previously, in 1955, GAMM (a German acronym for Society for Applied 
Mathematics and Mechanics) had formed a committee to design one universal, machine\-independent algorithmic language. The desire for this new language was in part due to the Europeans’ fear of being dominated by IBM. By late 1957, however, the appearance of several high\-level languages in the United States convinced the GAMM subcommittee that their effort had to be widened to include the Americans, and a letter of invitation was sent to ACM. In April 1958, after Fritz Bauer of GAMM presented the formal proposal to ACM, the two groups officially agreed to a joint language design project.
2\.5\.2 Early Design Process
GAMM and ACM each sent four members to the first design meeting. The meeting, which was held in Zurich from May 27 to June 1, 1958, began with the following goals for the new language:
• The syntax of the language should be as close as possible to standard math\-
ematical notation, and programs written in it should be readable with little further explanation.
• It should be possible to use the language for the description of algorithms 
in printed publications.
• Programs in the new language must be mechanically translatable into 
machine language.
The first goal indicated that the new language was to be used for scientific 
programming, which was the primary computer application area at that time. The second was something entirely new to the computing business. The last goal is an obvious necessity for any programming language.2\.5 The First Step Toward Sophistication: ALGOL 60 5354 Chapter 2 Evolution of the Major Programming Languages
The Zurich meeting succeeded in producing a language that met the stated 
goals, but the design process required innumerable compromises, both among individuals and between the two sides of the Atlantic. In some cases, the com\-promises were not so much over great issues as they were over spheres of influence. The question of whether to use a comma (the European method) or a period (the American method) for a decimal point is one example.
2\.5\.3 ALGOL 58 Overview
The language designed at the Zurich meeting was named the International Algorithmic Language (IAL). It was suggested during the design that the lan\-guage be named ALGOL, for ALGOrithmic Language, but the name was rejected because it did not reflect the international scope of the committee. During the following year, however, the name was changed to ALGOL, and the language subsequently became known as ALGOL 58\.
In many ways, ALGOL 58 was a descendant of Fortran, which is quite 
natural. It generalized many of Fortran’s features and added several new con\-structs and concepts. Some of the generalizations had to do with the goal of not tying the language to any particular machine, and others were attempts to make the language more flexible and powerful. A rare combination of simplicity and elegance emerged from the effort.
ALGOL 58 formalized the concept of data type, although only variables 
that were not floating\-point required explicit declaration. It added the idea of compound statements, which most subsequent languages incorporated. Some features of Fortran that were generalized were the following: Identifiers were allowed to have any length, as opposed to Fortran I’s restriction to six or fewer characters; any number of array dimensions was allowed, unlike Fortran I’s limitation to no more than three; the lower bound of arrays could be specified by the programmer, whereas in Fortran it was implicitly 1; nested selection statements were allowed, which was not the case in Fortran I.
ALGOL 58 acquired the assignment operator in a rather unusual way. 
Zuse used the form
expression 
\=\> variable
for the assignment statement in Plankalkül. Although Plankalkül had not yet 
been published, some of the European members of the ALGOL 58 committee were familiar with the language. The committee dabbled with the Plankalkül assignment form but, because of arguments about character set limitations,
4 the 
greater\-than symbol was changed to a colon. Then, largely at the insistence of the Americans, the whole statement was turned around to the Fortran form
variable 
:\= expression
The Europeans preferred the opposite form, but that would be the reverse of 
Fortran.
 4\. The card punches of that time did not include the greater\-than symbol.2\.5\.4 Reception of the ALGOL 58 Report
In December 1958, publication of the ALGOL 58 report (Perlis and Samelson, 
1958\) was greeted with a good deal of enthusiasm. In the United States, the new language was viewed more as a collection of ideas for programming language design than as a universal standard language. Actually, the ALGOL 58 report was not meant to be a finished product but rather a preliminary document for international discussion. Nevertheless, three major design and implementation efforts used the report as their basis. At the University of Michigan, the MAD language was born (Arden et al., 1961\). The U.S. Naval Electronics Group pro\-duced the NELIAC language (Huskey et al., 1963\). At System Development Corporation, JOVIAL was designed and implemented (Shaw, 1963\). JOVIAL, an acronym for Jules’ Own Version of the International Algebraic Language, represents the only language based on ALGOL 58 to achieve widespread use ( Jules was Jules I. Schwartz, one of JOVIAL ’s designers). JOVIAL became widely used because it was the official scientific language for the U.S. Air Force for a quarter century.
The rest of the U.S. computing community was not so kind to the new lan\-
guage. At first, both IBM and its major scientific user group, SHARE, seemed to embrace ALGOL 58\. IBM began an implementation shortly after the report was published, and SHARE formed a subcommittee, SHARE IAL, to study the language. The subcommittee subsequently recommended that ACM standard\-ize ALGOL 58 and that IBM implement it for all of the 700\-series computers. The enthusiasm was short\-lived, however. By the spring of 1959, both IBM and SHARE, through their Fortran experience, had had enough of the pain and expense of getting a new language started, both in terms of developing and using the first\-generation compilers and in terms of training users in the new language and persuading them to use it. By the middle of 1959, both IBM and SHARE had developed such a vested interest in Fortran that they decided to retain it as the scientific language for the IBM 700\-series machines, thereby 
abandoning ALGOL 58\.
2\.5\.5 ALGOL 60 Design Process
During 1959, ALGOL 58 was furiously debated in both Europe and the United States. Large numbers of suggested modifications and additions were published in the European ALGOL Bulletin and in Communications of the ACM. One of the 
most important events of 1959 was the presentation of the work of the Zurich committee to the International Conference on Information Processing, for there Backus introduced his new notation for describing the syntax of program\-ming languages, which later became known as BNF (Backus\-Naur form). BNF is described in detail in Chapter 3\.
In January 1960, the second ALGOL meeting was held, this time in Paris. 
The purpose of the meeting was to debate the 80 suggestions that had been formally submitted for consideration. Peter Naur of Denmark had become heavily involved in the development of ALGOL, even though he had not been 2\.5 The First Step Toward Sophistication: ALGOL 60 5556 Chapter 2 Evolution of the Major Programming Languages
a member of the Zurich group. It was Naur who created and published the 
ALGOL Bulletin. He spent a good deal of time studying Backus’s paper that 
introduced BNF and decided that BNF should be used to describe formally the results of the 1960 meeting. After making a few relatively minor changes to BNF , he wrote a description of the new proposed language in BNF and handed it out to the members of the 1960 group at the beginning of the meeting.
2\.5\.6 ALGOL 60 Overview
Although the 1960 meeting lasted only six days, the modifications made to ALGOL 58 were dramatic. Among the most important new developments were the following:
• The concept of block structure was introduced. This allowed the program\-
mer to localize parts of programs by introducing new data environments, or scopes.
• T wo different means of passing parameters to subprograms were allowed: 
pass by value and pass by name.
• Procedures were allowed to be recursive. The ALGOL 58 description was 
unclear on this issue. Note that although this recursion was new for the imperative languages, LISP had already provided recursive functions in 1959\.
• Stack\-dynamic arrays were allowed. A stack\-dynamic array is one for which 
the subscript range or ranges are specified by variables, so that the size of the array is set at the time storage is allocated to the array, which happens when the declaration is reached during execution. Stack\-dynamic arrays are described in detail in Chapter 6\.
Several features that might have had a dramatic impact on the success or 
failure of the language were proposed and rejected. Most important among these were input and output statements with formatting, which were omitted because they were thought to be machine\-dependent.
The ALGOL 60 report was published in May 1960 (Naur, 1960\). A num\-
ber of ambiguities still remained in the language description, and a third meet\-ing was scheduled for April 1962 in Rome to address the problems. At this meeting the group dealt only with problems; no additions to the language were allowed. The results of this meeting were published under the title “Revised Report on the Algorithmic Language ALGOL 60” (Backus et al., 1963\).
2\.5\.7 Evaluation
In some ways, ALGOL 60 was a great success; in other ways, it was a dismal failure. It succeeded in becoming, almost immediately, the only acceptable formal means of communicating algorithms in computing literature, and it remained that for more than 20 years. Every imperative programming language designed since 1960 owes something to ALGOL 60\. In fact, most are direct or indirect descendants; examples include PL/I, SIMULA 67, ALGOL 68, C, 
Pascal, Ada, C\+\+, Java, and C\#.
The ALGOL 58/ALGOL 60 design effort included a long list of firsts. It 
was the first time that an international group attempted to design a program\-ming language. It was the first language that was designed to be machine inde\-pendent. It was also the first language whose syntax was formally described. This successful use of the BNF formalism initiated several important fields of computer science: formal languages, parsing theory, and BNF\-based compiler design. Finally, the structure of ALGOL 60 affected machine architecture. In the most striking example of this, an extension of the language was used as the systems language of a series of large\-scale computers, the Burroughs B5000, B6000, and B7000 machines, which were designed with a hardware stack to implement efficiently the block structure and recursive subprograms of the language.
On the other side of the coin, ALGOL 60 never achieved widespread use 
in the United States. Even in Europe, where it was more popular than in the United States, it never became the dominant language. There are a number of reasons for its lack of acceptance. For one thing, some of the features of ALGOL 60 turned out to be too flexible; they made understanding difficult and implementation inefficient. The best example of this is the pass\-by\-name method of passing parameters to subprograms, which is explained in Chapter 9\. The difficulties of implementing ALGOL 60 are evidenced by Rutishauser’s statement in 1967 that few, if any, implementations included the full ALGOL 60 language (Rutishauser, 1967, p. 8\).
The lack of input and output statements in the language was another major 
reason for its lack of acceptance. Implementation\-dependent input/output made programs difficult to port to other computers.
Ironically, one of the most important contributions to computer science 
associated with ALGOL 60, BNF , was also a factor in its lack of acceptance. Although BNF is now considered a simple and elegant means of syntax descrip\-tion, in 1960 it seemed strange and complicated.
Finally, although there were many other problems, the entrenchment of 
Fortran among users and the lack of support by IBM were probably the most important factors in ALGOL 60’s failure to gain widespread use.
The ALGOL 60 effort was never really complete, in the sense that ambi\-
guities and obscurities were always a part of the language description (Knuth, 1967\).
The following is an example of an ALGOL 60 program:
comment ALGOL 60 Example Program
 Input: An integer, listlen, where listlen is less than
 100, followed by listlen\-integer values
 Output: The number of input values that are greater than
 the average of all the input values ;
begin integer array intlist \[1:99];2\.5 The First Step Toward Sophistication: ALGOL 60 5758 Chapter 2 Evolution of the Major Programming Languages
 integer listlen, counter, sum, average, result;
 sum :\= 0; result :\= 0; readint (listlen); if (listlen \> 0\) ∧ (listlen \< 100\) then
 begin
comment Read input into an array and compute the average;
 for counter :\= 1 step 1 until listlen do
 begin
 readint (intlist\[counter]);
 sum :\= sum \+ intlist\[counter]
 end;
comment Compute the average;
 average :\= sum / listlen;comment Count the input values that are \> average;
 for counter :\= 1 step 1 until listlen do
 if intlist\[counter] \> average
 then result :\= result \+ 1;
comment Print result;
 printstring("The number of values \> average is:"); printint (result) end
 else
 printstring ("Error—input list length is not legal";
end
2\.6 Computerizing Business Records: COBOL
The story of COBOL is, in a sense, the opposite of that of ALGOL 60\. Although 
it has been used more than any other programming language, COBOL has had little effect on the design of subsequent languages, except for PL/I. It may still be the most widely used language,
5 although it is difficult to be sure one 
way or the other. Perhaps the most important reason why COBOL has had little influence is that few have attempted to design a new language for busi\-ness applications since it appeared. That is due in part to how well COBOL ’s capabilities meet the needs of its application area. Another reason is that a great deal of growth in business computing over the past 30 years has occurred in small businesses. In these businesses, very little software development has taken place. Instead, most of the software used is purchased as off\-the\-shelf packages for various general business applications.
 5\. In the late 1990s, in a study associated with the Y2K problem, it was estimated that there 
were approximately 800 million lines of COBOL in use in the 22 square miles of Manhattan.2\.6\.1 Historical Background
The beginning of COBOL is somewhat similar to that of ALGOL 60, in the 
sense that the language was designed by a committee of people meeting for relatively short periods of time. At the time, in 1959, the state of business computing was similar to the state of scientific computing several years earlier, when Fortran was being designed. One compiled language for business appli\-cations, FLOW\-MATIC, had been implemented in 1957, but it belonged to one manufacturer, UNIVAC, and was designed for that company’s computers. Another language, AIMACO, was being used by the U.S. Air Force, but it was only a minor variation of FLOW\-MATIC. IBM had designed a programming language for business applications, COMTRAN (COMmercial TRANslator), but it had not yet been implemented. Several other language design projects were being planned.
2\.6\.2 FLOW\-MATIC
The origins of FLOW\-MATIC are worth at least a brief discussion, because it was the primary progenitor of COBOL. In December 1953, Grace Hopper at Remington\-Rand UNIVAC wrote a proposal that was indeed prophetic. It suggested that “mathematical programs should be written in mathematical notation, data processing programs should be written in English statements” (Wexelblat, 1981, p. 16\). Unfortunately, in 1953, it was impossible to convince nonprogrammers that a computer could be made to understand English words. It was not until 1955 that a similar proposal had some hope of being funded by UNIVAC management, and even then it took a prototype system to do the final convincing. Part of this selling process involved compiling and running a small program, first using English keywords, then using French keywords, and then using German keywords. This demonstration was considered remarkable by UNIVAC management and was instrumental in their acceptance of Hop\-per’s proposal.
2\.6\.3 COBOL Design Process
The first formal meeting on the subject of a common language for business applications, which was sponsored by the Department of Defense, was held at the Pentagon on May 28 and 29, 1959 (exactly one year after the Zurich ALGOL meeting). The consensus of the group was that the language, then named CBL (Common Business Language), should have the following general characteristics: Most agreed that it should use English as much as possible, although a few argued for a more mathematical notation. The language must be easy to use, even at the expense of being less powerful, in order to broaden the base of those who could program computers. In addition to making the language easy to use, it was believed that the use of English would allow man\-agers to read programs. Finally, the design should not be overly restricted by the problems of its implementation.2\.6 Computerizing Business Records: COBOL 5960 Chapter 2 Evolution of the Major Programming Languages
One of the overriding concerns at the meeting was that steps to create this 
universal language should be taken quickly, as a lot of work was already being done to create other business languages. In addition to the existing languages, RCA and Sylvania were working on their own business applications languages. It was clear that the longer it took to produce a universal language, the more difficult it would be for the language to become widely used. On this basis, it was decided that there should be a quick study of existing languages. For this task, the Short Range Committee was formed.
There were early decisions to separate the statements of the language into 
two categories—data description and executable operations—and to have state\-ments in these two categories be in different parts of programs. One of the debates of the Short Range Committee was over the inclusion of subscripts. Many com\-mittee members argued that subscripts were too complex for the people in data processing, who were thought to be uncomfortable with mathematical notation. Similar arguments revolved around whether arithmetic expressions should be included. The final report of the Short Range Committee, which was completed in December 1959, described the language that was later named COBOL 60\.
The language specifications for COBOL 60, published by the Government 
Printing Office in April 1960 (Department of Defense, 1960\), were described as “initial.” Revised versions were published in 1961 and 1962 (Department of Defense, 1961, 1962\). The language was standardized by the American National Standards Institute (ANSI) group in 1968\. The next three revisions were standard\-ized by ANSI in 1974, 1985, and 2002\. The language continues to evolve today.
2\.6\.4 Evaluation
The COBOL language originated a number of novel concepts, some of which eventually appeared in other languages. For example, the 
DEFINE verb 
of COBOL 60 was the first high\-level language construct for macros. More important, hierarchical data structures (records), which first appeared in Plan\-kalkül, were first implemented in COBOL. They have been included in most of the imperative languages designed since then. COBOL was also the first language that allowed names to be truly connotative, because it allowed both long names (up to 30 characters) and word\-connector characters (hyphens).
Overall, the data division is the strong part of COBOL ’s design, whereas 
the procedure division is relatively weak. Every variable is defined in detail in the data division, including the number of decimal digits and the location of the implied decimal point. File records are also described with this level of detail, as are lines to be output to a printer, which makes COBOL ideal for printing accounting reports. Perhaps the most important weakness of the original pro\-cedure division was in its lack of functions. Versions of COBOL prior to the 1974 standard also did not allow subprograms with parameters.
Our final comment on COBOL: It was the first programming language 
whose use was mandated by the Department of Defense (DoD). This mandate came after its initial development, because COBOL was not designed specifi\-cally for the DoD. In spite of its merits, COBOL probably would not have survived without that mandate. The poor performance of the early compilers 
simply made the language too expensive to use. Eventually, of course, compilers became more efficient and computers became much faster and cheaper and had much larger memories. T ogether, these factors allowed COBOL to succeed, inside and outside DoD. Its appearance led to the electronic mechanization of accounting, an important revolution by any measure.
The following is an example of a COBOL program. This program reads 
a file named 
BAL\-FWD\-FILE that contains inventory information about a 
certain collection of items. Among other things, each item record includes the number currently on hand (
BAL\-ON\-HAND ) and the item’s reorder point 
 (BAL\-REORDER\-POINT ). The reorder point is the threshold number of items 
on hand at which more must be ordered. The program produces a list of items that must be reordered as a file named 
REORDER\-LISTING .
IDENTIFICATION DIVISION.
PROGRAM\-ID. PRODUCE\-REORDER\-LISTING.
ENVIRONMENT DIVISION.
CONFIGURATION SECTION.
SOURCE\-COMPUTER. DEC\-VAX.
OBJECT\-COMPUTER. DEC\-VAX.
INPUT\-OUTPUT SECTION.
FILE\-CONTROL.
 SELECT BAL\-FWD\-FILE ASSIGN TO READER.
 SELECT REORDER\-LISTING ASSIGN TO LOCAL\-PRINTER.
DATA DIVISION.
FILE SECTION.
FD BAL\-FWD\-FILE
 LABEL RECORDS ARE STANDARD
 RECORD CONTAINS 80 CHARACTERS.
01 BAL\-FWD\-CARD.
 02 BAL\-ITEM\-NO PICTURE IS 9(5\).
 02 BAL\-ITEM\-DESC PICTURE IS X(20\).
 02 FILLER PICTURE IS X(5\).
 02 BAL\-UNIT\-PRICE PICTURE IS 999V99\.
 02 BAL\-REORDER\-POINT PICTURE IS 9(5\).
 02 BAL\-ON\-HAND PICTURE IS 9(5\).
 02 BAL\-ON\-ORDER PICTURE IS 9(5\).
 02 FILLER PICTURE IS X(30\).
FD REORDER\-LISTING
 LABEL RECORDS ARE STANDARD
 RECORD CONTAINS 132 CHARACTERS.
01 REORDER\-LINE.2\.6 Computerizing Business Records: COBOL 6162 Chapter 2 Evolution of the Major Programming Languages
 02 RL\-ITEM\-NO PICTURE IS Z(5\).
 02 FILLER PICTURE IS X(5\). 02 RL\-ITEM\-DESC PICTURE IS X(20\). 02 FILLER PICTURE IS X(5\). 02 RL\-UNIT\-PRICE PICTURE IS ZZZ.99\.
 02 FILLER PICTURE IS X(5\).
 02 RL\-AVAILABLE\-STOCK PICTURE IS Z(5\).
 02 FILLER PICTURE IS X(5\).
 02 RL\-REORDER\-POINT PICTURE IS Z(5\).
 02 FILLER PICTURE IS X(71\).
WORKING\-STORAGE SECTION.
01 SWITCHES.
 02 CARD\-EOF\-SWITCH PICTURE IS X.01 WORK\-FIELDS. 02 AVAILABLE\-STOCK PICTURE IS 9(5\).
PROCEDURE DIVISION.
000\-PRODUCE\-REORDER\-LISTING.
 OPEN INPUT BAL\-FWD\-FILE. OPEN OUTPUT REORDER\-LISTING. MOVE "N" TO CARD\-EOF\-SWITCH. PERFORM 100\-PRODUCE\-REORDER\-LINE UNTIL CARD\-EOF\-SWITCH IS EQUAL TO "Y".
 CLOSE BAL\-FWD\-FILE.
 CLOSE REORDER\-LISTING.
 STOP RUN.
100\-PRODUCE\-REORDER\-LINE.
 PERFORM 110\-READ\-INVENTORY\-RECORD.
 IF CARD\-EOF\-SWITCH IS NOT EQUAL TO "Y"]
 PERFORM 120\-CALCULATE\-AVAILABLE\-STOCK
 IF AVAILABLE\-STOCK IS LESS THAN BAL\-REORDER\-POINT
 PERFORM 130\-PRINT\-REORDER\-LINE.
110\-READ\-INVENTORY\-RECORD.
 READ BAL\-FWD\-FILE RECORD
 AT END
 MOVE "Y" TO CARD\-EOF\-SWITCH.
120\-CALCULATE\-AVAILABLE\-STOCK.
ADD BAL\-ON\-HAND BAL\-ON\-ORDER
 GIVING AVAILABLE\-STOCK.
130\-PRINT\-REORDER\-LINE.
 MOVE SPACE TO REORDER\-LINE. MOVE BAL\-ITEM\-NO TO RL\-ITEM\-NO.
 MOVE BAL\-ITEM\-DESC TO RL\-ITEM\-DESC. MOVE BAL\-UNIT\-PRICE TO RL\-UNIT\-PRICE. MOVE AVAILABLE\-STOCK TO RL\-AVAILABLE\-STOCK. MOVE BAL\-REORDER\-POINT TO RL\-REORDER\-POINT.
 WRITE REORDER\-LINE.
2\.7 The Beginnings of Timesharing: BASIC
BASIC (Mather and Waite, 1971\) is another programming language that 
has enjoyed widespread use but has gotten little respect. Like COBOL, it has largely been ignored by computer scientists. Also, like COBOL, in its earliest versions it was inelegant and included only a meager set of control statements.
BASIC was very popular on microcomputers in the late 1970s and early 
1980s. This followed directly from two of the main characteristics of early ver\-sions of BASIC. It was easy for beginners to learn, especially those who were not science oriented, and its smaller dialects can be implemented on comput\-ers with very small memories.
6 When the capabilities of microcomputers grew 
and other languages were implemented, the use of BASIC waned. A strong resurgence in the use of BASIC began with the appearance of Visual Basic (Microsoft, 1991\) in the early 1990s.
2\.7\.1 Design Process
BASIC (Beginner’s All\-purpose Symbolic Instruction Code) was originally designed at Dartmouth College (now Dartmouth University) in New Hamp\-shire by two mathematicians, John Kemeny and Thomas Kurtz, who, in the early 1960s, developed compilers for a variety of dialects of Fortran and ALGOL 60\. Their science students generally had little trouble learning or using those languages in their studies. However, Dartmouth was primarily a liberal arts institution, where science and engineering students made up only about 25 percent of the student body. It was decided in the spring of 1963 to design a new language especially for liberal arts students. This new language would use terminals as the method of computer access. The goals of the system were as follows:
 1\. It must be easy for nonscience students to learn and use.
 2\. It must be “pleasant and friendly.” 3\. It must provide fast turnaround for homework.
 6\. Some early microcomputers included BASIC interpreters that resided in 4096 bytes of 
ROM.2\.7 The Beginnings of Timesharing: BASIC 6364 Chapter 2 Evolution of the Major Programming Languages
 4\. It must allow free and private access.
 5\. It must consider user time more important than computer time.
The last goal was indeed a revolutionary concept. It was based at least partly 
on the belief that computers would become significantly cheaper as time went on, which of course they did.
The combination of the second, third, and fourth goals led to the time\-
shared aspect of BASIC. Only with individual access through terminals by numerous simultaneous users could these goals be met in the early 1960s.
In the summer of 1963, Kemeny began work on the compiler for the first 
version of BASIC, using remote access to a GE 225 computer. Design and coding of the operating system for BASIC began in the fall of 1963\. At 4:00 
A.M. on May 1, 1964, the first program using the timeshared BASIC was typed 
in and run. In June, the number of terminals on the system grew to 11, and by the fall it had ballooned to 20\.
2\.7\.2 Language Overview
The original version of BASIC was very small and, oddly, was not interactive: There was no way for an executing program to get input data from the user. Programs were typed in, compiled, and run, in a sort of batch\-oriented way. The original BASIC had only 14 different statement types and a single data type—floating\-point. Because it was believed that few of the targeted users would appreciate the difference between integer and floating\-point types, the type was referred to as “numbers.” Overall, it was a very limited language, though quite easy to learn.
2\.7\.3 Evaluation
The most important aspect of the original BASIC was that it was the first widely used language that was used through terminals connected to a remote computer.
7 T erminals had just begun to be available at that time. Before then, 
most programs were entered into computers through either punched cards or paper tape.
Much of the design of BASIC came from Fortran, with some minor influ\-
ence from the syntax of ALGOL 60\. Later, it grew in a variety of ways, with little or no effort made to standardize it. The American National Standards Institute issued a Minimal BASIC standard (ANSI, 1978b), but this represented only the bare minimum of language features. In fact, the original BASIC was very similar to Minimal BASIC.
Although it may seem surprising, Digital Equipment Corporation used a 
rather elaborate version of BASIC named BASIC\-PLUS to write significant 
 7\. LISP initially was used through terminals, but it was not widely used in the early 1960s.portions of their largest operating system for the PDP\-11 minicomputers, 
RSTS, in the 1970s.
BASIC has been criticized for the poor structure of programs written in 
it, among other things. By the evaluation criteria discussed in Chapter 1, spe\-cifically readability and reliability, the language does indeed fare very poorly. Clearly, the early versions of the language were not meant for and should not have been used for serious programs of any significant size. Later versions are much better suited to such tasks.
The resurgence of BASIC in the 1990s was driven by the appearance of 
Visual BASIC (VB). VB became widely used in large part because it provided a simple way of building graphical user interfaces (GUIs), hence the name Visual BASIC. Visual Basic .NET, or just VB.NET, is one of Microsoft’s .NET languages. Although it is a significant departure from VB, it quickly displaced the older language. Perhaps the most important difference between VB and VB.NET is that VB.NET fully supports object\-oriented programming.
The following is an example of a BASIC program:
REM BASIC Example Program
REM Input: An integer, listlen, where listlen is less REM than 100, followed by listlen\-integer valuesREM Output: The number of input values that are greater 
REM than the average of all input values
 DIM intlist(99\)
 result \= 0
 sum \= 0
 INPUT listlen
 IF listlen \> 0 AND listlen \< 100 THEN
REM Read input into an array and compute the sum 
 FOR counter \= 1 TO listlen
 INPUT intlist(counter)
 sum \= sum \+ intlist(counter)
 NEXT counter
REM Compute the average
 average \= sum / listlen
REM Count the number of input values that are \> average
 FOR counter \= 1 TO listlen
 IF intlist(counter) \> average 
 THEN result \= result \+ 1
 NEXT counter
REM Print the result
 PRINT "The number of values that are \> average is:";
 result
 ELSE
 PRINT "Error—input list length is not legal"
 END IFEND2\.7 The Beginnings of Timesharing: BASIC 65interview
User Design and Language Design
ALAN COOPER
Best\-selling author of About Face: The Essentials of User Interface Design, Alan 
Cooper also had a large hand in designing what can be touted as the language with the most concern for user interface design, Visual Basic. For him, it all comes down to a vision for humanizing technology.
SOME INFORMATION ON THE BASICS
How did you get started in all of this? I’m a high 
school dropout with an associate degree in program\-ming from a California community college. My first job was as a programmer for American President Lines (one of the United States’ oldest ocean transportation companies) in San Francisco. Except for a few months here and there, I’ve remained self\-employed.
What is your current job? Founder and chairman 
of Cooper , the company that humanizes technology (www.cooper .com).
What is or was your favorite job? Interaction 
design consultant.
You are very well known in the fields of lan\-
guage design and user interface design. Any thoughts on designing languages versus design\-ing software, versus designing anything else?
 It’s 
pretty much the same in the world of software: Know your user .
ABOUT THAT EARLY WINDOWS RELEASE
In the 1980s, you started using Windows and 
have talked about being lured by its plusses: the graphical user interface support and the dynami\-cally linked library that let you create tools that configured themselves. What about the parts of Windows that you eventually helped shape?
 I was 
very impressed by Microsoft’s inclusion of support for practical multitasking in Windows. This included dynamic relocation and interprocess communications.MSDOS.exe was the shell program for the first few 
releases of Windows. It was a t errible program, and I 
believed that it could be improved dramatically, and I was the guy to do it. In my spare time, I immediately began to write a better shell program than the one Windows came with. I called it Tripod. Microsoft’s original shell, MSDOS.exe, was one of the main stum\-bling blocks to the initial success of Windows. Tripod attempted to solve the problem by being easier to use and to configure.
When was that “Aha!” moment? It wasn’t until 
late in 1987, when I was interviewing a corporate cli\-ent, that the key design strategy for Tripod popped into my head. As the IS manager explained to me his need to create and publish a wide range of shell solutions to his disparate user base, I realized the conundrum that there is no such thing as an ideal shell. Every user would need their own personal shell, configured to their own needs and skill levels. In an instant, I perceived the solution to the shell design problem: It would be a shell construction set; a tool where each user would be able to construct exactly the shell that he or she needed for a unique mix of applications and training.
What is so compelling about the idea of a shell 
that can be individualized? Instead of me telling 
the users what the ideal shell was, they could design their own, personalized ideal shell. With a customiz\-able shell, a programmer would create a shell that was powerful and wide ranging but also somewhat danger\-ous, whereas an IT manager would create a shell that could be given to a desk clerk that exposed only those few application\-specific tools that the clerk used.
66How did you get from writing 
a shell program to collabo\-rating with Microsoft?
 Tripod 
and Ruby are the same thing. After I signed a deal with Bill Gates, I changed the name of the prototype from Tripod to Ruby. I then used the Ruby prototype as prototypes should be used: as a disposable model for constructing release\-quality code. Which is what I did. MS took the release version of Ruby and added QuickBASIC to it, creating VB. All of those original innovations were in Tripod/Ruby.
RUBY AS THE INCUBATOR FOR VISUAL BASIC
Let’s revisit your interest in early Windows and 
that DLL feature. The DLL wasn’t a thing, it was a 
facility in the OS. It allowed a programmer to build code objects that could be linked to at run time as opposed to only at compile time. This is what allowed me to invent the dynamically extensible parts of VB, where controls can be added by third\-party vendors.
The Ruby product embodied many significant 
advances in software design, but two of them stand out as exceptionally successful. As I mentioned, the dynamic linking capability of Windows had always intrigued me, but having the tools and knowing what to do with them were two different things. With Ruby, I finally found two practical uses for dynamic linking, and the original program contained both. First, the language was both installable and could be extended dynamically. Second, the palette of gizmos could be added to dynamically.
Was your language in Ruby the first to have a 
dynamic linked library and to be linked to a visual front end?
 As far as I know, yes.
Using a simple example, what would this enable a programmer to do with his or her program?
 Pur\-
chase a control, such as a grid control, from a third\-
party vendor , install it on his or her computer , and have the grid control appear as an integral part of the lan\-
guage, including the visual programming front end.Why do they call you “the father of Visual 
Basic”? Ruby came with a small language, one suited 
only for executing the dozen or so simple commands that a shell program needs. However , this language was implemented as a chain of DLLs, any number of which could be installed at run time. The internal parser would identify a verb and then pass it along the chain of DLLs until one of them acknowledged that it knew how to process the verb. If all of the DLLs passed, there was a syntax error. From our earliest discussions, 
both Microsoft and I had entertained the idea of grow\-
ing the language, possibly even replacing it altogether with a “real” language. C was the candidate most frequently mentioned, but eventually, Microsoft took advantage of this dynamic interface to unplug our little shell language and replace it entirely with Quick\-BASIC. This new marriage of language to visual front end was static and permanent, and although the origi\-nal dynamic interface made the coupling possible, it was lost in the process.
SOME FINAL COMMENTS ON NEW IDEAS
In the world of programming and programming 
tools, including languages and environments, what projects most interest you?
 I’m interested in 
creating programming tools that are designed to help users instead of programmers.
What’s the most critical rule, famous quote, or 
design idea to keep in mind? Bridges are not built 
by engineers. They are built by ironwo rkers.
Similarly, software programs are not built by engi\-
neers. They are built by programmers.“MSDOS.exe was the shell program for the first few 
releases of Windows. It was a terrible program, and I believed that it could be imp roved dramatically, 
and I was the guy to do it. In my spare time, I immediately began to write a better shell program than the one Windows came with.
”
6768 Chapter 2 Evolution of the Major Programming Languages
2\.8 Everything for Everybody: PL/I
PL/I represents the first large\-scale attempt to design a language that could 
be used for a broad spectrum of application areas. All previous and most sub\-sequent languages have focused on one particular application area, such as science, artificial intelligence, or business.
2\.8\.1 Historical Background
Like Fortran, PL/I was developed as an IBM product. By the early 1960s, the users of computers in industry had settled into two separate and quite dif\-ferent camps: scientific and business. From the IBM point of view, scientific programmers could use either the large\-scale 7090 or the small\-scale 1620 IBM computers. This group used floating\-point data and arrays extensively. Fortran was the primary language, although some assembly language was also used. They had their own user group, SHARE, and had little contact with anyone who worked on business applications.
For business applications, people used the large 7080 or the small 1401 
IBM computers. They needed the decimal and character string data types, as well as elaborate and efficient input and output facilities. They used COBOL, although in early 1963 when the PL/I story begins, the conversion from assem\-bly language to COBOL was far from complete. This category of users also had its own user group, GUIDE, and seldom had contact with scientific users.
In early 1963, IBM planners perceived the beginnings of a change in this 
situation. The two widely separated computer user groups were moving toward each other in ways that were thought certain to create problems. Scientists began to gather large files of data to be processed. This data required more sophisticated and more efficient input and output facilities. Business applica\-tions people began to use regression analysis to build management information systems, which required floating\-point data and arrays. It began to appear that computing installations would soon require two separate computers and techni\-cal staffs, supporting two very different programming languages.
8
These perceptions naturally led to the concept of designing a single univer\-
sal computer that would be capable of doing both floating\-point and decimal arithmetic, and therefore both scientific and business applications. Thus was born the concept of the IBM System/360 line of computers. Along with this came the idea of a programming language that could be used for both business and scientific applications. For good measure, features to support systems pro\-gramming and list processing were thrown in. Therefore, the new language was to replace Fortran, COBOL, LISP , and the systems applications of assembly language.
 8\. At the time, large computer installations required both full\-time hardware and full\-time sys\-
tem software maintenance staff.2\.8\.2 Design Process
The design effort began when IBM and SHARE formed the Advanced Lan\-
guage Development Committee of the SHARE Fortran Project in October 1963\. This new committee quickly met and formed a subcommittee called the 3 × 3 Committee, so named because it had three members from IBM and three from SHARE. The 3 × 3 Committee met for three or four days every other week to design the language.
As with the Short Range Committee for COBOL, the initial design was 
scheduled for completion in a remarkably short time. Apparently, regardless of the scope of a language design effort, in the early 1960s the prevailing belief was that it could be done in three months. The first version of PL/I, which was then named Fortran VI, was supposed to be completed by December, less than three months after the committee was formed. The committee pleaded successfully on two different occasions for extensions, moving the due date back to January and then to late February 1964\.
The initial design concept was that the new language would be an exten\-
sion of Fortran IV , maintaining compatibility, but that goal was dropped quickly along with the name Fortran VI. Until 1965, the language was known as NPL (New Programming Language). The first published report on NPL was given at the SHARE meeting in March 1964\. A more complete descrip\-tion followed in April, and the version that would actually be implemented was published in December 1964 (IBM, 1964\) by the compiler group at the IBM Hursley Laboratory in England, which was chosen to do the imple\-mentation. In 1965, the name was changed to PL/I to avoid the confusion of the name NPL with the National Physical Laboratory in England. If the compiler had been developed outside the United Kingdom, the name might have remained NPL.
2\.8\.3 Language Overview
Perhaps the best single\-sentence description of PL/I is that it included what were then considered the best parts of ALGOL 60 (recursion and block struc\-ture), Fortran IV (separate compilation with communication through global data), and COBOL 60 (data structures, input/output, and report\-generating facilities), along with an extensive collection of new constructs, all somehow cobbled together. Because PL/I is no longer a popular language, we will not attempt, even briefly, to discuss all the features of the language, or even its most controversial constructs. Instead, we will mention some of the lan\-guage’s contributions to the pool of knowledge of programming languages.
PL/I was the first programming language to have the following facilities:
• Programs were allowed to create concurrently executing subprograms. 
Although this was a good idea, it was poorly developed in PL/I.
• It was possible to detect and handle 23 different types of exceptions, or 
run\-time errors.2\.8 Everything for Everybody: PL/I 6970 Chapter 2 Evolution of the Major Programming Languages
• Subprograms were allowed to be used recursively, but the capability could 
be disabled, allowing more efficient linkage for nonrecursive subprograms.
• Pointers were included as a data type.• Cross\-sections of arrays could be referenced. For example, the third row 
of a matrix could be referenced as if it were a single\-dimensioned array.
2\.8\.4 Evaluation
Any evaluation of PL/I must begin by recognizing the ambitiousness of the design effort. In retrospect, it appears naive to think that so many constructs could have been combined successfully. However, that judgment must be tem\-pered by acknowledging that there was little language design experience at the time. Overall, the design of PL/I was based on the premise that any construct that was useful and could be implemented should be included, with insufficient concern about how a programmer could understand and make effective use of such a collection of constructs and features. Edsger Dijkstra, in his T uring Award Lecture (Dijkstra, 1972\), made one of the strongest criticisms of the complexity of PL/I: “I absolutely fail to see how we can keep our growing programs firmly within our intellectual grip when by its sheer baroqueness the programming language—our basic tool, mind you!—already escapes our intellectual control.”
In addition to the problem with the complexity due to its large size, PL/I 
suffered from a number of what are now considered to be poorly designed constructs. Among these were pointers, exception handling, and concurrency, although we must point out that in all cases, these constructs had not appeared in any previous language.
In terms of usage, PL/I must be considered at least a partial success. In the 
1970s, it enjoyed significant use in both business and scientific applications. It was also widely used during that time as an instructional vehicle in colleges, primarily in several subset forms, such as PL/C (Cornell, 1977\) and PL/CS (Conway and Constable, 1976\).
The following is an example of a PL/I program:
/\* PL/I PROGRAM EXAMPLE
 INPUT: AN INTEGER, LISTLEN, WHERE LISTLEN IS LESS THAN
 100, FOLLOWED BY LISTLEN\-INTEGER VALUES
 OUTPUT: THE NUMBER OF INPUT VALUES THAT ARE GREATER THAN
 THE AVERAGE OF ALL INPUT VALUES \*/
PLIEX: PROCEDURE OPTIONS (MAIN);
 DECLARE INTLIST (1:99\) FIXED.
 DECLARE (LISTLEN, COUNTER, SUM, AVERAGE, RESULT) FIXED;
 SUM \= 0;
 RESULT \= 0;
 GET LIST (LISTLEN); IF (LISTLEN \> 0\) \& (LISTLEN \< 100\) THEN DO;
/\* READ INPUT DATA INTO AN ARRAY AND COMPUTE THE SUM \*/ DO COUNTER \= 1 TO LISTLEN; GET LIST (INTLIST (COUNTER)); SUM \= SUM \+ INTLIST (COUNTER);
 END;
/\* COMPUTE THE AVERAGE \*/
 AVERAGE \= SUM / LISTLEN;
/\* COUNT THE NUMBER OF VALUES THAT ARE \> AVERAGE \*/
 DO COUNTER \= 1 TO LISTLEN;
 IF INTLIST (COUNTER) \> AVERAGE THEN
 RESULT \= RESULT \+ 1;
 END;
/\* PRINT RESULT \*/ PUT SKIP LIST ('THE NUMBER OF VALUES \> AVERAGE IS:'); PUT LIST (RESULT);
 END;
 ELSE
 PUT SKIP LIST ('ERROR—INPUT LIST LENGTH IS ILLEGAL');
 END PLIEX; 
2\.9 Two Early Dynamic Languages: APL and SNOBOL
The structure of this section is different from that of the other sections because 
the languages discussed here are very different. Neither APL nor SNOBOL had much influence on later mainstream languages.
9 Some of the interesting 
features of APL are discussed later in the book.
In appearance and in purpose, APL and SNOBOL are quite different. 
They share two fundamental characteristics, however: dynamic typing and dynamic storage allocation. Variables in both languages are essentially untyped. A variable acquires a type when it is assigned a value, at which time it assumes the type of the value assigned. Storage is allocated to a variable only when it is assigned a value, because before that there is no way to know the amount of storage that will be needed.
2\.9\.1 Origins and Characteristics of APL
APL (Brown et al., 1988\) was designed around 1960 by Kenneth E. Iverson at IBM. It was not originally designed to be an implemented programming language but rather was intended to be a vehicle for describing computer architecture. 
 9\. However, they have some influence on some nonmainstream languages ( J is based on APL, 
ICON is based on SNOBOL, and AWK is partially based on SNOBOL).2\.9 Two Early Dynamic Languages: APL and SNOBOL 7172 Chapter 2 Evolution of the Major Programming Languages
APL was first described in the book from which it gets its name, A Programming 
Language (Iverson, 1962\). In the mid\-1960s, the first implementation of APL 
was developed at IBM.
APL has a large number of powerful operators that are specified with a 
large number of symbols, which created a problem for implementors. Initially, APL was used through IBM printing terminals. These terminals had special print balls that provided the odd character set required by the language. One reason APL has so many operators is that it provides a large number of unit operations on arrays. For example, the transpose of any matrix is done with a single operator. The large collection of operators provides very high expressiv\-ity but also makes APL programs difficult to read. Therefore, people think of APL as a language that is best used for “throw\-away” programming. Although programs can be written quickly, they should be discarded after use because they are difficult to maintain.
APL has been around for nearly 50 years and is still used today, although 
not widely. Furthermore, it has not changed a great deal over its lifetime.
2\.9\.2 Origins and Characteristics of SNOBOL
SNOBOL (pronounced “snowball”; Griswold et al., 1971\) was designed in the early 1960s by three people at Bell Laboratories: D. J. Farber, R. E. Griswold, and I. P . Polonsky (Farber et al., 1964\). It was designed specifically for text processing. The heart of SNOBOL is a collection of powerful operations for string pattern matching. One of the early applications of SNOBOL was for writing text editors. Because the dynamic nature of SNOBOL makes it slower than alternative languages, it is no longer used for such programs. However, SNOBOL is still a live and supported language that is used for a variety of text\-processing tasks in several different application areas.
2\.10 The Beginnings of Data Abstraction: SIMULA 67
Although SIMULA 67 never achieved widespread use and had little impact on 
the programmers and computing of its time, some of the constructs it intro\-duced make it historically important.
2\.10\.1 Design Process
T wo Norwegians, Kristen Nygaard and Ole\-Johan Dahl, developed the lan\-guage SIMULA I between 1962 and 1964 at the Norwegian Computing Cen\-ter (NCC) in Oslo. They were primarily interested in using computers for simulation but also worked in operations research. SIMULA I was designed exclusively for system simulation and was first implemented in late 1964 on a UNIVAC 1107 computer.As soon as the SIMULA I implementation was completed, Nygaard and 
Dahl began efforts to extend the language by adding new features and modify\-ing some existing constructs in order to make the language useful for general\-purpose applications. The result of this work was SIMULA 67, whose design was first presented publicly in March 1967 (Dahl and Nygaard, 1967\). We will discuss only SIMULA 67, although some of the features of interest in SIMULA 67 are also in SIMULA I.
2\.10\.2 Language Overview
SIMULA 67 is an extension of ALGOL 60, taking both block structure and the control statements from that language. The primary deficiency of ALGOL 60 (and other languages at that time) for simulation applications was the design of its subprograms. Simulation requires subprograms that are allowed to restart at the position where they previously stopped. Subprograms with this kind of control are known as coroutines because the caller and called subprograms 
have a somewhat equal relationship with each other, rather than the rigid master/slave relationship they have in most imperative languages.
T o provide support for coroutines in SIMULA 67, the class construct was 
developed. This was an important development because the concept of data abstraction began with it. Furthermore, data abstraction provides the founda\-tion for object\-oriented programming.
It is interesting to note that the important concept of data abstraction was 
not developed and attributed to the class construct until 1972, when Hoare (1972\) recognized the connection.
2\.11 Orthogonal Design: ALGOL 68
ALGOL 68 was the source of several new ideas in language design, some of 
which were subsequently adopted by other languages. We include it here for that reason, even though it never achieved widespread use in either Europe or the United States.
2\.11\.1 Design Process
The development of the ALGOL family did not end when the revised report on ALGOL 60 appeared in 1962, although it was six years until the next design iteration was published. The resulting language, ALGOL 68 (van Wijngaarden et al., 1969\), was dramatically different from its predecessor.
One of the most interesting innovations of ALGOL 68 was one of its pri\-
mary design criteria: orthogonality. Recall our discussion of orthogonality in Chapter 1\. The use of orthogonality resulted in several innovative features of ALGOL 68, one of which is described in the following section.2\.11 Orthogonal Design: ALGOL 68 7374 Chapter 2 Evolution of the Major Programming Languages
2\.11\.2 Language Overview
One important result of orthogonality in ALGOL 68 was its inclusion of user\-
defined data types. Earlier languages, such as Fortran, included only a few basic data structures. PL/I included a larger number of data structures, which made it harder to learn and difficult to implement, but it obviously could not provide an appropriate data structure for every need.
The approach of ALGOL 68 to data structures was to provide a few primi\-
tive types and structures and allow the user to combine those primitives into a large number of different structures. This provision for user\-defined data types was carried over to some extent into all of the major imperative languages designed since then. User\-defined data types are valuable because they allow the user to design data abstractions that fit particular problems very closely. All aspects of data types are discussed in Chapter 6\.
As another first in the area of data types, ALGOL 68 introduced the 
kind of dynamic arrays that will be termed implicit heap\-dynamic in Chapter 5\. 
A dynamic array is one in which the declaration does not specify subscript bounds. Assignments to a dynamic array cause allocation of required storage. In ALGOL 68, dynamic arrays are called 
flex arrays.
2\.11\.3 Evaluation
ALGOL 68 includes a significant number of features that had not been previ\-ously used. Its use of orthogonality, which some may argue was overdone, was nevertheless revolutionary.
ALGOL 68 repeated one of the sins of ALGOL 60, however, and it was an 
important factor in its limited popularity. The language was described using an elegant and concise but also unknown metalanguage. Before one could read the language\-describing document (van Wijngaarden et al., 1969\), he or she had to learn the new metalanguage, called van Wijngaarden grammars, which were far more complex than BNF . T o make matters worse, the designers invented a collection of words to explain the grammar and the language. For example, keywords were called indicants, substring extraction was called trimming, and 
the process of a subprogram execution was called a coercion of deproceduring, 
which might be meek, firm, or something else.
It is natural to contrast the design of PL/I with that of ALGOL 68, because 
they appeared only a few years apart. ALGOL 68 achieved writability by the principle of orthogonality: a few primitive concepts and the unrestricted use of a few combining mechanisms. PL/I achieved writability by including a large number of fixed constructs. ALGOL 68 extended the elegant simplicity of ALGOL 60, whereas PL/I simply threw together the features of several lan\-guages to attain its goals. Of course, it must be remembered that the goal of PL/I was to provide a unified tool for a broad class of problems, whereas ALGOL 68 was targeted to a single class: scientific applications.
PL/I achieved far greater acceptance than ALGOL 68, due largely to IBM’s 
promotional efforts and the problems of understanding and implementing ALGOL 68\. Implementation was a difficult problem for both, but PL/I had 
the resources of IBM to apply to building a compiler. ALGOL 68 enjoyed no such benefactor.
2\.12 Some Early Descendants of the ALGOLs
All imperative languages owe some of their design to ALGOL 60 and/or 
ALGOL 68\. This section discusses some of the early descendants of these languages.
2\.12\.1 Simplicity by Design: Pascal
2\.12\.1\.1 Historical Background
Niklaus Wirth (Wirth is pronounced “Virt”) was a member of the International Federation of Information Processing (IFIP) Working Group 2\.1, which was created to continue the development of ALGOL in the mid\-1960s. In August 1965, Wirth and C. A. R. (“T ony”) Hoare contributed to that effort by present\-ing to the group a somewhat modest proposal for additions and modifications to ALGOL 60 (Wirth and Hoare, 1966\). The majority of the group rejected the proposal as being too small an improvement over ALGOL 60\. Instead, a much more complex revision was developed, which eventually became ALGOL 68\. Wirth, along with a few other group members, did not believe that the ALGOL 68 report should have been released, based on the complexity of both the lan\-guage and the metalanguage used to describe it. This position later proved to have some validity because the ALGOL 68 documents, and therefore the language, were indeed found to be challenging by the computing community.
The Wirth and Hoare version of ALGOL 60 was named ALGOL\-W. It 
was implemented at Stanford University and was used primarily as an instruc\-tional vehicle, but only at a few universities. The primary contributions of ALGOL\-W were the value\-result method of passing parameters and the 
case 
statement for multiple selection. The value\-result method is an alternative to ALGOL 60’s pass\-by\-name method. Both are discussed in Chapter 9\. The 
case statement is discussed in Chapter 8\.
Wirth’s next major design effort, again based on ALGOL 60, was his most 
successful: Pascal.10 The original published definition of Pascal appeared in 
1971 (Wirth, 1971\). This version was modified somewhat in the implemen\-tation process and is described in Wirth (1973\). The features that are often ascribed to Pascal in fact came from earlier languages. For example, user\-defined data types were introduced in ALGOL 68, the 
case statement in 
ALGOL\-W, and Pascal’s records are similar to those of COBOL and PL/I.
 10\. Pascal is named after Blaise Pascal, a seventeenth\-century French philosopher and mathema\-
tician who invented the first mechanical adding machine in 1642 (among other things).2\.12 Some Early Descendants of the ALGOLs 7576 Chapter 2 Evolution of the Major Programming Languages
2\.12\.1\.2 Evaluation
The largest impact of Pascal was on the teaching of programming. In 1970, 
most students of computer science, engineering, and science were introduced to programming with Fortran, although some universities used PL/I, languages based on PL/I, and ALGOL\-W. By the mid\-1970s, Pascal had become the most widely used language for this purpose. This was quite natural, because Pascal was designed specifically for teaching programming. It was not until the late 1990s that Pascal was no longer the most commonly used language for teaching programming in colleges and universities.
Because Pascal was designed as a teaching language, it lacks several features 
that are essential for many kinds of applications. The best example of this is the impossibility of writing a subprogram that takes as a parameter an array of variable length. Another example is the lack of any separate compilation capability. These deficiencies naturally led to many nonstandard dialects, such as T urbo Pascal.
Pascal’s popularity, for both teaching programming and other applications, 
was based primarily on its remarkable combination of simplicity and expres\-sivity. Although there are some insecurities in Pascal, it is still a relatively safe language, particularly when compared with Fortran or C. By the mid\-1990s, the popularity of Pascal was on the decline, both in industry and in universi\-ties, primarily due to the rise of Modula\-2, Ada, and C\+\+, all of which included features not available in Pascal.
The following is an example of a Pascal program:
{Pascal Example Program
 Input: An integer, listlen, where listlen is less than
 100, followed by listlen\-integer values
 Output: The number of input values that are greater than 
 the average of all input values }
program pasex (input, output);
 type intlisttype \= array \[1\..99] of integer;
 var
 intlist : intlisttype;
 listlen, counter, sum, average, result : integer;
 begin
 result :\= 0;
 sum :\= 0;
 readln (listlen);
 if ((listlen \> 0\) and (listlen \< 100\)) then
 begin
{ Read input into an array and compute the sum }
 for counter :\= 1 to listlen do
 begin
 readln (intlist\[counter]); sum :\= sum \+ intlist\[counter] end;{ Compute the average }
 average :\= sum / listlen;{ Count the number of input values that are \> average } for counter :\= 1 to listlen do
 if (intlist\[counter] \> average) then
 result :\= result \+ 1;
{ Print the result }
 writeln ('The number of values \> average is:', 
 result)
 end { of the then clause of if (( listlen \> 0 ... }
 else
 writeln ('Error—input list length is not legal')
end.
2\.12\.2 A Portable Systems Language: C
Like Pascal, C contributed little to the previously known collection of language 
features, but it has been widely used over a long period of time. Although origi\-nally designed for systems programming, C is well suited for a wide variety of applications.
2\.12\.2\.1 Historical Background
C’s ancestors include CPL, BCPL, B, and ALGOL 68\. CPL was developed at Cambridge University in the early 1960s. BCPL is a simple systems language, also developed at Cambridge, this time by Martin Richards in 1967 (Richards, 1969\).
The first work on the UNIX operating system was done in the late 1960s by 
Ken Thompson at Bell Laboratories. The first version was written in assembly language. The first high\-level language implemented under UNIX was B, which was based on BCPL. B was designed and implemented by Thompson in 1970\.
Neither BCPL nor B is a typed language, which is an oddity among 
high\-level languages, although both are much lower\-level than a language such as Java. Being untyped means that all data are considered machine words, which, although simple, leads to many complications and insecuri\-ties. For example, there is the problem of specifying floating\-point rather than integer arithmetic in an expression. In one implementation of BCPL, the variable operands of a floating\-point operation were preceded by peri\-ods. Variable operands not preceded by periods were considered to be inte\-gers. An alternative to this would have been to use different symbols for the floating\-point operators.
This problem, along with several others, led to the development of a 
new typed language based on B. Originally called NB but later named C, it was designed and implemented by Dennis Ritchie at Bell Laboratories in 1972 (Kernighan and Ritchie, 1978\). In some cases through BCPL, and in other cases directly, C was influenced by ALGOL 68\. This is seen in its 
for 2\.12 Some Early Descendants of the ALGOLs 7778 Chapter 2 Evolution of the Major Programming Languages
and switch statements, in its assigning operators, and in its treatment of 
pointers.
The only “standard” for C in its first decade and a half was the book by 
Kernighan and Ritchie (1978\).11 Over that time span, the language slowly 
evolved, with different implementors adding different features. In 1989, ANSI produced an official description of C (ANSI, 1989\), which included many of the features that implementors had already incorporated into the language. This standard was updated in 1999 (ISO, 1999\). This later version includes a few significant changes to the language. Among these are a complex data type, a Boolean data type, and C\+\+\-style comments (
//). We will refer to the 1989 
version, which has long been called ANSI C, as C89; we will refer to the 1999 version as C99\.
2\.12\.2\.2 Evaluation
C has adequate control statements and data\-structuring facilities to allow its use in many application areas. It also has a rich set of operators that provide a high degree of expressiveness.
One of the most important reasons why C is both liked and disliked is its 
lack of complete type checking. For example, in versions before C99, functions could be written for which parameters were not type checked. Those who like C appreciate the flexibility; those who do not like it find it too insecure. A major reason for its great increase in popularity in the 1980s was that a compiler for it was part of the widely used UNIX operating system. This inclusion in UNIX provided an essentially free and quite good compiler that was available to pro\-grammers on many different kinds of computers.
The following is an example of a C program:
/\* C Example Program
 Input: An integer, listlen, where listlen is less than 
 100, followed by listlen\-integer values
 Output: The number of input values that are greater than 
 the average of all input values \*/
int main (){
 int intlist\[99], listlen, counter, sum, average, result;
 sum \= 0;
 result \= 0;
 scanf("%d", \&listlen);
 if ((listlen \> 0\) \&\& (listlen \< 100\)) {
/\* Read input into an array and compute the sum \*/
 for (counter \= 0; counter \< listlen; counter\+\+) {
 scanf("%d", \&intlist\[counter]);
 sum \+\= intlist\[counter];
 }
 11\. This language is often referred to as “K \& R C.”/\* Compute the average \*/
 average \= sum / listlen;/\* Count the input values that are \> average \*/ for (counter \= 0; counter \< listlen; counter\+\+)
 if (intlist\[counter] \> average) result\+\+;
/\* Print result \*/
 printf("Number of values \> average is:%d\\n", result);
 }
 else
 printf("Error—input list length is not legal\\n");
 }
2\.13 Programming Based on Logic: Prolog
Simply put, logic programming is the use of a formal logic notation to commu\-
nicate computational processes to a computer. Predicate calculus is the notation used in current logic programming languages.
Programming in logic programming languages is nonprocedural. Pro\-
grams in such languages do not state exactly how a result is to be computed but 
rather describe the necessary form and/or characteristics of the result. What is needed to provide this capability in logic programming languages is a concise means of supplying the computer with both the relevant information and an inferencing process for computing desired results. Predicate calculus supplies the basic form of communication to the computer, and the proof method, named resolution, developed first by Robinson (1965\), supplies the inferenc\-ing technique.
2\.13\.1 Design Process
During the early 1970s, Alain Colmerauer and Phillippe Roussel in the Artifi\-cial Intelligence Group at the University of Aix\-Marseille, together with Robert Kowalski of the Department of Artificial Intelligence at the University of Edin\-burgh, developed the fundamental design of Prolog. The primary components of Prolog are a method for specifying predicate calculus propositions and an implementation of a restricted form of resolution. Both predicate calculus and resolution are described in Chapter 16\. The first Prolog interpreter was devel\-oped at Marseille in 1972\. The version of the language that was implemented is described in Roussel (1975\). The name Prolog is from programming logic.
2\.13\.2 Language Overview
Prolog programs consist of collections of statements. Prolog has only a few 
kinds of statements, but they can be complex.2\.13 Programming Based on Logic: Prolog 7980 Chapter 2 Evolution of the Major Programming Languages
One common use of Prolog is as a kind of intelligent database. This appli\-
cation provides a simple framework for discussing the Prolog language.
The database of a Prolog program consists of two kinds of statements: facts 
and rules. The following are examples of fact statements:
mother(joanne, jake).
father(vern, joanne).
These state that joanne is the mother of jake , and vern is the father of 
joanne .
An example of a rule statement is
grandparent(X, Z) :\- parent(X, Y), parent(Y, Z).
This states that it can be deduced that X is the grandparent of Z if it is true 
that X is the parent of Y and Y is the parent of Z, for some specific values for 
the variables X, Y, and Z.
The Prolog database can be interactively queried with goal statements, an 
example of which is
father(bob, darcie).
This asks if bob is the father of darcie . When such a query, or goal, is 
presented to the Prolog system, it uses its resolution process to attempt to determine the truth of the statement. If it can conclude that the goal is true, it displays “true.” If it cannot prove it, it displays “false.” 
2\.13\.3 Evaluation
In the 1980s, there was a relatively small group of computer scientists who believed that logic programming provided the best hope for escaping from the complexity of imperative languages, and also from the enormous prob\-lem of producing the large amount of reliable software that was needed. So far, however, there are two major reasons why logic programming has not become more widely used. First, as with some other nonimperative approaches, programs written in logic languages thus far have proven to be highly inefficient relative to equivalent imperative programs. Second, it has been determined that it is an effective approach for only a few relatively small areas of application: certain kinds of database management systems and some areas of AI.
There is a dialect of Prolog that supports object\-oriented programming—
Prolog\+\+ (Moss, 1994\). Logic programming and Prolog are described in greater detail in Chapter 16\.2\.14 History’s Largest Design Effort: Ada
The Ada language is the result of the most extensive and expensive language 
design effort ever undertaken. The following paragraphs briefly describe the evolution of Ada.
2\.14\.1 Historical Background
The Ada language was developed for the Department of Defense (DoD), so the state of their computing environment was instrumental in determining its form. By 1974, over half of the applications of computers in DoD were embedded sys\-tems. An embedded system is one in which the computer hardware is embedded in the device it controls or for which it provides services. Software costs were rising rapidly, primarily because of the increasing complexity of systems. More than 450 different programming languages were in use for DoD projects, and none of them was standardized by DoD. Every defense contractor could define a new and differ\-ent language for every contract.
12 Because of this language proliferation, applica\-
tion software was rarely reused. Furthermore, no software development tools were created (because they are usually language dependent). A great many languages were in use, but none was actually suitable for embedded systems applications. For these reasons, in 1974, the Army, Navy, and Air Force each independently proposed the development of a single high\-level language for embedded systems.
2\.14\.2 Design Process
Noting this widespread interest, in January 1975, Malcolm Currie, director of Defense Research and Engineering, formed the High\-Order Language Work\-ing Group (HOLWG), initially headed by Lt. Col. William Whitaker of the Air Force. The HOLWG had representatives from all of the military services and liaisons with Great Britain, France, and what was then West Germany. Its initial charter was to do the following:
• Identify the requirements for a new DoD high\-level language.
• Evaluate existing languages to determine whether there was a viable 
candidate.
• Recommend adoption or implementation of a minimal set of programming 
languages.
In April 1975, the HOLWG produced the Strawman requirements docu\-
ment for the new language (Department of Defense, 1975a). This was distrib\-uted to military branches, federal agencies, selected industrial and university representatives, and interested parties in Europe.
 12\. This result was largely due to the widespread use of assembly language for embedded sys\-
tems, along with the fact that most embedded systems used specialized processors.2\.14 History’s Largest Design Effort: Ada 8182 Chapter 2 Evolution of the Major Programming Languages
The Strawman document was followed by Woodenman (Department of 
Defense, 1975b) in August 1975, Tinman (Department of Defense, 1976\) in January 1976, Ironman (Department of Defense, 1977\) in January 1977, and finally Steelman (Department of Defense, 1978\) in June 1978\.
After a tedious process, the many submitted proposals for the language 
were narrowed down to four finalists, all of which were based on Pascal. In May 1979, the Cii Honeywell/Bull language design proposal was chosen from the four finalists as the design that would be used. The Cii Honeywell/Bull design team in France, the only foreign competitor among the final four, was led by Jean Ichbiah.
In the spring of 1979, Jack Cooper of the Navy Materiel Command rec\-
ommended the name for the new language, Ada, which was then adopted. The name commemorates Augusta Ada Byron (1815–1851\), countess of Lovelace, mathematician, and daughter of poet Lord Byron. She is generally recognized as being the world’s first programmer. She worked with Charles Babbage on his mechanical computers, the Difference and Analytical Engines, writing pro\-grams for several numerical processes.
The design and the rationale for Ada were published by ACM in its 
SIGPLAN Notices (ACM, 1979\) and distributed to a readership of more than 
10,000 people. A public test and evaluation conference was held in October 1979 in Boston, with representatives from over 100 organizations from the United States and Europe. By November, more than 500 language reports had been received from 15 different countries. Most of the reports suggested small modifications rather than drastic changes and outright rejections. Based on the language reports, the next version of the requirements specification, the Stoneman document (Department of Defense, 1980a), was released in February 1980\.
A revised version of the language design was completed in July 1980 and 
was accepted as MIL\-STD 1815, the standard Ada Language Reference Manual. 
The number 1815 was chosen because it was the year of the birth of Augusta Ada Byron. Another revised version of the Ada Language Reference Manual 
was released in July 1982\. In 1983, the American National Standards Insti\-tute standardized Ada. This “final” official version is described in Goos and Hartmanis (1983\). The Ada language design was then frozen for a minimum of five years.
2\.14\.3 Language Overview
This subsection briefly describes four of the major contributions of the Ada language.
Packages in the Ada language provide the means for encapsulating data 
objects, specifications for data types, and procedures. This, in turn, provides the support for the use of data abstraction in program design, as described in Chapter 11\.
The Ada language includes extensive facilities for exception handling, 
which allow the programmer to gain control after any one of a wide variety of exceptions, or run\-time errors, has been detected. Exception handling is 
discussed in Chapter 14\.
Program units can be generic in Ada. For example, it is possible to write 
a sort procedure that uses an unspecified type for the data to be sorted. Such a generic procedure must be instantiated for a specified type before it can be used, which is done with a statement that causes the compiler to generate a version of the procedure with the given type. The availability of such generic units increases the range of program units that might be reused, rather than duplicated, by programmers. Generics are discussed in Chapters 9 and 11\.
The Ada language also provides for concurrent execution of special pro\-
gram units, named tasks, using the rendezvous mechanism. Rendezvous is the name of a method of intertask communication and synchronization. Concur\-rency is discussed in Chapter 13\.
2\.14\.4 Evaluation
Perhaps the most important aspects of the design of the Ada language to con\-sider are the following:
• Because the design was competitive, there were no limits on participation.
• The Ada language embodies most of the concepts of software engineer\-
ing and language design of the late 1970s. Although one can question the actual approaches used to incorporate these features, as well as the wisdom of including such a large number of features in a language, most agree that the features are valuable.
• Although most people did not anticipate it, the development of a compiler 
for the Ada language was a difficult task. Only in 1985, almost four years after the language design was completed, did truly usable Ada compilers begin to appear.
The most serious criticism of Ada in its first few years was that it was too 
large and too complex. In particular, Hoare (1981\) stated that it should not be used for any application where reliability is critical, which is precisely the type of application for which it was designed. On the other hand, others have praised it as the epitome of language design for its time. In fact, even Hoare eventually softened his view of the language.
The following is an example of an Ada program:
\-\- Ada Example Program
\-\- Input: An integer, List\_Len, where List\_Len is less 
\-\- than 100, followed by List\_Len\-integer values
\-\- Output: The number of input values that are greater 
\-\- than the average of all input valueswith Ada.Text\_IO, Ada.Integer.Text\_IO;
use Ada.Text\_IO, Ada.Integer.Text\_IO;2\.14 History’s Largest Design Effort: Ada 8384 Chapter 2 Evolution of the Major Programming Languages
procedure Ada\_Ex is
 type Int\_List\_Type is array (1\..99\) of Integer;
 Int\_List : Int\_List\_Type; List\_Len, Sum, Average, Result : Integer;begin Result:\= 0;
 Sum :\= 0;
 Get (List\_Len);
 if (List\_Len \> 0\) and (List\_Len \< 100\) then
\-\- Read input data into an array and compute the sum
 for Counter :\= 1 .. List\_Len loop
 Get (Int\_List(Counter));
 Sum :\= Sum \+ Int\_List(Counter);
 end loop;
\-\- Compute the average Average :\= Sum / List\_Len;\-\- Count the number of values that are \> average
 for Counter :\= 1 .. List\_Len loop
 if Int\_List(Counter) \> Average then
 Result:\= Result\+ 1; end if;
 end loop;
\-\- Print result Put ("The number of values \> average is:"); Put (Result); New\_Line; else
 Put\_Line ("Error—input list length is not legal");
 end if;
end Ada\_Ex;
2\.14\.5 Ada 95 and Ada 2005
T wo of the most important new features of Ada 95 are described briefly in the 
following paragraphs. In the remainder of the book, we will use the name Ada 83 for the original version and Ada 95 (its actual name) for the later version when it is important to distinguish between the two versions. In discussions of language features common to both versions, we will use the name Ada. The Ada 95 standard language is defined in ARM (1995\).
The type derivation mechanism of Ada 83 is extended in Ada 95 to allow 
adding new components to those inherited from a base class. This provides for inheritance, a key ingredient in object\-oriented programming languages. Dynamic binding of subprogram calls to subprogram definitions is accom\-plished through subprogram dispatching, which is based on the tag value of derived types through classwide types. This feature provides for polymorphism, another principal feature of object\-oriented programming. These features of 
Ada 95 are discussed in Chapter 12\.
The rendezvous mechanism of Ada 83 provided only a cumbersome and 
inefficient means of sharing data among concurrent processes. It was necessary to introduce a new task to control access to the shared data. The protected objects of Ada 95 offer an attractive alternative to this. The shared data is encapsulated in a syntactic structure that controls all access to the data, either by rendezvous or by subprogram call. The new features of Ada 95 for concur\-rency and shared data are discussed in Chapter 13\.
It is widely believed that the popularity of Ada 95 suffered because 
the Department of Defense stopped requiring its use in military software systems. There were, of course, other factors that hindered its growth in popularity. Most important among these was the widespread acceptance of C\+\+ for object\-oriented programming, which occurred before Ada 95 was released.
There were several additions to Ada 95 to get Ada 2005\. Among these were 
interfaces, similar to those of Java, more control of scheduling algorithms, and synchronized interfaces.
Ada is widely used in both commercial and defense avionics, air traffic 
control, and rail transportation, as well as in other areas.
2\.15 Object\-Oriented Programming: Smalltalk
Smalltalk was the first programming language that fully supported object\-
oriented programming. It is therefore an important part of any discussion of the evolution of programming languages.
2\.15\.1 Design Process
The concepts that led to the development of Smalltalk originated in the Ph.D. dissertation work of Alan Kay in the late 1960s at the University of Utah (Kay, 1969\). Kay had the remarkable foresight to predict the future availability of powerful desktop computers. Recall that the first microcomputer systems were not marketed until the mid\-1970s, and they were only remotely related to the machines envisioned by Kay, which were seen to execute a million or more instructions per second and contain several megabytes of memory. Such machines, in the form of workstations, became widely available only in the early 1980s.
Kay believed that desktop computers would be used by nonprogrammers 
and thus would need very powerful human\-interfacing capabilities. The com\-puters of the late 1960s were largely batch oriented and were used exclusively by professional programmers and scientists. For use by nonprogrammers, Kay determined, a computer would have to be highly interactive and use sophisti\-cated graphics in its user interface. Some of the graphics concepts came from 2\.15 Object\-Oriented Programming: Smalltalk 8586 Chapter 2 Evolution of the Major Programming Languages
the LOGO experience of Seymour Papert, in which graphics were used to aid 
children in the use of computers (Papert, 1980\).
Kay originally envisioned a system he called Dynabook, which was meant 
to be a general information processor. It was based in part on the Flex language, which he had helped design. Flex was based primarily on SIMULA 67\. Dynabook used the paradigm of the typical desk, on which there are a number of papers, some partially covered. The top sheet is often the focus of attention, with the oth\-ers temporarily out of focus. The display of Dynabook would model this scene, using screen windows to represent various sheets of paper on the desktop. The user would interact with such a display both through keystrokes and by touch\-ing the screen with his or her fingers. After the preliminary design of Dynabook earned him a Ph.D., Kay’s goal became to see such a machine constructed.
Kay found his way to the Xerox Palo Alto Research Center (Xerox PARC) 
and presented his ideas on Dynabook. This led to his employment there and the subsequent birth of the Learning Research Group at Xerox. The first charge of the group was to design a language to support Kay’s programming paradigm and implement it on the best personal computer then available. These efforts resulted in an “Interim” Dynabook, consisting of a Xerox Alto workstation and Smalltalk\-72 software. T ogether, they formed a research tool for further development. A number of research projects were conducted with this system, including several experiments to teach programming to children. Along with the experiments came further developments, leading to a sequence of languages that ended with Smalltalk\-80\. As the language grew, so did the power of the hardware on which it resided. By 1980, both the language and the Xerox hard\-ware nearly matched the early vision of Alan Kay.
2\.15\.2 Language Overview
The Smalltalk world is populated by nothing but objects, from integer con\-stants to large complex software systems. All computing in Smalltalk is done by the same uniform technique: sending a message to an object to invoke one of its methods. A reply to a message is an object, which either returns the requested information or simply notifies the sender that the requested process\-ing has been completed. The fundamental difference between a message and a subprogram call is this: A message is sent to a data object, specifically to one of the methods defined for the object. The called method is then executed, often modifying the data of the object to which the message was sent; a subprogram call is a message to the code of a subprogram. Usually the data to be processed by the subprogram is sent to it as a parameter.
13
In Smalltalk, object abstractions are classes, which are very similar to the 
classes of SIMULA 67\. Instances of the class can be created and are then the objects of the program.
The syntax of Smalltalk is unlike that of most other programming lan\-
guage, in large part because of the use of messages, rather than arithmetic and 
 13\. Of course, a method call can also pass data to be processed by the called method.logic expressions and conventional control statements. One of the Smalltalk 
control constructs is illustrated in the example in the next subsection.
2\.15\.3 Evaluation
Smalltalk has done a great deal to promote two separate aspects of comput\-ing: graphical user interfaces and object\-oriented programming. The window\-ing systems that are now the dominant method of user interfaces to software systems grew out of Smalltalk. T oday, the most significant software design methodologies and programming languages are object oriented. Although the origin of some of the ideas of object\-oriented languages came from SIMULA 67, they reached maturation in Smalltalk. It is clear that Smalltalk’s impact on the computing world is extensive and will be long\-lived.
The following is an example of a Smalltalk class definition:
"Smalltalk Example Program"
"The following is a class definition, instantiations of which can draw equilateral polygons of any number of sides"class name Polygon
superclass Object
instance variable names ourPen
numSides
sideLength
"Class methods"
 "Create an instance"
 new
 ^ super new getPen
 "Get a pen for drawing polygons"
 getPen
 ourPen \<\- Pen new defaultNib: 2
 "Instance methods"
 "Draw a polygon"
 draw
 numSides timesRepeat: \[ourPen go: sideLength; 
 turn: 360 // numSides]
 "Set length of sides"
 length: len
 sideLength \<\- len
 "Set number of sides"
 sides: num numSides \<\- num2\.15 Object\-Oriented Programming: Smalltalk 8788 Chapter 2 Evolution of the Major Programming Languages
2\.16 Combining Imperative and Object\-Oriented Features: C\+\+
The origins of C were discussed in Section 2\.12; the origins of Simula 67 were 
discussed in Section 2\.10; the origins of Smalltalk were discussed in Section 2\.15\. C\+\+ builds language facilities, borrowed from Simula 67, on top of C to support much of what Smalltalk pioneered. C\+\+ has evolved from C through a sequence of modifications to improve its imperative features and to add con\-structs to support object\-oriented programming.
2\.16\.1 Design Process
The first step from C toward C\+\+ was made by Bjarne Stroustrup at Bell Laboratories in 1980\. The initial modifications to C included the addition of function parameter type checking and conversion and, more significantly, classes, which are related to those of SIMULA 67 and Smalltalk. Also included were derived classes, public/private access control of inherited components, constructor and destructor methods, and friend classes. During 1981, inline functions, default parameters, and overloading of the assignment operator were added. The resulting language was called C with Classes and is described in Stroustrup (1983\).
It is useful to consider some goals of C with Classes. The primary goal 
was to provide a language in which programs could be organized as they could be organized in SIMULA 67—that is, with classes and inheritance. A second important goal was that there should be little or no performance penalty rela\-tive to C. For example, array index range checking was not even considered because a significant performance disadvantage, relative to C, would result. A third goal of C with Classes was that it could be used for every application for which C could be used, so virtually none of the features of C would be removed, not even those considered to be unsafe.
By 1984, this language was extended by the inclusion of virtual methods, 
which provide dynamic binding of method calls to specific method definitions, method name and operator overloading, and reference types. This version of the language was called C\+\+. It is described in Stroustrup (1984\).
In 1985, the first available implementation appeared: a system named 
Cfront, which translated C\+\+ programs into C programs. This version of Cfront and the version of C\+\+ it implemented were named Release 1\.0\. It is described in Stroustrup (1986\).
Between 1985 and 1989, C\+\+ continued to evolve, based largely on user 
reactions to the first distributed implementation. This next version was named Release 2\.0\. Its Cfront implementation was released in June 1989\. The most important features added to C\+\+ Release 2\.0 were support for multiple inheri\-tance (classes with more than one parent class) and abstract classes, along with some other enhancements. Abstract classes are described in Chapter 12\.
Release 3\.0 of C\+\+ evolved between 1989 and 1990\. It added templates, 
which provide parameterized types, and exception handling. The current ver\-sion of C\+\+, which was standardized in 1998, is described in ISO (1998\).In 2002, Microsoft released its .NET computing platform, which included 
a new version of C\+\+, named Managed C\+\+, or MC\+\+. MC\+\+ extends C\+\+ to provide access to the functionality of the .NET Framework. The additions include properties, delegates, interfaces, and a reference type for garbage\-collected objects. Properties are discussed in Chapter 11\. Delegates are briefly discussed in the introduction to C\# in Section 2\.19\. Because .NET does not support multiple inheritance, neither does MC\+\+.
2\.16\.2 Language Overview
Because C\+\+ has both functions and methods, it supports both procedural and object\-oriented programming.
Operators in C\+\+ can be overloaded, meaning the user can create opera\-
tors for existing operators on user\-defined types. C\+\+ methods can also be overloaded, meaning the user can define more than one method with the same name, provided either the numbers or types of their parameters are different.
Dynamic binding in C\+\+ is provided by virtual methods. These methods 
define type\-dependent operations, using overloaded methods, within a collec\-tion of classes that are related through inheritance. A pointer to an object of class A can also point to objects of classes that have class A as an ancestor. When this pointer points to an overloaded virtual method, the method of the current type is chosen dynamically.
Both methods and classes can be templated, which means that they can be 
parameterized. For example, a method can be written as a templated method to allow it to have versions for a variety of parameter types. Classes enjoy the same flexibility.
C\+\+ supports multiple inheritance. It also includes exception handling that 
is significantly different from that of Ada. One difference is that hardware\-detectable exceptions cannot be handled. The exception\-handling constructs of Ada and C\+\+ are discussed in Chapter 14\.
2\.16\.3 Evaluation
C\+\+ rapidly became and remains a widely used language. One factor in its popularity is the availability of good and inexpensive compilers. Another factor is that it is almost completely backward compatible with C (meaning that C programs can be, with few changes, compiled as C\+\+ programs), and in most implementations it is possible to link C\+\+ code with C code—and thus rela\-tively easy for the many C programmers to learn C\+\+. Finally, at the time C\+\+ first appeared, when object\-oriented programming began to receive widespread interest, C\+\+ was the only available language that was suitable for large com\-mercial software projects.
On the negative side, because C\+\+ is a very large and complex language, 
it clearly suffers drawbacks similar to those of PL/I. It inherited most of the insecurities of C, which make it less safe than languages such as Ada and Java.2\.16 Combining Imperative and Object\-Oriented Features: C\+\+ 8990 Chapter 2 Evolution of the Major Programming Languages
2\.16\.4 A Related Language: Objective\-C
Objective\-C (Kochan, 2009\) is another hybrid language with both impera\-
tive and object\-oriented features. Objective\-C was designed by Brad Cox and T om Love in the early 1980s. Initially, it consisted of C plus the classes and message passing of Smalltalk. Among the programming languages that were created by adding support for object\-oriented programming to an impera\-tive language, Objective\-C is the only one to use the Smalltalk syntax for that support.
After Steve Jobs left Apple and founded NeXT, he licensed Objective\-C 
and it was used to write the NeXT computer system software. NeXT also released its Objective\-C compiler, along with the NeXT step development environment and a library of utilities. After the NeXT project failed, Apple bought NeXT and used Objective\-C to write MAC OS X. Objective\-C is the language of all iPhone software, which explains its rapid rise in popularity after the iPhone appeared.
One characteristic that Objective\-C inherited from Smalltalk is the 
dynamic binding of messages to objects. This means that there is no static checking of messages. If a message is sent to an object and the object cannot respond to the message, it is not known until run time, when an exception is raised.
In 2006, Apple announced Objective\-C 2\.0, which added a form of garbage 
collection and new syntax for declaring properties. Unfortunately, garbage col\-lection is not supported by the iPhone run\-time system.
Objective\-C is a strict superset of C, so all of the insecurities of that lan\-
guage are present in Objective\-C.
2\.16\.5 Another Related Language: Delphi
Delphi (Lischner, 2000\) is a hybrid language, similar to C\+\+ and Objetive\-C in that it was created by adding object\-oriented support, among other things, to an existing imperative language, in this case Pascal. Many of the differences between C\+\+ and Delphi are a result of the predecessor languages and the surrounding programming cultures from which they are derived. Because C is a powerful but potentially unsafe language, C\+\+ also fits that description, at least in the areas of array subscript range checking, pointer arithmetic, and its numerous type coercions. Likewise, because Pascal is more elegant and safer than C, Delphi is more elegant and safer than C\+\+. Delphi is also less complex than C\+\+. For example, Delphi does not allow user\-defined operator overloading, generic subprograms, and parameterized classes, all of which are part of C\+\+.
Delphi, like Visual C\+\+, provides a graphical user interface (GUI) to the 
developer and simple ways to create GUI interfaces to applications written in Delphi. Delphi was designed by Anders Hejlsberg, who had previously devel\-oped the T urbo Pascal system. Both of these were marketed and distributed by Borland. Hejlsberg was also the lead designer of C\#.2\.16\.6 A Loosely Related Language: Go
The Go programming language is not directly related to C\+\+, although it is 
C\-based. It is in this section in part because it does not deserve its own section and it does not fit elsewhere.
Go was designed by Rob Pike, Ken Thompson, and Robert Griesemer at 
Google. Thompson is the designer of the predecessor of C, B, as well as the codesigner with Dennis Ritchie of UNIX. He and Pike were both formerly employed at Bell Labs. The initial design was begun in 2007 and the first implementation was released in late 2009\. One of the initial motivations for Go was the slowness of compilation of large C\+\+ programs at Google. One of the characteristics of the initial compiler for Go is that is it extremely fast. The Go language borrows some of its syntax and constructs from C. Some of the new features of Go include the following: (1\) Data declarations are syntactically reversed from the other C\-based languages; (2\) the variables precede the type name; (3\) variable declarations can be given a type by inference if the variable is given an initial value; and (4\) functions can return multiple values. Go does not support traditional object\-oriented programming, as it has no form of inheri\-tance. However, methods can be defined for any type. It also does not have generics. The control statements of Go are similar to those of other C\-based languages, although the switch does not include the implicit fall through to the next segment. Go includes a goto statement, pointers, associative arrays, interfaces (though they are different from those of Java and C\#), and support for concurrency using its goroutines.
2\.17 An Imperative\-Based Object\-Oriented Language: Java
Java’s designers started with C\+\+, removed some constructs, changed some, and 
added a few others. The resulting language provides much of the power and flexibility of C\+\+, but in a smaller, simpler, and safer language.
2\.17\.1 Design Process
Java, like many programming languages, was designed for an application for which there appeared to be no satisfactory existing language. In 1990, Sun Microsystems determined there was a need for a programming language for embedded consumer electronic devices, such as toasters, microwave ovens, and interactive TV systems. Reliability was one of the primary goals for such a language. It may not seem that reliability would be an important factor in the software for a microwave oven. If an oven had malfunctioning software, it prob\-ably would not pose a grave danger to anyone and most likely would not lead to large legal settlements. However, if the software in a particular model was found to be erroneous after a million units had been manufactured and sold, their recall would entail significant cost. Therefore, reliability is an important 
characteristic of the software in consumer electronic products.2\.17 An Imperative\-Based Object\-Oriented Language: Java 9192 Chapter 2 Evolution of the Major Programming Languages
After considering C and C\+\+, it was decided that neither would be sat\-
isfactory for developing software for consumer electronic devices. Although C was relatively small, it did not provide support for object\-oriented pro\-gramming, which they deemed a necessity. C\+\+ supported object\-oriented programming, but it was judged to be too large and complex, in part because it also supported procedure\-oriented programming. It was also believed that neither C nor C\+\+ provided the necessary level of reliability. So, a new lan\-guage, later named Java, was designed. Its design was guided by the fun\-damental goal of providing greater simplicity and reliability than C\+\+ was believed to provide.
Although the initial impetus for Java was consumer electronics, none of the 
products with which it was used in its early years were ever marketed. Starting in 1993, when the World Wide Web became widely used, and largely because of the new graphical browsers, Java was found to be a useful tool for Web pro\-gramming. In particular, Java applets, which are relatively small Java programs that are interpreted in Web browsers and whose output can be included in displayed Web documents, quickly became very popular in the middle to late 1990s. In the first few years of Java popularity, the Web was its most common application.
The Java design team was headed by James Gosling, who had previously 
designed the UNIX emacs editor and the NeWS windowing system.
2\.17\.2 Language Overview
As we stated previously, Java is based on C\+\+ but it was specifically designed to be smaller, simpler, and more reliable. Like C\+\+, Java has both classes and primitive types. Java arrays are instances of a predefined class, whereas in C\+\+ they are not, although many C\+\+ users build wrapper classes for arrays to add features like index range checking, which is implicit in Java.
Java does not have pointers, but its reference types provide some of the 
capabilities of pointers. These references are used to point to class instances. All objects are allocated on the heap. References are always implicitly deref\-erenced, when necessary. So they behave more like ordinary scalar variables.
Java has a primitive Boolean type named 
boolean , used mainly for the 
control expressions of its control statements (such as if and while ). Unlike C 
and C\+\+, arithmetic expressions cannot be used for control expressions.
One significant difference between Java and many of its predecessors that 
support object\-oriented programming, including C\+\+, is that it is not possible to write stand\-alone subprograms in Java. All Java subprograms are methods and are defined in classes. Furthermore, methods can be called through a class or object only. One consequence of this is that while C\+\+ supports both pro\-cedural and object\-oriented programming, Java supports object\-oriented pro\-gramming only.
Another important difference between C\+\+ and Java is that C\+\+ supports 
multiple inheritance directly in its class definitions. Java supports only single inheritance of classes, although some of the benefits of multiple inheritance can 
be gained by using its interface construct.
Among the C\+\+ constructs that were not copied into Java are structs and 
unions.
Java includes a relatively simple form of concurrency control through its 
synchronized modifier, which can appear on methods and blocks. In either 
case, it causes a lock to be attached. The lock ensures mutually exclusive access or execution. In Java, it is relatively easy to create concurrent processes, which in Java are called threads.
Java uses implicit storage deallocation for its objects, often called garbage 
collection . This frees the programmer from needing to delete objects explicitly 
when they are no longer needed. Programs written in languages that do not have garbage collection often suffer from what is sometimes called memory leakage, which means that storage is allocated but never deallocated. This can obviously lead to eventual depletion of all available storage. Object deallocation is discussed in detail in Chapter 6\.
Unlike C and C\+\+, Java includes assignment type coercions (implicit type 
conversions) only if they are widening (from a “smaller” type to a “larger” type). So 
int to float coercions are done across the assignment operator, but float 
to int coercions are not.
2\.17\.3 Evaluation
The designers of Java did well at trimming the excess and/or unsafe features of C\+\+. For example, the elimination of half of the assignment coercions that are done in C\+\+ was clearly a step toward higher reliability. Index range checking of array accesses also makes the language safer. The addition of concurrency enhances the scope of applications that can be written in the language, as do the class libraries for graphical user interfaces, database access, and networking.
Java’s portability, at least in intermediate form, has often been attributed 
to the design of the language, but it is not. Any language can be translated to an intermediate form and “run” on any platform that has a virtual machine for that intermediate form. The price of this kind of portability is the cost of interpretation, which traditionally has been about an order of magnitude more than execution of machine code. The initial version of the Java interpreter, called the Java Virtual Machine ( JVM), indeed was at least 10 times slower than equivalent compiled C programs. However, many Java programs are now translated to machine code before being executed, using Just\-in\-Time ( JIT) compilers. This makes the efficiency of Java programs competitive with that of programs in conventionally compiled languages such as C\+\+.
The use of Java increased faster than that of any other programming lan\-
guage. Initially, this was due to its value in programming dynamic Web docu\-ments. Clearly, one of the reasons for Java’s rapid rise to prominence is simply that programmers like its design. Some developers thought C\+\+ was simply too 2\.17 An Imperative\-Based Object\-Oriented Language: Java 9394 Chapter 2 Evolution of the Major Programming Languages
large and complex to be practical and safe. Java offered them an alternative that 
has much of the power of C\+\+, but in a simpler, safer language. Another reason is that the compiler/interpreter system for Java is free and easily obtained on the Web. Java is now widely used in a variety of different applications areas.
The most recent version of Java, Java 7, appeared in 2011\. Since its begin\-
ning, many features have been added to the language, including an enumeration 
class, generics, and a new iteration construct.
The following is an example of a Java program:
// Java Example Program
// Input: An integer, listlen, where listlen is less 
// than 100, followed by length\-integer values
// Output: The number of input data that are greater than
// the average of all input values
import java.io.\*;
class IntSort {
public static void main(String args\[]) throws IOException {
 DataInputStream in \= new DataInputStream(System.in);
 int listlen,
 counter,
 sum \= 0, average, result \= 0; int\[] intlist \= new int\[99];
 listlen \= Integer.parseInt(in.readLine());
 if ((listlen \> 0\) \&\& (listlen \< 100\)) {
/\* Read input into an array and compute the sum \*/
 for (counter \= 0; counter \< listlen; counter\+\+) {
 intlist\[counter] \=
 Integer.valueOf(in.readLine()).intValue();
 sum \+\= intlist\[counter];
 } /\* Compute the average \*/ average \= sum / listlen;
/\* Count the input values that are \> average \*/
 for (counter \= 0; counter \< listlen; counter\+\+)
 if (intlist\[counter] \> average) result\+\+;
/\* Print result \*/
 System.out.println(
 "\\nNumber of values \> average is:" \+ result); 
 } //\*\* end of then clause of if ((listlen \> 0\) ...
 else System.out.println(
 "Error—input list length is not legal\\n");
 } //\*\* end of method main} //\*\* end of class IntSort2\.18 Scripting Languages
Scripting languages have evolved over the past 25 years. Early scripting 
languages were used by putting a list of commands, called a script , in a file 
to be interpreted. The first of these languages, named sh (for shell), began 
as a small collection of commands that were interpreted as calls to system subprograms that performed utility functions, such as file management and simple file filtering. T o this were added variables, control flow statements, functions, and various other capabilities, and the result is a complete pro\-gramming language. One of the most powerful and widely known of these is 
ksh (Bolsky and Korn, 1995\), which was developed by David Korn at Bell 
Laboratories.
Another scripting language is awk, developed by Al Aho, Brian Kernighan, 
and Peter Weinberger at Bell Laboratories (Aho et al., 1988\). awk began as a 
report\-generation language but later became a more general\-purpose language.
2\.18\.1 Origins and Characteristics of Perl
The Perl language, developed by Larry Wall, was originally a combination of 
sh and awk. Perl has grown significantly since its beginnings and is now a 
powerful, although still somewhat primitive, programming language. Although it is still often called a scripting language, it is actually more similar to a typical imperative language, since it is always compiled, at least into an intermediate language, before it is executed. Furthermore, it has all the constructs to make it applicable to a wide variety of areas of computational problems.
Perl has a number of interesting features, only a few of which are men\-
tioned in this chapter and later discussed in the book.
Variables in Perl are statically typed and implicitly declared. There are 
three distinctive namespaces for variables, denoted by the first character of the variables’ names. All scalar variable names begin with dollar signs (
$), all 
array names begin with at signs ( @), and all hash names (hashes are briefly 
described below) begin with percent signs ( %). This convention makes vari\-
able names in programs more readable than those of any other programming language.
Perl includes a large number of implicit variables. Some of them are used 
to store Perl parameters, such as the particular form of newline character or characters that are used in the implementation. Implicit variables are com\-monly used as default parameters to built\-in functions and default operands for some operators. The implicit variables have distinctive—although cryptic—names, such as 
$! and @\_. The implicit variables’ names, like the user\-defined 
variable names, use the three namespaces, so $! is a scalar.
Perl’s arrays have two characteristics that set them apart from the arrays 
of the common imperative languages. First, they have dynamic length, mean\-ing that they can grow and shrink as needed during execution. Second, arrays can be sparse, meaning that there can be gaps between the elements. These 2\.18 Scripting Languages 9596 Chapter 2 Evolution of the Major Programming Languages
gaps do not take space in memory, and the iteration statement used for arrays, 
foreach , iterates over the missing elements.
Perl includes associative arrays, which are called hashes . These data struc\-
tures are indexed by strings and are implicitly controlled hash tables. The Perl system supplies the hash function and increases the size of the structure when necessary.
Perl is a powerful, but somewhat dangerous, language. Its scalar type stores 
both strings and numbers, which are normally stored in double\-precision floating\-point form. Depending on the context, numbers may be coerced to strings and vice versa. If a string is used in numeric context and the string cannot be converted to a number, zero is used and there is no warning or error message provided for the user. This effect can lead to errors that are not detected by the compiler or run\-time system. Array indexing cannot be checked, because there is no set subscript range for any array. References to nonexistent elements return 
undef , 
which is interpreted as zero in numeric context. So, there is also no error detec\-tion in array element access.
Perl’s initial use was as a UNIX utility for processing text files. It was and 
still is widely used as a UNIX system administration tool. When the World Wide Web appeared, Perl achieved widespread use as a Common Gateway Interface language for use with the Web, although it is now rarely used for that purpose. Perl is used as a general\-purpose language for a variety of applications, such as computational biology and artificial intelligence.
The following is an example of a Perl program:
\# Perl Example Program
\# Input: An integer, $listlen, where $listlen is less 
\# than 100, followed by $listlen\-integer values.
\# Output: The number of input values that are greater than
\# the average of all input values.
($sum, $result) \= (0, 0\);$listlen \= ;if (($listlen \> 0\) \&\& ($listlen \< 100\)) {
\# Read input into an array and compute the sum
 for ($counter \= 0; $counter \< $listlen; $counter\+\+) {
 $intlist\[$counter] \= ;
 } \#\- end of for (counter ...
\# Compute the average
 $average \= $sum / $listlen;
\# Count the input values that are \> average
 foreach $num (@intlist) {
 if ($num \> $average) { $result\+\+; }
 } \#\- end of foreach $num ...
\# Print result
 print "Number of values \> average is: $result \\n";
} \#\- end of if (($listlen ...else {
 print "Error\-\-input list length is not legal \\n";
}
2\.18\.2 Origins and Characteristics of JavaScript
Use of the Web exploded in the mid\-1990s after the first graphical browsers 
appeared. The need for computation associated with HTML documents, which by themselves are completely static, quickly became critical. Computation on the server side was made possible with the Common Gateway Interface (CGI), which allowed HTML documents to request the execution of programs on the server, with the results of such computations returned to the browser in the form of HTML documents. Computation on the browser end became available with the advent of Java applets. Both of these approaches have now been replaced for the most part by newer technologies, primarily scripting languages.
JavaScript (Flanagan, 2002\) was originally developed by Brendan Eich at 
Netscape. Its original name was Mocha. It was later renamed LiveScript. In late 1995, LiveScript became a joint venture of Netscape and Sun Microsystems and its name was changed to JavaScript. JavaScript has gone through extensive evolution, moving from version 1\.0 to version 1\.5 by adding many new fea\-tures and capabilities. A language standard for JavaScript was developed in the late 1990s by the European Computer Manufacturers Association (ECMA) as ECMA\-262\. This standard has also been approved by the International Stan\-dards Organization (ISO) as ISO\-16262\. Microsoft’s version of JavaScript is named JScript .NET.
Although a JavaScript interpreter could be embedded in many different 
applications, its most common use is embedded in Web browsers. JavaScript code is embedded in HTML documents and interpreted by the browser when the documents are displayed. The primary uses of JavaScript in Web program\-ming are to validate form input data and create dynamic HTML documents. JavaScript also is now used with the Rails Web development framework.
In spite of its name, JavaScript is related to Java only through the use 
of similar syntax. Java is strongly typed, but JavaScript is dynamically typed (see Chapter 5\). JavaScript’s character strings and its arrays have dynamic length. Because of this, array indices are not checked for validity, although this is required in Java. Java fully supports object\-oriented programming, but JavaScript supports neither inheritance nor dynamic binding of method calls to methods.
One of the most important uses of JavaScript is for dynamically creating 
and modifying HTML documents. JavaScript defines an object hierarchy that matches a hierarchical model of an HTML document, which is defined by the Document Object Model. Elements of an HTML document are accessed through these objects, providing the basis for dynamic control of the elements of documents.2\.18 Scripting Languages 9798 Chapter 2 Evolution of the Major Programming Languages
Following is a JavaScript script for the problem previously solved in several 
languages in this chapter. Note that it is assumed that this script will be called from an HTML document and interpreted by a Web browser.
// example.js
// Input: An integer, listLen, where listLen is less// than 100, followed by listLen\-numeric values// Output: The number of input values that are greater
// than the average of all input values 
var intList \= new Array(99\);
var listLen, counter, sum \= 0, result \= 0;
 
listLen \= prompt ( "Please type the length of the input list", "");if ((listLen \> 0\) \&\& (listLen \< 100\)) {
 // Get the input and compute its sum for (counter \= 0; counter \< listLen; counter\+\+) {
 intList\[counter] \= prompt (
 "Please type the next number", "");
 sum \+\= parseInt(intList\[counter]);
 }
 // Compute the average average \= sum / listLen;
 
// Count the input values that are \> average
 for (counter \= 0; counter \< listLen; counter\+\+) 
 if (intList\[counter] \> average) result\+\+;
 
// Display the results
 document.write("Number of values \> average is: ",
 result, "  
");
} else
 document.write(
 "Error \- input list length is not legal   
");
2\.18\.3 Origins and Characteristics of PHP
PHP (Converse and Park, 2000\) was developed by Rasmus Lerdorf, a member 
of the Apache Group, in 1994\. His initial motivation was to provide a tool to help track visitors to his personal Web site. In 1995, he developed a package called Personal Home Page T ools, which became the first publicly distributed version of PHP . Originally, PHP was an abbreviation for Personal Home Page. Later, its user community began using the recursive name PHP: Hypertext Preprocessor, which subsequently forced the original name into obscurity. PHP 
is now developed, distributed, and supported as an open\-source product. PHP processors are resident on most Web servers.
PHP is an HTML\-embedded server\-side scripting language specifically 
designed for Web applications. PHP code is interpreted on the Web server when an HTML document in which it is embedded has been requested by a browser. PHP code usually produces HTML code as output, which replaces the PHP code in the HTML document. Therefore, a Web browser never sees PHP code.
PHP is similar to JavaScript, in its syntactic appearance, the dynamic 
nature of its strings and arrays, and its use of dynamic typing. PHP’s arrays are a combination of JavaScript’s arrays and Perl’s hashes.
The original version of PHP did not support object\-oriented program\-
ming, but that support was added in the second release. However, PHP does not support abstract classes or interfaces, destructors, or access controls for class members.
PHP allows simple access to HTML form data, so form processing is easy 
with PHP . PHP provides support for many different database management systems. This makes it a useful language for building programs that need Web access to databases.
2\.18\.4 Origins and Characteristics of Python
Python (Lutz and Ascher, 2004\) is a relatively recent object\-oriented inter\-preted scripting language. Its initial design was by Guido van Rossum at Stichting Mathematisch Centrum in the Netherlands in the early 1990s. Its development is now being done by the Python Software Foundation. Python is being used for the same kinds of applications as Perl: system administration, CGI programming, and other relatively small computing tasks. Python is an open\-source system and is available for most common computing platforms. The Python implementation is available at 
www.python.org , which also has 
extensive information regarding Python.
Python’s syntax is not based directly on any commonly used language. It is 
type checked, but dynamically typed. Instead of arrays, Python includes three kinds of data structures: lists; immutable lists, which are called tuples ; and 
hashes, which are called dictionaries . There is a collection of list methods, 
such as 
append , insert , remove , and sort , as well as a collection of meth\-
ods for dictionaries, such as keys , values , copy , and has\_key . Python also 
supports list comprehensions, which originated with the Haskell language. List comprehensions are discussed in Section 15\.8\.
Python is object oriented, includes the pattern\-matching capabilities of 
Perl, and has exception handling. Garbage collection is used to reclaim objects when they are no longer needed.
Support for CGI programming, and form processing in particular, is pro\-
vided by the 
cgi module. Modules that support cookies, networking, and data\-
base access are also available.2\.18 Scripting Languages 99100 Chapter 2 Evolution of the Major Programming Languages
Python includes support for concurrency with its threads, as well as sup\-
port for network programming with its sockets. It also has more support for functional programming than other nonfunctional programming languages.
One of the more interesting features of Python is that it can be easily 
extended by any user. The modules that support the extensions can be written in any compiled language. Extensions can add functions, variables, and object types. These extensions are implemented as additions to the Python interpreter.
2\.18\.5 Origins and Characteristics of Ruby
Ruby (Thomas et al., 2005\) was designed by Yukihiro Matsumoto (aka Matz) in the early 1990s and released in 1996\. Since then it has continually evolved. The motivation for Ruby was dissatisfaction of its designer with Perl and Python. Although both Perl and Python support object\-oriented programming,
14 nei\-
ther is a pure object\-oriented language, at least in the sense that each has primi\-tive (nonobject) types and each supports functions.
The primary characterizing feature of Ruby is that it is a pure object\-
oriented language, just as is Smalltalk. Every data value is an object and all operations are via method calls. The operators in Ruby are only syntactic mechanisms to specify method calls for the corresponding operations. Because they are methods, they can be redefined. All classes, predefined or user defined, can be subclassed.
Both classes and objects in Ruby are dynamic in the sense that methods can 
be dynamically added to either. This means that both classes and objects can have different sets of methods at different times during execution. So, different instantiations of the same class can behave differently. Collections of methods, data, and constants can be included in the definition of a class.
The syntax of Ruby is related to that of Eiffel and Ada. There is no need 
to declare variables, because dynamic typing is used. The scope of a variable is specified in its name: A variable whose name begins with a letter has local scope; one that begins with 
@ is an instance variable; one that begins with $ 
has global scope. A number of features of Perl are present in Ruby, including implicit variables with silly names, such as 
$\_.
As is the case with Python, any user can extend and/or modify Ruby. Ruby 
is culturally interesting because it is the first programming language designed in Japan that has achieved relatively widespread use in the United States.
2\.18\.6 Origins and Characteristics of Lua
Lua15 was designed in the early 1990s by Roberto Ierusalimschy, Waldemar 
Celes, and Luis Henrique de Figueiredo at the Pontifical University of Rio de Janeiro in Brazil. It is a scripting language that supports procedural and 
 14\. Actully, Python’s support for object\-oriented programming is partial.
 15\. The name Lua is derived from the Portuguese word for moon.functional programming with extensibility as one of its primary goals. Among 
the languages that influenced its design are Scheme, Icon, and Python.
Lua is similar to JavaScript in that it does not support object\-oriented 
programming but it was clearly influenced by it. Both have objects that play the role of both classes and objects and both have prototype inheritance rather than class inheritance. However, in Lua, the language can be extended to sup\-port object\-oriented programming.
As in Scheme, Lua’s functions are first\-class values. Also, Lua supports 
closures. These capabilities allow it to be used for functional programming. Also like Scheme, Lua has only a single data structure, although in Lua’s case, it is the table. Lua’s tables extend PHP’s associate arrays, which subsume the arrays of traditional imperative languages. References to table elements can take the form of references to traditional arrays, associative arrays, or records. Because functions are first\-class values, they can be stored in tables, and such tables can serve as namespaces.
Lua uses garbage collection for its objects, which are all heap allocated. It 
uses dynamic typing, as do most of the other scripting languages.
Lua is a relatively small and simple language, having only 21 reserved 
words. The design philosophy of the language was to provide the bare essentials and relatively simple ways to extend the language to allow it to fit a variety of application areas. Much of its extensibility derives from its table data structure, which can be customized using Lua’s metatable concept.
Lua can conveniently be used as a scripting language extension to other 
languages. Like early implementations of Java, Lua is translated to an interme\-diate code and interpreted. It easily can be embedded simply in other systems, in part because of the small size of its interpreter, which is only about 150K bytes.
During 2006 and 2007, the popularity of Lua grew rapidly, in part due to 
its use in the gaming industry. The sequence of scripting languages that have appeared over the past 20 years has already produced several widely used lan\-guages. Lua, the latest arrival among them, is quickly becoming one.
2\.19 The Flagship .NET Language: C\#
C\#, along with the new development platform .NET,16 was announced by 
Microsoft in 2000\. In January 2002, production versions of both were released.
2\.19\.1 Design Process
C\# is based on C\+\+ and Java but includes some ideas from Delphi and Visual BASIC. Its lead designer, Anders Hejlsberg, also designed T urbo Pascal and Delphi, which explains the Delphi parts of the heritage of C\#.
 16\. The .NET development system is briefly discussed in Chapter 1\.2\.19 The Flagship .NET Language: C\# 101102 Chapter 2 Evolution of the Major Programming Languages
The purpose of C\# is to provide a language for component\-based software 
development, specifically for such development in the .NET Framework. In this environment, components from a variety of languages can be easily com\-bined to form systems. All of the .NET languages, which include C\#, Visual Basic .NET, Managed C\+\+, F\#, and JScript .NET,
17 use the Common T ype 
System (CTS). The CTS provides a common class library. All types in all five .NET languages inherit from a single class root, 
System.Object . Compilers 
that conform to the CTS specification create objects that can be combined into software systems. All .NET languages are compiled into the same intermedi\-ate form, Intermediate Language (IL).
18 Unlike Java, however, the IL is never 
interpreted. A Just\-in\-Time compiler is used to translate IL into machine code before it is executed.
2\.19\.2 Language Overview
Many believe that one of Java’s most important advances over C\+\+ lies in the fact that it excludes some of C\+\+’s features. For example, C\+\+ supports multiple inheritance, pointers, structs, 
enum types, operator overloading, and a goto 
statement, but Java includes none of these. The designers of C\# obviously disagreed with this wholesale removal of features, because all of these except multiple inheritance have been included in the new language.
T o the credit of C\#’s designers, however, in several cases, the C\# version of 
a C\+\+ feature has been improved. For example, the 
enum types of C\# are safer 
than those of C\+\+, because they are never implicitly converted to integers. This allows them to be more type safe. The 
struct type was changed significantly, 
resulting in a truly useful construct, whereas in C\+\+ it serves virtually no pur\-pose. C\#’s structs are discussed in Chapter 12\. C\# takes a stab at improving the 
switch statement that is used in C, C\+\+, and Java. C\#’s switch is discussed in 
Chapter 8\.
Although C\+\+ includes function pointers, they share the lack of safety that 
is inherent in C\+\+’s pointers to variables. C\# includes a new type, delegates, which are both object\-oriented and type\-safe method references to subpro\-grams. Delegates are used for implementing event handlers, controlling the execution of threads, and callbacks.
19 Callbacks are implemented in Java with 
interfaces; in C\+\+, method pointers are used.
In C\#, methods can take a variable number of parameters, as long as they 
are all the same type. This is specified by the use of a formal parameter of array type, preceded by the 
params reserved word.
Both C\+\+ and Java use two distinct typing systems: one for primitives and 
one for objects. In addition to being confusing, this leads to a frequent need to 
 17\. Many other languages have been modified to be .NET languages.
 18\. Initially, IL was called MSIL (Microsoft Intermediate Language), but apparently many 
people thought that name was too long.
 19\. When an object calls a method of another object and needs to be notified when that method 
has completed its task, the called method calls its caller back. This is known as a callback.convert values between the two systems—for example, to put a primitive value 
into a collection that stores objects. C\# makes the conversion between values of the two typing systems partially implicit through the implicit boxing and unboxing operations, which are discussed in detail in Chapter 12\.
20
Among the other features of C\# are rectangular arrays, which are not sup\-
ported in most programming languages, and a foreach statement, which is an 
iterator for arrays and collection objects. A similar foreach statement is found 
in Perl, PHP , and Java 5\.0\. Also, C\# includes properties, which are an alterna\-tive to public data members. Properties are specified as data members with get and set methods, which are implicitly called when references and assignments are made to the associated data members.
C\# has evolved continuously and quickly from its initial release in 2002\. 
The most recent version is C\# 2010\. C\# 2010 adds a form of dynamic typing, implicit typing, and anonymous types (see Chapter 6\).
2\.19\.3 Evaluation
C\# was meant to be an improvement over both C\+\+ and Java as a general\-purpose programming language. Although it can be argued that some of its features are a step backward, C\# clearly includes some constructs that move it beyond its predecessors. Some of its features will surely be adopted by pro\-gramming languages of the near future. Some already do.
The following is an example of a C\# program:
// C\# Example Program
// Input: An integer, listlen, where listlen is less than
// 100, followed by listlen\-integer values.
// Output: The number of input values that are greater 
// than the average of all input values.
using System;
public class Ch2example {
 static void Main() {
 int\[] intlist;
 int listlen,
 counter,
 sum \= 0,
 average,
 result \= 0; intList \= new int\[99];
 listlen \= Int32\.Parse(Console.readLine());
 if ((listlen \> 0\) \&\& (listlen \< 100\)) {
// Read input into an array and compute the sum
 for (counter \= 0; counter \< listlen; counter\+\+) {
 20\. This feature was added to Java in Java 5\.0\.2\.19 The Flagship .NET Language: C\# 103104 Chapter 2 Evolution of the Major Programming Languages
 intList\[counter] \= 
 Int32\.Parse(Console.readLine()); sum \+\= intList\[counter]; } //\- end of for (counter ...// Compute the average
 average \= sum / listlen;
// Count the input values that are \> average
 foreach (int num in intList) 
 if (num \> average) result\+\+;
// Print result 
 Console.WriteLine(
 "Number of values \> average is:" \+ result);
 } //\- end of if ((listlen ...
 else
 Console.WriteLine( "Error\-\-input list length is not legal"); } //\- end of method Main
} //\- end of class Ch2example
2\.20 Markup/Programming Hybrid Languages
A markup/programming hybrid language is a markup language in which some 
of the elements can specify programming actions, such as control flow and computation. The following subsections introduce two such hybrid languages, XSL T and JSP .
2\.20\.1 XSLT
eXtensible Markup Language (XML) is a metamarkup language. Such a language is used to define markup languages. XML\-derived markup lan\-guages are used to define data documents, which are called XML docu\-ments. Although XML documents are human readable, they are processed by computers. This processing sometimes consists only of transformations to forms that can be effectively displayed or printed. In many cases, such transformations are to HTML, which can be displayed by a Web browser. In other cases, the data in the document is processed, just as with other forms of data files.
The transformation of XML documents to HTML documents is specified 
in another markup language, eXtensible Stylesheet Language T ransformations (XSL T) (
www.w3\.org/TR/XSLT ). XSL T can specify programming\-like opera\-
tions. Therefore, XSL T is a markup/programming hybrid language. XSL T was defined by the World Wide Web Consortium (W3C) in the late 1990s.
An XSL T processor is a program that takes as input an XML data docu\-
ment and an XSL T document (which is also in the form of an XML document). In this processing, the XML data document is transformed to another XML document,21 using the transformations described in the XSL T document. The 
XSL T document specifies transformations by defining templates, which are data patterns that could be found by the XSL T processor in the XML input file. Associated with each template in the XSL T document are its transformation instructions, which specify how the matching data is to be transformed before being put in the output document. So, the templates (and their associated pro\-cessing) act as subprograms, which are “executed” when the XSL T processor finds a pattern match in the data of the XML document.
XSL T also has programming constructs at a lower level. For example, a 
looping construct is included, which allows repeated parts of the XML docu\-ment to be selected. There is also a sort process. These lower\-level constructs are specified with XSL T tags, such as 
 .
2\.20\.2 JSP
The “core” part of the Java Server Pages Standard T ag Library ( JSTL) is another markup/programming hybrid language, although its form and pur\-pose are different from those of XSL T. Before discussing JSTL, it is necessary to introduce the ideas of servlets and Java Server Pages ( JSP). A servlet is an 
instance of a Java class that resides on and is executed on a Web server system. The execution of a servlet is requested by a markup document being displayed by a Web browser. The servlet’s output, which is in the form of an HTML document, is returned to the requesting browser. A program that runs in the Web server process, called a servlet container , controls the execution of serv\-
lets. Servlets are commonly used for form processing and for database access.
JSP is a collection of technologies designed to support dynamic Web docu\-
ments and provide other processing needs of Web documents. When a JSP document, which is often a mixture of HTML and Java, is requested by a browser, the JSP processor program, which resides on a Web server system, converts the document to a servlet. The document’s embedded Java code is copied to the servlet. The plain HTML is copied into Java print statements that output it as is. The JSTL markup in the JSP document is processed, as discussed in the following paragraph. The servlet produced by the JSP proces\-sor is run by the servlet container.
The JSTL defines a collection of XML action elements that control the 
processing of the JSP document on the Web server. These elements have the same form as other elements of HTML and XML. One of the most commonly used JSTL control action elements is 
if, which specifies a Boolean expression 
as an attribute.22 The content of the if element (the text between the opening 
tag ( ) and its closing tag (  )) is HTML code that will be included 
in the output document only if the Boolean expression evaluates to true. The 
if element is related to the C/C\+\+ \#if preprocessor command. The JSP 
 21\. The output document of the XSL T processor could also be in HTML or plain text.2\.20 Markup/Programming Hybrid Languages 105
 22\. An attribute in HTML, which is embedded in the opening tag of an element, provides further 
information about that element.106 Chapter 2 Evolution of the Major Programming Languages
container processes the JSTL parts of JSP documents in a way that is similar to 
how the C/C\+\+ preprocessor processes C and C\+\+ programs. The preprocessor commands are instructions for the preprocessor to specify how the output file is to be constructed from the input file. Similarly, JSTL control action elements are instructions for the JSP processor to specify how to build the XML output file from the XML input file.
One common use of the 
if element is for the validation of form data 
submitted by a browser user. Form data is accessible by the JSP processor and can be tested with the 
if element to ensure that it is sensible data. If not, the 
if element can insert an error message for the user in the output document.
For multiple selection control, JSTL has choose , when , and otherwise 
elements. JSTL also includes a forEach element, which iterates over collec\-
tions, which typically are form values from a client. The forEach element can 
include begin , end, and step attributes to control its iterations.
SUMMARY
We have investigated the development and the development environments of a number of programming languages. This chapter gives the reader a good perspective on current issues in language design. We have set the stage for an in\-depth discussion of the important features of contemporary languages.
BIBLIOGRAPHIC NOTES
Perhaps the most important source of historical information about the devel\-opment of early programming languages is History of Programming Languages, 
edited by Richard Wexelblat (1981\). It contains the developmental background and environment of 13 important programming languages, as told by the design\-ers themselves. A similar work resulted from a second “history” conference, pub\-lished as a special issue of ACM SIGPLAN Notices (ACM, 1993a). In this work, 
the history and evolution of 13 more programming languages are discussed.
The paper “Early Development of Programming Languages” (Knuth and 
Pardo, 1977\), which is part of the Encyclopedia of Computer Science and T echnology, 
is an excellent 85\-page work that details the development of languages up to and including Fortran. The paper includes example programs to demonstrate the features of many of those languages.
Another book of great interest is Programming Languages: History and Fun\-
damentals, by Jean Sammet (1969\). It is a 785\-page work filled with details of 
80 programming languages of the 1950s and 1960s. Sammet has also pub\-lished several updates to her book, such as Roster of Programming Languages for 
1974–75 (1976\).REVIEW QUESTIONS
 1\. In what year was Plankalkül designed? In what year was that design 
published? 
 2\. What two common data structures were included in Plankalkül? 3\. How were the pseudocodes of the early 1950s implemented? 4\. Speedcoding was invented to overcome two significant shortcomings of 
the computer hardware of the early 1950s. What were they?
 5\. Why was the slowness of interpretation of programs acceptable in the 
early 1950s?
 6\. What hardware capability that first appeared in the IBM 704 computer 
strongly affected the evolution of programming languages? Explain why.
 7\. In what year was the Fortran design project begun? 8\. What was the primary application area of computers at the time Fortran 
was designed?
 9\. What was the source of all of the control flow statements of Fortran I? 10\. What was the most significant feature added to Fortran I to get Fortran 
II?
 11\. What control flow statements were added to Fortran IV to get Fortran 
77?
 12\. Which version of Fortran was the first to have any sort of dynamic 
variables?
 13\. Which version of Fortran was the first to have character string handling? 14\. Why were linguists interested in artificial intelligence in the late 1950s? 15\. Where was LISP developed? By whom? 16\. In what way are Scheme and Common LISP opposites of each other? 17\. What dialect of LISP is used for introductory programming courses at 
some universities?
 18\. What two professional organizations together designed ALGOL 60? 19\. In what version of ALGOL did block structure appear? 20\. What missing language element of ALGOL 60 damaged its chances for 
widespread use?
 21\. What language was designed to describe the syntax of ALGOL 60? 22\. On what language was COBOL based? 23\. In what year did the COBOL design process begin? 24\. What data structure that appeared in COBOL originated with 
Plankalkül?
 25\. What organization was most responsible for the early success of 
COBOL (in terms of extent of use)?Review Questions 107108 Chapter 2 Evolution of the Major Programming Languages
 26\. What user group was the target of the first version of BASIC?
 27\. Why was BASIC an important language in the early 1980s? 28\. PL/I was designed to replace what two languages? 29\. For what new line of computers was PL/I designed? 30\. What features of SIMULA 67 are now important parts of some object\-
oriented languages?
 31\. What innovation of data structuring was introduced in ALGOL 68 but is 
often credited to Pascal?
 32\. What design criterion was used extensively in ALGOL 68? 33\. What language introduced the 
case statement?
 34\. What operators in C were modeled on similar operators in ALGOL 68? 35\. What are two characteristics of C that make it less safe than Pascal? 36\. What is a nonprocedural language? 37\. What are the two kinds of statements that populate a Prolog database? 38\. What is the primary application area for which Ada was designed? 39\. What are the concurrent program units of Ada called? 40\. What Ada construct provides support for abstract data types? 41\. What populates the Smalltalk world? 42\. What three concepts are the basis for object\-oriented programming? 43\. Why does C\+\+ include the features of C that are known to be unsafe? 44\. From what language does Objective\-C borrow its syntax for method 
calls?
 45\. What programming paradigm that nearly all recently designed languages 
support is not supported by Go?
 46\. What is the primary application for Objective\-C? 47\. What language designer worked on both C and Go? 48\. What do the Ada and COBOL languages have in common? 49\. What was the first application for Java? 50\. What characteristic of Java is most evident in JavaScript? 51\. How does the typing system of PHP and JavaScript differ from that of 
Java?
 52\. What array structure is included in C\# but not in C, C\+\+, or Java? 53\. What two languages was the original version of Perl meant to replace? 54\. For what application area is JavaScript most widely used? 55\. What is the relationship between JavaScript and PHP , in terms of their 
use?
 56\. PHP’s primary data structure is a combination of what two data struc\-
tures from other languages? 57\. What data structure does Python use in place of arrays?
 58\. What characteristic does Ruby share with Smalltalk? 59\. What characteristic of Ruby’s arithmetic operators makes them unique 
among those of other languages?
 60\. What data structures are built into Lua? 61\. Is Lua normally compiled, purely interpreted, or impurely interpreted? 62\. What feature of Delphi’s classes is included in C\#? 63\. What deficiency of the 
switch statement of C is addressed with the 
changes made by C\# to that statement?
 64\. What is the primary platform on which C\# is used? 65\. What are the inputs to an XSL T processor? 66\. What is the output of an XSL T processor? 67\. What element of the JSTL is related to a subprogram? 68\. T o what is a JSP document converted by a JSP processor? 69\. Where are servlets executed?
PROBLEM SET
 1\. What features of Plankalkül do you think would have had the greatest 
influence on Fortran 0 if the Fortran designers had been familiar with Plankalkül?
 2\. Determine the capabilities of Backus’s 701 Speedcoding system, and 
compare them with those of a contemporary programmable hand calculator.
 3\. Write a short history of the A\-0, A\-1, and A\-2 systems designed by 
Grace Hopper and her associates.
 4\. As a research project, compare the facilities of Fortran 0 with those of 
the Laning and Zierler system.
 5\. Which of the three original goals of the ALGOL design committee, in 
your opinion, was most difficult to achieve at that time?
 6\. Make an educated guess as to the most common syntax error in LISP 
programs.
 7\. LISP began as a pure functional language but gradually acquired more 
and more imperative features. Why?
 8\. Describe in detail the three most important reasons, in your opinion, 
why ALGOL 60 did not become a very widely used language.
 9\. Why, in your opinion, did COBOL allow long identifiers when Fortran 
and ALGOL did not?Problem Set 109110 Chapter 2 Evolution of the Major Programming Languages
 10\. Outline the major motivation of IBM in developing PL/I.
 11\. Was IBM’s assumption, on which it based its decision to develop PL/I, 
correct, given the history of computers and language developments since 1964?
 12\. Describe, in your own words, the concept of orthogonality in program\-
ming language design.
 13\. What is the primary reason why PL/I became more widely used than 
ALGOL 68?
 14\. What are the arguments both for and against the idea of a typeless 
language?
 15\. Are there any logic programming languages other than Prolog? 16\. What is your opinion of the argument that languages that are too com\-
plex are too dangerous to use, and we should therefore keep all languages small and simple?
 17\. Do you think language design by committee is a good idea? Support 
your opinion.
 18\. Languages continually evolve. What sort of restrictions do you think 
are appropriate for changes in programming languages? Compare your answers with the evolution of Fortran.
 19\. Build a table identifying all of the major language developments, 
together with when they occurred, in what language they first appeared, and the identities of the developers.
 20\. There have been some public interchanges between Microsoft and 
Sun concerning the design of Microsoft’s J\+\+ and C\# and Sun’s Java. Read some of these documents, which are available on their respective Web sites, and write an analysis of the disagreements concerning the delegates.
 21\. In recent years data structures have evolved within scripting languages 
to replace traditional arrays. Explain the chronological sequence of these developments.
 22\. Explain two reasons why pure interpretation is an acceptable implemen\-
tation method for several recent scripting languages.
 23\. Perl 6, when it arrives, will likely be a significantly enlarged language. 
Make an educated guess as to whether a language like Lua will also grow continuously over its lifetime. Support your answer.
 24\. Why, in your opinion, do new scripting languages appear more fre\-
quently than new compiled languages?
 25\. Give a brief general description of a markup/programming hybrid 
language.PROGRAMMING EXERCISES
 1\. T o understand the value of records in a programming language, write a 
small program in a C\-based language that uses an array of structs that store student information, including name, age, GPA as a float, and grade level as a string (e.g., “freshmen,” etc.). Also, write the same pro\-gram in the same language without using structs.
 2\. T o understand the value of recursion in a programming language, write a 
program that implements quicksort, first using recursion and then with\-out recursion.
 3\. T o understand the value of counting loops, write a program that imple\-
ments matrix multiplication using counting loop constructs. Then write the same program using only logical loops—for example, 
while loops.Programming Exercises 111This page intentionally left blank 113 3\.1 Introduction
 3\.2 The General Problem of Describing Syntax
 3\.3 Formal Methods of Describing Syntax
 3\.4 Attribute Grammars
 3\.5 Describing the Meanings of Programs: Dynamic Semantics3
Describing Syntax 
and Semantics114 Chapter 3 Describing Syntax and Semantics 
This chapter covers the following topics. First, the terms syntax and seman\-
tics are defined. Then, a detailed discussion of the most common method of 
describing syntax, context\-free grammars (also known as Backus\-Naur Form), 
is presented. Included in this discussion are derivations, parse trees, ambiguity, 
descriptions of operator precedence and associativity, and extended Backus\-Naur 
Form. Attribute grammars, which can be used to describe both the syntax and static semantics of programming languages, are discussed next. In the last section, three formal methods of describing semantics—operational, axiomatic, and denotational semantics—are introduced. Because of the inherent complexity of the semantics description methods, our discussion of them is brief. One could easily write an entire book on just one of the three (as several authors have).
3\.1 Introduction
The task of providing a concise yet understandable description of a program\-
ming language is difficult but essential to the language’s success. ALGOL 60 and ALGOL 68 were first presented using concise formal descriptions; in both cases, however, the descriptions were not easily understandable, partly because each used a new notation. The levels of acceptance of both languages suffered as a result. On the other hand, some languages have suffered the problem of having many slightly different dialects, a result of a simple but informal and imprecise definition.
One of the problems in describing a language is the diversity of the peo\-
ple who must understand the description. Among these are initial evaluators, implementors, and users. Most new programming languages are subjected to a period of scrutiny by potential users, often people within the organization that employs the language’s designer, before their designs are completed. These are the initial evaluators. The success of this feedback cycle depends heavily on the clarity of the description.
Programming language implementors obviously must be able to deter\-
mine how the expressions, statements, and program units of a language are formed, and also their intended effect when executed. The difficulty of the implementors’ job is, in part, determined by the completeness and precision of the language description.
Finally, language users must be able to determine how to encode software 
solutions by referring to a language reference manual. T extbooks and courses enter into this process, but language manuals are usually the only authoritative printed information source about a language.
The study of programming languages, like the study of natural languages, 
can be divided into examinations of syntax and semantics. The 
syntax of a 
programming language is the form of its expressions, statements, and program units. Its semantics is the meaning of those expressions, statements, and pro\-
gram units. For example, the syntax of a Java 
while statement is
while (boolean\_expr) statementThe semantics of this statement form is that when the current value of the 
Boolean expression is true, the embedded statement is executed. Otherwise, control continues after the 
while construct. Then control implicitly returns 
to the Boolean expression to repeat the process.
Although they are often separated for discussion purposes, syntax and 
semantics are closely related. In a well\-designed programming language, semantics should follow directly from syntax; that is, the appearance of a state\-ment should strongly suggest what the statement is meant to accomplish.
Describing syntax is easier than describing semantics, partly because a con\-
cise and universally accepted notation is available for syntax description, but none has yet been developed for semantics.
3\.2 The General Problem of Describing Syntax
A language, whether natural (such as English) or artificial (such as Java), is a set 
of strings of characters from some alphabet. The strings of a language are called sentences or statements. The syntax rules of a language specify which strings 
of characters from the language’s alphabet are in the language. English, for example, has a large and complex collection of rules for specifying the syntax of its sentences. By comparison, even the largest and most complex programming languages are syntactically very simple.
Formal descriptions of the syntax of programming languages, for sim\-
plicity’s sake, often do not include descriptions of the lowest\-level syntactic units. These small units are called lexemes . The description of lexemes can 
be given by a lexical specification, which is usually separate from the syntactic description of the language. The lexemes of a programming language include its numeric literals, operators, and special words, among others. One can think of programs as strings of lexemes rather than of characters.
Lexemes are partitioned into groups—for example, the names of variables, 
methods, classes, and so forth in a programming language form a group called identifiers. Each lexeme group is represented by a name, or token. So, a token 
of a language is a category of its lexemes. For example, an identifier is a token that can have lexemes, or instances, such as 
sum and total . In some cases, a 
token has only a single possible lexeme. For example, the token for the arith\-metic operator symbol 
\+ has just one possible lexeme. Consider the following 
Java statement:
index \= 2 \* count \+ 17;
The lexemes and tokens of this statement are
Lexemes T okens
index identifier
\= equal\_sign
2 int\_literal3\.2 The General Problem of Describing Syntax 115116 Chapter 3 Describing Syntax and Semantics 
\* mult\_op
count identifier
\+ plus\_op
17 int\_literal
; semicolon
The example language descriptions in this chapter are very simple, and most 
include lexeme descriptions.
3\.2\.1 Language Recognizers
In general, languages can be formally defined in two distinct ways: by recognition 
and by generation (although neither provides a definition that is practical by itself 
for people trying to learn or use a programming language). Suppose we have a language L that uses an alphabet 
/H9018 of characters. T o define L formally using the 
recognition method, we would need to construct a mechanism R, called a recogni\-tion device, capable of reading strings of characters from the alphabet /H9018\. R would 
indicate whether a given input string was or was not in L. In effect, R would either accept or reject the given string. Such devices are like filters, separating legal sentences from those that are incorrectly formed. If R, when fed any string of characters over /H9018, accepts it only if it is in L, then R is a description of L. Because 
most useful languages are, for all practical purposes, infinite, this might seem like a lengthy and ineffective process. Recognition devices, however, are not used to enumerate all of the sentences of a language—they have a different purpose.
The syntax analysis part of a compiler is a recognizer for the language the 
compiler translates. In this role, the recognizer need not test all possible strings of characters from some set to determine whether each is in the language. Rather, it need only determine whether given programs are in the language. In effect then, the syntax analyzer determines whether the given programs are syntactically correct. The structure of syntax analyzers, also known as parsers, is discussed in Chapter 4\.
3\.2\.2 Language Generators
A language generator is a device that can be used to generate the sentences of a language. We can think of the generator as having a button that produces a sentence of the language every time it is pushed. Because the particular sentence that is produced by a generator when its button is pushed is unpredictable, a generator seems to be a device of limited usefulness as a language descriptor. However, people prefer certain forms of generators over recognizers because they can more easily read and understand them. By contrast, the syntax\-checking portion of a compiler (a language recognizer) is not as useful a language descrip\-tion for a programmer because it can be used only in trial\-and\-error mode. For example, to determine the correct syntax of a particular statement using a com\-piler, the programmer can only submit a speculated version and note whether the compiler accepts it. On the other hand, it is often possible to determine 
whether the syntax of a particular statement is correct by comparing it with the structure of the generator.
There is a close connection between formal generation and recognition 
devices for the same language. This was one of the seminal discoveries in com\-puter science, and it led to much of what is now known about formal languages and compiler design theory. We return to the relationship of generators and recognizers in the next section.
3\.3 Formal Methods of Describing Syntax
This section discusses the formal language\-generation mechanisms, usually 
called grammars , that are commonly used to describe the syntax of program\-
ming languages.
3\.3\.1 Backus\-Naur Form and Context\-Free Grammars
In the middle to late 1950s, two men, Noam Chomsky and John Backus, in unrelated research efforts, developed the same syntax description formalism, which subsequently became the most widely used method for programming language syntax.
3\.3\.1\.1 Context\-Free Grammars
In the mid\-1950s, Chomsky, a noted linguist (among other things), described four classes of generative devices or grammars that define four classes of languages (Chomsky, 1956, 1959\). T wo of these grammar classes, named context\-free and regular, turned out to be useful for describing the syntax of 
programming languages. The forms of the tokens of programming languages can be described by regular grammars. The syntax of whole programming languages, with minor exceptions, can be described by context\-free grammars. Because Chomsky was a linguist, his primary interest was the theoretical nature of natural languages. He had no interest at the time in the artificial languages used to communicate with computers. So it was not until later that his work was applied to programming languages.
3\.3\.1\.2 Origins of Backus\-Naur Form
Shortly after Chomsky’s work on language classes, the ACM\-GAMM group began designing ALGOL 58\. A landmark paper describing ALGOL 58 was presented by John Backus, a prominent member of the ACM\-GAMM group, at an international conference in 1959 (Backus, 1959\). This paper introduced a new formal notation for specifying programming language syntax. The new notation was later modified slightly by Peter Naur for the description of 3\.3 Formal Methods of Describing Syntax 117118 Chapter 3 Describing Syntax and Semantics 
ALGOL 60 (Naur, 1960\). This revised method of syntax description became 
known as Backus\-Naur Form , or simply BNF .
BNF is a natural notation for describing syntax. In fact, something similar 
to BNF was used by Panini to describe the syntax of Sanskrit several hundred years before Christ (Ingerman, 1967\).
Although the use of BNF in the ALGOL 60 report was not immediately 
accepted by computer users, it soon became and is still the most popular method of concisely describing programming language syntax.
It is remarkable that BNF is nearly identical to Chomsky’s generative 
devices for context\-free languages, called context\-free grammars . In the 
remainder of the chapter, we refer to context\-free grammars simply as gram\-mars. Furthermore, the terms BNF and grammar are used interchangeably.
3\.3\.1\.3 Fundamentals
A metalanguage is a language that is used to describe another language. BNF 
is a metalanguage for programming languages.
BNF uses abstractions for syntactic structures. A simple Java assignment 
statement, for example, might be represented by the abstraction  (pointed brackets are often used to delimit names of abstractions). The actual definition of  can be given by
 →  
\= 
The text on the left side of the arrow, which is aptly called the left\-hand side 
(LHS), is the abstraction being defined. The text to the right of the arrow is the definition of the LHS. It is called the right\-hand side (RHS) and con\-
sists of some mixture of tokens, lexemes, and references to other abstractions. (Actually, tokens are also abstractions.) Altogether, the definition is called a rule, or production . In the example rule just given, the abstractions  
and  obviously must be defined for the  definition to be useful.
This particular rule specifies that the abstraction  is defined as 
an instance of the abstraction , followed by the lexeme 
\=, followed by an 
instance of the abstraction . One example sentence whose syntactic structure is described by the rule is
total \= subtotal1 \+ subtotal2
The abstractions in a BNF description, or grammar, are often called nonter\-
minal symbols , or simply nonterminals , and the lexemes and tokens of the 
rules are called terminal symbols , or simply terminals . A BNF description, 
or grammar , is a collection of rules.
Nonterminal symbols can have two or more distinct definitions, represent\-
ing two or more possible syntactic forms in the language. Multiple definitions 
can be written as a single rule, with the different definitions separated by the symbol \|, meaning logical OR. For example, a Java if statement can be 
described with the rules
 → if (  ) 
 → if (  )  else 
or with the rule → 
if (  ) 
 \|if (  )  else 
In these rules,  represents either a single statement or a compound 
statement.
Although BNF is simple, it is sufficiently powerful to describe nearly all 
of the syntax of programming languages. In particular, it can describe lists of similar constructs, the order in which different constructs must appear, and nested structures to any depth, and even imply operator precedence and opera\-tor associativity.
3\.3\.1\.4 Describing Lists
Variable\-length lists in mathematics are often written using an ellipsis (. . .); 1, 2, . . . is an example. BNF does not include the ellipsis, so an alternative 
method is required for describing lists of syntactic elements in programming languages (for example, a list of identifiers appearing on a data declaration statement). For BNF , the alternative is recursion. A rule is recursive if its 
LHS appears in its RHS. The following rules illustrate how recursion is used to describe lists:
 → identifier
 
\| identifier, 
This defines  as either a single token (identifier) or an identifier 
followed by a comma and another instance of . Recursion is used to describe lists in many of the example grammars in the remainder of this chapter.
3\.3\.1\.5 Grammars and Derivations
A grammar is a generative device for defining languages. The sentences of the language are generated through a sequence of applications of the rules, beginning with a special nonterminal of the grammar called the start sym\-
bol. This sequence of rule applications is called a derivation . In a grammar 
for a complete programming language, the start symbol represents a com\-plete program and is often named . The simple grammar shown in 
Example 3\.1 is used to illustrate derivations.3\.3 Formal Methods of Describing Syntax 119120 Chapter 3 Describing Syntax and Semantics 
EXAMPLE 3\.1 A Grammar for a Small Language
 → begin  end 
 → 
 \|  ; 
 →  \= 
 → A \| B \| C
 →  \+ 
 \|  – 
 \| 
The language described by the grammar of Example 3\.1 has only one state\-
ment form: assignment. A program consists of the special word begin , fol\-
lowed by a list of statements separated by semicolons, followed by the special word 
end. An expression is either a single variable or two variables separated 
by either a \+ or \- operator. The only variable names in this language are A, 
B, and C.
A derivation of a program in this language follows:
 \=\> begin  end
 \=\> begin  ;  end
 \=\> begin  \=  ;  end
 \=\> begin A \=  ;  end
 \=\> begin A \=  \+  ;  end
 \=\> begin A \= B \+  ;  end
 \=\> begin A \= B \+ C ;  end
 \=\> begin A \= B \+ C ;  end
 \=\> begin A \= B \+ C ;  \=  end
 \=\> begin A \= B \+ C ; B \=  end
 \=\> begin A \= B \+ C ; B \=  end
 \=\> begin A \= B \+ C ; B \= C end
This derivation, like all derivations, begins with the start symbol, in this case 
. The symbol \=\> is read “derives.” Each successive string in the 
sequence is derived from the previous string by replacing one of the nonter\-minals with one of that nonterminal’s definitions. Each of the strings in the derivation, including , is called a sentential form .
In this derivation, the replaced nonterminal is always the leftmost non\-
terminal in the previous sentential form. Derivations that use this order of replacement are called leftmost derivations . The derivation continues until 
the sentential form contains no nonterminals. That sentential form, consisting of only terminals, or lexemes, is the generated sentence.3\.3 Formal Methods of Describing Syntax 121
In addition to leftmost, a derivation may be rightmost or in an order that is 
neither leftmost nor rightmost. Derivation order has no effect on the language generated by a grammar.
By choosing alternative RHSs of rules with which to replace nonterminals 
in the derivation, different sentences in the language can be generated. By exhaustively choosing all combinations of choices, the entire language can be generated. This language, like most others, is infinite, so one cannot generate all the sentences in the language in finite time.
Example 3\.2 is another example of a grammar for part of a typical program\-
ming language.
EXAMPLE 3\.2 A Grammar for Simple Assignment Statements
 →  \= 
 → A \| B \| C 
 →  \+ 
 \|  \* 
 \| (  )
 \|  
The grammar of Example 3\.2 describes assignment statements whose right 
sides are arithmetic expressions with multiplication and addition operators and parentheses. For example, the statement
A \= B \* ( A \+ C )
is generated by the leftmost derivation:
 \=\>  \= 
 \=\> A \= 
 \=\> A \=  \* 
 \=\> A \= B \* 
 \=\> A \= B \* (  )
 \=\> A \= B \* (  \+  )
 \=\> A \= B \* ( A \+  )
 \=\> A \= B \* ( A \+  )
 \=\> A \= B \* ( A \+ C )
3\.3\.1\.6 Parse Trees
One of the most attractive features of grammars is that they naturally describe 
the hierarchical syntactic structure of the sentences of the languages they define. These hierarchical structures are called parse trees . For example, the parse tree 
in Figure 3\.1 shows the structure of the assignment statement derived previously.122 Chapter 3 Describing Syntax and Semantics 
Figure 3\.1
A parse tree for the 
simple statement 
A \= B \* (A \+ C)

A
A\= 

B\* 


C\+ ()
Every internal node of a parse tree is labeled with a nonterminal sym\-
bol; every leaf is labeled with a terminal symbol. Every subtree of a parse tree describes one instance of an abstraction in the sentence.
3\.3\.1\.7 Ambiguity
A grammar that generates a sentential form for which there are two or more distinct parse trees is said to be ambiguous . Consider the grammar shown in 
Example 3\.3, which is a minor variation of the grammar shown in Example 3\.2\.
EXAMPLE 3\.3 An Ambiguous Grammar for Simple Assignment Statements
 →  \= 
 → A \| B \| C
 →  \+ 
 \|  \* 
 \| (  )
 \|  
The grammar of Example 3\.3 is ambiguous because the sentence
A \= B \+ C \* A
has two distinct parse trees, as shown in Figure 3\.2\. The ambiguity occurs because 
the grammar specifies slightly less syntactic structure than does the grammar of 3\.3 Formal Methods of Describing Syntax 123
Example 3\.2\. Rather than allowing the parse tree of an expression to grow only 
on the right, this grammar allows growth on both the left and the right.
Figure 3\.2
Two distinct parse trees 
for the same sentence, 
A \= B \+ C \* A

A
CB\= 

\+ 

A\*  

A
B\= 
 \* 

C A \+  
Syntactic ambiguity of language structures is a problem because compilers 
often base the semantics of those structures on their syntactic form. Specifically, the compiler chooses the code to be generated for a statement by examining its parse tree. If a language structure has more than one parse tree, then the mean\-ing of the structure cannot be determined uniquely. This problem is discussed in two specific examples in the following subsections.
There are several other characteristics of a grammar that are sometimes 
useful in determining whether a grammar is ambiguous.
1 They include the fol\-
lowing: (1\) if the grammar generates a sentence with more than one leftmost derivation and (2\) if the grammar generates a sentence with more than one rightmost derivation.
Some parsing algorithms can be based on ambiguous grammars. When 
such a parser encounters an ambiguous construct, it uses nongrammatical infor\-mation provided by the designer to construct the correct parse tree. In many cases, an ambiguous grammar can be rewritten to be unambiguous but still generate the desired language.
3\.3\.1\.8 Operator Precedence
When an expression includes two different operators, for example, x \+ y \* z , 
one obvious semantic issue is the order of evaluation of the two operators (for example, in this expression is it add and then multiply, or vice versa?). This seman\-tic question can be answered by assigning different precedence levels to operators. For example, if 
\* has been assigned higher precedence than \+ (by the language 
 1\. Note that it is mathematically impossible to determine whether an arbitrary grammar is 
ambiguous.124 Chapter 3 Describing Syntax and Semantics 
designer), multiplication will be done first, regardless of the order of appearance 
of the two operators in the expression.
As stated previously, a grammar can describe a certain syntactic structure so 
that part of the meaning of the structure can be determined from its parse tree. In particular, the fact that an operator in an arithmetic expression is generated lower in the parse tree (and therefore must be evaluated first) can be used to indicate that it has precedence over an operator produced higher up in the tree. In the first parse tree of Figure 3\.2, for example, the multiplication operator is generated lower in the tree, which could indicate that it has precedence over the addition operator in the expression. The second parse tree, however, indi\-cates just the opposite. It appears, therefore, that the two parse trees indicate conflicting precedence information.
Notice that although the grammar of Example 3\.2 is not ambiguous, the 
precedence order of its operators is not the usual one. In this grammar, a parse tree of a sentence with multiple operators, regardless of the particular operators involved, has the rightmost operator in the expression at the lowest point in the parse tree, with the other operators in the tree moving progres\-sively higher as one moves to the left in the expression. For example, in the expression 
A \+ B \* C , \* is the lowest in the tree, indicating it is to be done 
first. However, in the expression A \* B \+ C , \+ is the lowest, indicating it is 
to be done first.
A grammar can be written for the simple expressions we have been dis\-
cussing that is both unambiguous and specifies a consistent precedence of the 
\+ and \* operators, regardless of the order in which the operators appear in an 
expression. The correct ordering is specified by using separate nonterminal symbols to represent the operands of the operators that have different pre\-cedence. This requires additional nonterminals and some new rules. Instead of using  for both operands of both 
\+ and \*, we could use three non\-
terminals to represent operands, which allows the grammar to force different operators to different levels in the parse tree. If  is the root symbol for expressions, 
\+ can be forced to the top of the parse tree by having  
directly generate only \+ operators, using the new nonterminal, , as 
the right operand of \+. Next, we can define  to generate \* operators, 
using  as the left operand and a new nonterminal, , as its right operand. Now, 
\* will always be lower in the parse tree, simply because it is 
farther from the start symbol than \+ in every derivation. The grammar of 
Example 3\.4 is such a grammar.3\.3 Formal Methods of Describing Syntax 125
The grammar in Example 3\.4 generates the same language as the grammars of 
Examples 3\.2 and 3\.3, but it is unambiguous and it specifies the usual prece\-dence order of multiplication and addition operators. The following derivation of the sentence 
A \= B \+ C \* A uses the grammar of Example 3\.4:
 \=\>  \= 
 \=\> A \= 
 \=\> A \=  \+ 
 \=\> A \=  \+ 
 \=\> A \=  \+ 
 \=\> A \=  \+ 
 \=\> A \= B \+ 
 \=\> A \= B \+  \* 
 \=\> A \= B \+  \* 
 \=\> A \= B \+  \* 
 \=\> A \= B \+ C \* 
 \=\> A \= B \+ C \* 
 \=\> A \= B \+ C \* A
The unique parse tree for this sentence, using the grammar of Example 3\.4, is shown in Figure 3\.3\.
The connection between parse trees and derivations is very close: Either 
can easily be constructed from the other. Every derivation with an unambigu\-ous grammar has a unique parse tree, although that tree can be represented by different derivations. For example, the following derivation of the sentence 
A \= B \+ C \* A is different from the derivation of the same sentence given 
previously. This is a rightmost derivation, whereas the previous one is leftmost. Both of these derivations, however, are represented by the same parse tree.EXAMPLE 3\.4 An Unambiguous Grammar for Expressions
 →  \= 
 → A \| B \| C 
 →  \+ 
 \| 
 →  \* 
 \| 
 → (  )
 \|  126 Chapter 3 Describing Syntax and Semantics 
 \=\>  \= 
 \=\>  \=  \+ 
 \=\>  \=  \+  \* 
 \=\>  \=  \+  \* 
 \=\>  \=  \+  \* A
 \=\>  \=  \+  \* A
 \=\>  \=  \+  \* A
 \=\>  \=  \+ C \* A
 \=\>  \=  \+ C \* A
 \=\>  \=  \+ C \* A
 \=\>  \=  \+ C \* A
 \=\>  \= B \+ C \* A
 \=\> A \= B \+ C \* A
Figure 3\.3
The unique parse tree 
for A \= B \+ C \* A 
using an unambiguous grammar

A


C B\= 

\+ 

A\*  
 2\. An expression with two occurrences of the same operator has the same issue; for example, 
A / B / C .3\.3\.1\.9 Associativity of Operators
When an expression includes two operators that have the same precedence (as 
\* and / usually have)—for example, A / B \* C —a semantic rule is required 
to specify which should have precedence.2 This rule is named associativity.3\.3 Formal Methods of Describing Syntax 127
As was the case with precedence, a grammar for expressions may correctly 
imply operator associativity. Consider the following example of an assignment statement:
A \= B \+ C \+ A
The parse tree for this sentence, as defined with the grammar of Example 3\.4, is shown in Figure 3\.4\.
The parse tree of Figure 3\.4 shows the left addition operator lower than 
the right addition operator. This is the correct order if addition is meant to be left associative, which is typical. In most cases, the associativity of addition in a computer is irrelevant. In mathematics, addition is associa\-tive, which means that left and right associative orders of evaluation mean the same thing. That is, 
(A \+ B) \+ C \= A \+ (B \+ C) . Floating\-point 
addition in a computer, however, is not necessarily associative. For example, suppose floating\-point values store seven digits of accuracy. Consider the problem of adding 11 numbers together, where one of the numbers is 10
7 
and the other ten are 1\. If the small numbers (the 1’s) are each added to the large number, one at a time, there is no effect on that number, because the small numbers occur in the eighth digit of the large number. However, 
if the small numbers are first added together and the result is added to the 
large number, the result in seven\-digit accuracy is 1\. 000001 \* 10
7\. Subtrac\-
tion and division are not associative, whether in mathematics or in a com\-puter. Therefore, correct associativity may be essential for an expression that contains either of them.
Figure 3\.4
A parse tree for A \= B 
\+ C \+ A illustrating 
the associativity of addition

A\= 


B
\+  
\+

C
A128 Chapter 3 Describing Syntax and Semantics 
When a grammar rule has its LHS also appearing at the beginning of its 
RHS, the rule is said to be left recursive . This left recursion specifies left 
associativity. For example, the left recursion of the rules of the grammar of Example 3\.4 causes it to make both addition and multiplication left associa\-tive. Unfortunately, left recursion disallows the use of some important syntax analysis algorithms. When such algorithms are to be used, the grammar must be modified to remove the left recursion. This, in turn, disallows the grammar from precisely specifying that certain operators are left associative. Fortunately, left associativity can be enforced by the compiler, even though the grammar does not dictate it.
In most languages that provide it, the exponentiation operator is right asso\-
ciative. T o indicate right associativity, right recursion can be used. A grammar rule is right recursive if the LHS appears at the right end of the RHS. Rules such as
 → 
 \*\* 
 \|
 → (  )
 \|id
could be used to describe exponentiation as a right\-associative operator.
3\.3\.1\.10 An Unambiguous Grammar for if\-then\-else
The BNF rules for an Ada if\-then\-else statement are as follows:
 → if  then 
 if  then  else 
If we also have  → , this grammar is ambiguous. The simplest 
sentential form that illustrates this ambiguity is
if  then if  then  else 
The two parse trees in Figure 3\.5 show the ambiguity of this sentential form. 
Consider the following example of this construct:
if done \=\= true
 then if denom \=\= 0
 then quotient \= 0;
 else quotient \= num / denom;
The problem is that if the upper parse tree in Figure 3\.5 is used as the basis for 
translation, the else clause would be executed when done is not true, which 
probably is not what was intended by the author of the construct. We will examine the practical problems associated with this else\-association problem in Chapter 8\.
We will now develop an unambiguous grammar that describes this 
if 
statement. The rule for if constructs in many languages is that an else 
clause, when present, is matched with the nearest previous unmatched then . 3\.3 Formal Methods of Describing Syntax 129
Therefore, there cannot be an if statement without an else between a 
then and its matching else . So, for this situation, statements must be distin\-
guished between those that are matched and those that are unmatched, where unmatched statements are 
else \-less ifs and all other statements are matched. 
The problem with the earlier grammar is that it treats all statements as if they had equal syntactic significance—that is, as if they were all matched.
T o reflect the different categories of statements, different abstractions, or 
nonterminals, must be used. The unambiguous grammar based on these ideas follows:
 →  
\| 
 → if  then  else 
 \|any non\-if statement
 → if  then 
 \|if  then  else 
There is just one possible parse tree, using this grammar, for the following 
sentential form:
if  then if  then  else Figure 3\.5
Two distinct parse trees 
for the same sentential form
if  then  else 
if  then 

if  then  else if  then  130 Chapter 3 Describing Syntax and Semantics 
3\.3\.2 Extended BNF
Because of a few minor inconveniences in BNF , it has been extended in 
several ways. Most extended versions are called Extended BNF , or simply EBNF , even though they are not all exactly the same. The extensions do not enhance the descriptive power of BNF; they only increase its readability and writability.
Three extensions are commonly included in the various versions of EBNF . 
The first of these denotes an optional part of an RHS, which is delimited by brackets. For example, a C 
if\-else statement can be described as
 → if ( )  \[else  ]
Without the use of the brackets, the syntactic description of this statement 
would require the following two rules:
 → if ( ) 
 \| if ( )  else 
The second extension is the use of braces in an RHS to indicate that the 
enclosed part can be repeated indefinitely or left out altogether. This exten\-sion allows lists to be built with a single rule, instead of using recursion and two rules. For example, lists of identifiers separated by commas can be described by the following rule:
 →  
{,  }
This is a replacement of the recursion by a form of implied iteration; the part 
enclosed within braces can be iterated any number of times.
The third common extension deals with multiple\-choice options. When a 
single element must be chosen from a group, the options are placed in paren\-theses and separated by the OR operator, 
\|. For example,
 →  (\* \| / \| %) 
In BNF , a description of this  would require the following three rules: → 
 \* 
 \|  / 
 \|  % 
The brackets, braces, and parentheses in the EBNF extensions are metasym\-
bols, which means they are notational tools and not terminal symbols in the 
syntactic entities they help describe. In cases where these metasymbols are also terminal symbols in the language being described, the instances that are terminal symbols can be underlined or quoted. Example 3\.5 illustrates the use of braces and multiple choices in an EBNF grammar.3\.3 Formal Methods of Describing Syntax 131
The BNF rule
 →  \+ 
clearly specifies—in fact forces—the \+ operator to be left associative. However, 
the EBNF version,
 →  {\+ }
does not imply the direction of associativity. This problem is overcome in 
a syntax analyzer based on an EBNF grammar for expressions by designing the syntax analysis process to enforce the correct associativity. This is further discussed in Chapter 4\.
Some versions of EBNF allow a numeric superscript to be attached to the 
right brace to indicate an upper limit to the number of times the enclosed part can be repeated. Also, some versions use a plus (
\+) superscript to indicate one 
or more repetitions. For example,
 → begin  {} end
and
 → begin {}\+ end 
are equivalent.EXAMPLE 3\.5 BNF and EBNF Versions of an Expression Grammar
BNF:
  →  \+ 
 \|  \- 
 \| 
 →  \* 
 \|  / 
 \| 
 →  \*\* 
  → 
()
 \| id
EBNF:
  →  {(\+ \| \-) }
  →  { (\* \| /) }
  →  { \*\* }
  → ()
 \| id132 Chapter 3 Describing Syntax and Semantics 
In recent years, some variations on BNF and EBNF have appeared. Among 
these are the following:
• In place of the arrow, a colon is used and the RHS is placed on the next 
line.
• Instead of a vertical bar to separate alternative RHSs, they are simply 
placed on separate lines.
• In place of square brackets to indicate something being optional, the sub\-
script opt is used. For example,
Constructor Declarator → SimpleName (FormalParameterList opt)
• Rather than using the \| symbol in a parenthesized list of elements to indi\-
cate a choice, the words “one of ” are used. For example,
AssignmentOperator → one of \= \*\= /\= %\= \+\= \-\=
 \<\<\= \>\>\= \&\= ^\= \|\=
There is a standard for EBNF , ISO/IEC 14977:1996(1996\), but it is rarely 
used. The standard uses the equal sign ( \=) instead of an arrow in rules, termi\-
nates each RHS with a semicolon, and requires quotes on all terminal symbols. It also specifies a host of other notational rules.
3\.3\.3 Grammars and Recognizers
Earlier in this chapter, we suggested that there is a close relationship between generation and recognition devices for a given language. In fact, given a context\-free grammar, a recognizer for the language generated by the grammar can be algorithmically constructed. A number of software sys\-tems have been developed that perform this construction. Such systems allow the quick creation of the syntax analysis part of a compiler for a new language and are therefore quite valuable. One of the first of these syntax analyzer generators is named yacc
3 ( Johnson, 1975\). There are now many 
such systems available.
3\.4 Attribute Grammars
An attribute grammar is a device used to describe more of the structure of a 
programming language than can be described with a context\-free grammar. An attribute grammar is an extension to a context\-free grammar. The extension 
 3\. The term yacc is an acronym for “yet another compiler compiler.”3\.4 Attribute Grammars 133
allows certain language rules to be conveniently described, such 
as type compatibility. Before we formally define the form of attri\-bute grammars, we must clarify the concept of static semantics.
3\.4\.1 Static Semantics
There are some characteristics of the structure of programming languages that are difficult to describe with BNF , and some that are impossible. As an example of a syntax rule that is difficult to specify with BNF , consider type compatibility rules. In Java, for example, a floating\-point value cannot be assigned to an inte\-ger type variable, although the opposite is legal. Although this restriction can be specified in BNF , it requires additional non\-terminal symbols and rules. If all of the typing rules of Java were specified in BNF, the grammar would become too large to be useful, because the size of the grammar determines the size of the syntax analyzer.
As an example of a syntax rule that cannot be specified 
in BNF , consider the common rule that all variables must be declared before they are referenced. It has been proven that this rule cannot be specified in BNF .
These problems exemplify the categories of language rules 
called static semantics rules. The static semantics of a language is only indi\-
rectly related to the meaning of programs during execution; rather, it has to do with the legal forms of programs (syntax rather than semantics). Many static semantic rules of a language state its type constraints. Static semantics is so named because the analysis required to check these specifications can be done at compile time.
Because of the problems of describing static semantics with BNF , a variety 
of more powerful mechanisms has been devised for that task. One such mecha\-nism, attribute grammars, was designed by Knuth (1968a) to describe both the syntax and the static semantics of programs.
Attribute grammars are a formal approach both to describing and checking 
the correctness of the static semantics rules of a program. Although they are not always used in a formal way in compiler design, the basic concepts of attribute grammars are at least informally used in every compiler (see Aho et al., 1986\).
Dynamic semantics, which is the meaning of expressions, statements, and 
program units, is discussed in Section 3\.5\.
3\.4\.2 Basic Concepts
Attribute grammars are context\-free grammars to which have been added attri\-butes, attribute computation functions, and predicate functions. Attributes , 
which are associated with grammar symbols (the terminal and nonterminal symbols), are similar to variables in the sense that they can have values assigned to them. Attribute computation functions , sometimes called semantic history note
Attribute grammars have been 
used in a wide variety of appli\-cations. They have been used to provide complete descriptions 
of the syntax and static seman\-
tics of programming languages (Watt, 1979\); they have been used as the formal definition of 
a language that can be input to 
a compiler generation system (Farrow, 1982\); and they have 
been used as the basis of several 
syntax\-directed editing systems 
(Teitelbaum and Reps, 1981; 
Fischer et al., 1984\). In addi\-tion, attribute grammars have 
been used in natural\-language 
processing systems (Correa, 
1992\).134 Chapter 3 Describing Syntax and Semantics 
functions, are associated with grammar rules. They are used to specify how 
attribute values are computed. Predicate functions , which state the static 
semantic rules of the language, are associated with grammar rules.
These concepts will become clearer after we formally define attribute 
grammars and provide an example.
3\.4\.3 Attribute Grammars Defined
An attribute grammar is a grammar with the following additional features:
• Associated with each grammar symbol X is a set of attributes A(X). The 
set A(X) consists of two disjoint sets S(X) and I(X), called synthesized and inherited attributes, respectively. Synthesized attributes are used 
to pass semantic information up a parse tree, while inherited attributes 
pass semantic information down and across a tree.
• Associated with each grammar rule is a set of semantic functions and 
a possibly empty set of predicate functions over the attributes of the symbols in the grammar rule. For a rule X
0SX1 c Xn, the synthe\-
sized attributes of X 0 are computed with semantic functions of the form 
S(X 0\)\=f(A(X 1\), c , A(X n)). So the value of a synthesized attribute on 
a parse tree node depends only on the values of the attributes on that node’s children nodes. Inherited attributes of symbols X
j, 1…j…n 
(in the rule above), are computed with a semantic function of the form 
I(X j)\=f(A(X 0\), c , A(X n)). So the value of an inherited attribute on 
a parse tree node depends on the attribute values of that node’s par\-ent node and those of its sibling nodes. Note that, to avoid circular\-ity, inherited attributes are often restricted to functions of the form 
I(X
j)\=f(A(X 0\), c , A(X( j\-1\))). This form prevents an inherited attri\-
bute from depending on itself or on attributes to the right in the parse tree.
• A predicate function has the form of a Boolean expression on the union of the 
attribute set {A(X 0\), c , A(X n)} and a set of literal attribute values. The only 
derivations allowed with an attribute grammar are those in which every predi\-cate associated with every nonterminal is true. A false predicate function value indicates a violation of the syntax or static semantics rules of the language.
A parse tree of an attribute grammar is the parse tree based on its underly\-
ing BNF grammar, with a possibly empty set of attribute values attached to each node. If all the attribute values in a parse tree have been computed, the tree is said to be fully attributed . Although in practice it is not always done this way, it 
is convenient to think of attribute values as being computed after the complete unattributed parse tree has been constructed by the compiler.
3\.4\.4 Intrinsic Attributes
Intrinsic attributes are synthesized attributes of leaf nodes whose values are deter\-
mined outside the parse tree. For example, the type of an instance of a variable in a program could come from the symbol table, which is used to store variable names 3\.4 Attribute Grammars 135
and their types. The contents of the symbol table are set based on earlier declara\-
tion statements. Initially, assuming that an unattributed parse tree has been con\-structed and that attribute values are needed, the only attributes with values are the intrinsic attributes of the leaf nodes. Given the intrinsic attribute values on a parse tree, the semantic functions can be used to compute the remaining attribute values.
3\.4\.5 Examples of Attribute Grammars
As a very simple example of how attribute grammars can be used to describe static semantics, consider the following fragment of an attribute grammar that describes the rule that the name on the 
end of an Ada procedure must 
match the procedure’s name. (This rule cannot be stated in BNF .) The string attribute of , denoted by .string, is the actual string of characters that were found immediately following the reserved word 
procedure by the compiler. Notice that when there is more than one 
occurrence of a nonterminal in a syntax rule in an attribute grammar, the nonterminals are subscripted with brackets to distinguish them. Neither the subscripts nor the brackets are part of the described language.
Syntax rule:  → 
procedure  \[1] 
  end  \[2];
Predicate:  \[1]string \=\=  \[2].string
In this example, the predicate rule states that the name string attribute of the 
 nonterminal in the subprogram header must match the name string attribute of the  nonterminal following the end of the subprogram.
Next, we consider a larger example of an attribute grammar. In this case, the 
example illustrates how an attribute grammar can be used to check the type rules of a simple assignment statement. The syntax and static semantics of this assign\-ment statement are as follows: The only variable names are 
A, B, and C. The 
right side of the assignments can be either a variable or an expression in the form of a variable added to another variable. The variables can be one of two types: int or real. When there are two variables on the right side of an assignment, they need not be the same type. The type of the expression when the operand types are not the same is always real. When they are the same, the expression type is that of the operands. The type of the left side of the assignment must match the type of the right side. So the types of operands in the right side can be mixed, but the assignment is valid only if the target and the value resulting from evaluating the right side have the same type. The attribute grammar specifies these static semantic rules.
The syntax portion of our example attribute grammar is
 → 
 \= 
 →  \+ 
 \| 
 → A \| B \| C136 Chapter 3 Describing Syntax and Semantics 
The attributes for the nonterminals in the example attribute grammar are 
described in the following paragraphs:
• actual\_type —A synthesized attribute associated with the nonterminals  
and . It is used to store the actual type, int or real, of a variable or expression. In the case of a variable, the actual type is intrinsic. In the case of an expression, it is determined from the actual types of the child node or children nodes of the  nonterminal.
• expected\_type— An inherited attribute associated with the nonterminal 
. It is used to store the type, either int or real, that is expected for the expression, as determined by the type of the variable on the left side of the assignment statement.
The complete attribute grammar follows in Example 3\.6\.
EXAMPLE 3\.6 An Attribute Grammar for Simple Assignment Statements
 1\. Syntax rule:  →  \= 
 Semantic rule: .expected\_type ← .actual\_type
 2\. Syntax rule:  → \[2] \+ \[3]
 Semantic rule: .actual\_type ← 
 if (\[2].actual\_type \= int) and 
 (\[3].actual\_type \= int) 
 then int else real end if 
 Predicate: .actual\_type 
\=\= .expected\_type
 3\. Syntax rule:  → 
 Semantic rule: .actual\_type ← .actual\_type
 Predicate: .actual\_type \=\= .expected\_type
 4\. Syntax rule:  → A \| B \| C
 Semantic rule: .actual\_type ← look\-up(.string )
The look\-up function looks up a given variable name in the symbol table and 
returns the variable’s type.
A parse tree of the sentence A \= A \+ B generated by the grammar in 
Example 3\.6 is shown in Figure 3\.6\. As in the grammar, bracketed numbers are added after the repeated node labels in the tree so they can be referenced unambiguously.3\.4 Attribute Grammars 137
3\.4\.6 Computing Attribute Values
Now, consider the process of computing the attribute values of a parse tree, 
which is sometimes called decorating the parse tree. If all attributes were 
inherited, this could proceed in a completely top\-down order, from the root to the leaves. Alternatively, it could proceed in a completely bottom\-up order, from the leaves to the root, if all the attributes were synthesized. Because our grammar has both synthesized and inherited attributes, the evaluation process cannot be in any single direction. The following is an evaluation of the attributes, in an order in which it is possible to compute them:
 1\. .actual\_type ← look\-up(
A) (Rule 4\)
 2\. .expected\_type ← .actual\_type (Rule 1\)
 3\. \[2].actual\_type ← look\-up(A) (Rule 4\)
\[3].actual\_type ← look\-up(B) (Rule 4\)
 4\. .actual\_type ← either int or real (Rule 2\)
 5\. .expected\_type \=\= .actual\_type is either
 TRUE or FALSE (Rule 2\)
The tree in Figure 3\.7 shows the flow of attribute values in the example of 
Figure 3\.6\. Solid lines are used for the parse tree; dashed lines show attribute flow in the tree.
The tree in Figure 3\.8 shows the final attribute values on the nodes. In this 
example, 
A is defined as a real and B is defined as an int.
Determining attribute evaluation order for the general case of an attribute 
grammar is a complex problem, requiring the construction of a dependency graph to show all attribute dependencies.
\[3]
B\[2]
A \=\+
AFigure 3\.6
A parse tree for 
A \= A \+ B138 Chapter 3 Describing Syntax and Semantics 
expected\_type
\[3]
B\[2]
A \=\+
A
actual\_type
actual\_type actual\_typeactual\_type

\[3]
Bactual\_type \=
int\_typeactual\_type \=
real\_type\[2]
A\=\+
Aactual\_type \=
real\_type expected\_type \= real\_type
actual\_type \= real\_typeFigure 3\.7
The flow of attributes 
in the tree
Figure 3\.8
A fully attributed parse tree
3\.4\.7 Evaluation
Checking the static semantic rules of a language is an essential part of all com\-
pilers. Even if a compiler writer has never heard of an attribute grammar, he or she would need to use their fundamental ideas to design the checks of static semantics rules for his or her compiler.
One of the main difficulties in using an attribute grammar to describe all of 
the syntax and static semantics of a real contemporary programming language is the size and complexity of the attribute grammar. The large number of attri\-butes and semantic rules required for a complete programming language make such grammars difficult to write and read. Furthermore, the attribute values on a large parse tree are costly to evaluate. On the other hand, less formal attribute 3\.5 Describing the Meanings of Programs: Dynamic Semantics 139
grammars are a powerful and commonly used tool for compiler writers, who 
are more interested in the process of producing a compiler than they are in formalism.
3\.5 Describing the Meanings of Programs: Dynamic Semantics
We now turn to the difficult task of describing the dynamic semantics , or 
meaning, of the expressions, statements, and program units of a programming language. Because of the power and naturalness of the available notation, describing syntax is a relatively simple matter. On the other hand, no univer\-sally accepted notation or approach has been devised for dynamic semantics. In this section, we briefly describe several of the methods that have been devel\-oped. For the remainder of this section, when we use the term semantics, we 
mean dynamic semantics.
There are several different reasons underlying the need for a methodology 
and notation for describing semantics. Programmers obviously need to know precisely what the statements of a language do before they can use them effec\-tively in their programs. Compiler writers must know exactly what language constructs mean to design implementations for them correctly. If there were a precise semantics specification of a programming language, programs written in the language potentially could be proven correct without testing. Also, com\-pilers could be shown to produce programs that exhibited exactly the behavior given in the language definition; that is, their correctness could be verified. A complete specification of the syntax and semantics of a programming language could be used by a tool to generate a compiler for the language automatically. Finally, language designers, who would develop the semantic descriptions of their languages, could in the process discover ambiguities and inconsistencies in their designs.
Software developers and compiler designers typically determine the 
semantics of programming languages by reading English explanations in lan\-guage manuals. Because such explanations are often imprecise and incomplete, this approach is clearly unsatisfactory. Due to the lack of complete semantics specifications of programming languages, programs are rarely proven correct without testing, and commercial compilers are never generated automatically from language descriptions.
Scheme, a functional language described in Chapter 15, is one of only 
a few programming languages whose definition includes a formal semantics description. However, the method used is not one described in this chapter, as this chapter is focused on approaches that are suitable for imperative languages.
3\.5\.1 Operational Semantics
The idea behind operational semantics is to describe the meaning of a 
statement or program by specifying the effects of running it on a machine. The effects on the machine are viewed as the sequence of changes in its 140 Chapter 3 Describing Syntax and Semantics 
state, where the machine’s state is the collection of the values in its storage. 
An obvious operational semantics description, then, is given by executing a compiled version of the program on a computer. Most programmers have, on at least one occasion, written a small test program to determine the meaning of some programming language construct, often while learning the language. Essentially, what such a programmer is doing is using operational semantics to determine the meaning of the construct.
There are several problems with using this approach for complete formal 
semantics descriptions. First, the individual steps in the execution of machine language and the resulting changes to the state of the machine are too small and too numerous. Second, the storage of a real computer is too large and complex. There are usually several levels of memory devices, as well as connections to enumerable other computers and memory devices through networks. There\-fore, machine languages and real computers are not used for formal operational semantics. Rather, intermediate\-level languages and interpreters for idealized computers are designed specifically for the process.
There are different levels of uses of operational semantics. At the highest 
level, the interest is in the final result of the execution of a complete program. This is sometimes called natural operational semantics . At the lowest level, 
operational semantics can be used to determine the precise meaning of a pro\-gram through an examination of the complete sequence of state changes that occur when the program is executed. This use is sometimes called structural 
operational semantics .
3\.5\.1\.1 The Basic Process
The first step in creating an operational semantics description of a language 
is to design an appropriate intermediate language, where the primary char\-acteristic of the language is clarity. Every construct of the intermediate lan\-guage must have an obvious and unambiguous meaning. This language is at the intermediate level, because machine language is too low\-level to be easily understood and another high\-level language is obviously not suitable. If the semantics description is to be used for natural operational semantics, a virtual machine (an interpreter) must be constructed for the intermediate language. The virtual machine can be used to execute either single statements, code seg\-ments, or whole programs. The semantics description can be used without a virtual machine if the meaning of a single statement is all that is required. In this use, which is structural operational semantics, the intermediate code can be visually inspected.
The basic process of operational semantics is not unusual. In fact, the con\-
cept is frequently used in programming textbooks and programming language reference manuals. For example, the semantics of the C 
for construct can be 
described in terms of simpler statements, as in3\.5 Describing the Meanings of Programs: Dynamic Semantics 141
The human reader of such a description is the virtual computer and is assumed 
to be able to “execute” the instructions in the definition correctly and recognize the effects of the “execution.”
The intermediate language and its associated virtual machine used for 
formal operational semantics descriptions are often highly abstract. The inter\-mediate language is meant to be convenient for the virtual machine, rather than for human readers. For our purposes, however, a more human\-oriented intermediate language could be used. As such an example, consider the follow\-ing list of statements, which would be adequate for describing the semantics of the simple control statements of a typical programming language:
 ident 
\= var
 ident \= ident \+ 1
 ident \= ident – 1
 goto label
 if var relop var goto label 
In these statements, relop is one of the relational operators from the set 
{\=, \<\>, \>, \<, \>\=, \<\=} , ident is an identifier, and var is either an identifier 
or a constant. These statements are all simple and therefore easy to understand and implement.
A slight generalization of these three assignment statements allows more 
general arithmetic expressions and assignment statements to be described. The new statements are
ident 
\= var bin\_op var
ident \= un\_op var
where bin\_op is a binary arithmetic operator and un\_op is a unary operator. 
Multiple arithmetic data types and automatic type conversions, of course, com\-plicate this generalization. Adding just a few more relatively simple instructions would allow the semantics of arrays, records, pointers, and subprograms to be described.
In Chapter 8, the semantics of various control statements are described 
using this intermediate language.C Statement Meaning
for (expr1; expr2; expr3\) {
 . . .
} expr1 ;
loop: if expr2 \=\= 0 goto out
 . . . expr3
;
 goto loop
out: . . .142 Chapter 3 Describing Syntax and Semantics 
3\.5\.1\.2 Evaluation
The first and most significant use of formal operational semantics was to 
describe the semantics of PL/I (Wegner, 1972\). That particular abstract machine and the translation rules for PL/I were together named the Vienna Definition Language (VDL), after the city where IBM designed it.
Operational semantics provides an effective means of describing semantics 
for language users and language implementors, as long as the descriptions are kept simple and informal. The VDL description of PL/I, unfortunately, is so complex that it serves no practical purpose.
Operational semantics depends on programming languages of lower 
levels, not mathematics. The statements of one programming language are described in terms of the statements of a lower\-level programming language. This approach can lead to circularities, in which concepts are indirectly defined in terms of themselves. The methods described in the following two sections are much more formal, in the sense that they are based on mathematics and logic, not programming languages.
3\.5\.2 Denotational Semantics
Denotational semantics is the most rigorous and most widely known formal 
method for describing the meaning of programs. It is solidly based on recursive function theory. A thorough discussion of the use of denotational semantics to describe the semantics of programming languages is necessarily long and com\-plex. It is our intent to provide the reader with an introduction to the central concepts of denotational semantics, along with a few simple examples that are relevant to programming language specifications.
The process of constructing a denotational semantics specification for a 
programming language requires one to define for each language entity both a mathematical object and a function that maps instances of that language entity onto instances of the mathematical object. Because the objects are rigorously defined, they model the exact meaning of their corresponding entities. The idea is based on the fact that there are rigorous ways of manipulating mathemati\-cal objects but not programming language constructs. The difficulty with this method lies in creating the objects and the mapping functions. The method is named denotational because the mathematical objects denote the meaning of 
their corresponding syntactic entities.
The mapping functions of a denotational semantics programming language 
specification, like all functions in mathematics, have a domain and a range. The domain is the collection of values that are legitimate parameters to the function; the range is the collection of objects to which the parameters are mapped. In denotational semantics, the domain is called the syntactic domain , because it is 
syntactic structures that are mapped. The range is called the semantic domain .
Denotational semantics is related to operational semantics. In operational 
semantics, programming language constructs are translated into simpler pro\-gramming language constructs, which become the basis of the meaning of the 3\.5 Describing the Meanings of Programs: Dynamic Semantics 143
construct. In denotational semantics, programming language constructs are 
mapped to mathematical objects, either sets or, more often, functions. How\-ever, unlike operational semantics, denotational semantics does not model the step\-by\-step computational processing of programs.
3\.5\.2\.1 Two Simple Examples
We use a very simple language construct, character string representations of binary numbers, to introduce the denotational method. The syntax of such binary numbers can be described by the following grammar rules:
 → 
'0'
 \| '1'
 \|  '0'
 \|  '1'
A parse tree for the example binary number, 110, is shown in Figure 3\.9\. Notice 
that we put apostrophes around the syntactic digits to show they are not math\-ematical digits. This is similar to the relationship between ASCII coded digits and mathematical digits. When a program reads a number as a string, it must be con\-verted to a mathematical number before it can be used as a value in the program.

 '0'

'1''1'Figure 3\.9
A parse tree of the 
binary number 110
The syntactic domain of the mapping function for binary numbers is the 
set of all character string representations of binary numbers. The semantic domain is the set of nonnegative decimal numbers, symbolized by N.
T o describe the meaning of binary numbers using denotational semantics, 
we associate the actual meaning (a decimal number) with each rule that has a single terminal symbol as its RHS.
In our example, decimal numbers must be associated with the first two 
grammar rules. The other two grammar rules are, in a sense, computational rules, because they combine a terminal symbol, to which an object can be associated, with a nonterminal, which can be expected to represent some construct. Presuming an evaluation that progresses upward in the parse tree, 144 Chapter 3 Describing Syntax and Semantics 
the nonterminal in the right side would already have its meaning attached. 
So, a syntax rule with a nonterminal as its RHS would require a function that computed the meaning of the LHS, which represents the meaning of the complete RHS.
The semantic function, named M
bin, maps the syntactic objects, as 
described in the previous grammar rules, to the objects in N, the set of non\-negative decimal numbers. The function 
Mbin is defined as follows:
Mbin('0') \= 0
Mbin('1') \= 1
Mbin( '0') \= 2 \* Mbin( )
Mbin( '1') \= 2 \* Mbin( ) \+ 1
The meanings, or denoted objects (which in this case are decimal numbers), 
can be attached to the nodes of the parse tree shown on the previous page, yielding the tree in Figure 3\.10\. This is syntax\-directed semantics. Syntactic entities are mapped to mathematical objects with concrete meaning.

 '0'

'1''1'136 Figure 3\.10
A parse tree with 
denoted objects for 110
In part because we need it later, we now show a similar example for describ\-
ing the meaning of syntactic decimal literals. In this case, the syntactic domain is the set of character string representations of decimal numbers. The semantic domain is once again the set N.
 → 
'0'\|'1'\|'2'\|'3'\|'4'\|'5'\|'6'\|'7''8'\|'9'
 \| ('0'\|'1'\|'2'\|'3'\|'4'\|'5'\|'6'\|'7'\|'8'\|'9' )
The denotational mappings for these syntax rules areM
dec('0') \= 0, Mdec('1') \= 1, Mdec('2') \= 2, . . ., Mdec('9') \= 9
Mdec( '0') \= 10 \* Mdec( )
Mdec( '1') \= 10 \* Mdec( ) \+ 1
. . .
Mdec( '9') \= 10 \* Mdec( ) \+ 93\.5 Describing the Meanings of Programs: Dynamic Semantics 145
In the following sections, we present the denotational semantics descrip\-
tions of a few simple constructs. The most important simplifying assumption made here is that both the syntax and static semantics of the constructs are correct. In addition, we assume that only two scalar types are included: integer and Boolean.
3\.5\.2\.2 The State of a Program
The denotational semantics of a program could be defined in terms of state changes in an ideal computer. Operational semantics are defined in this way, and denotational semantics are defined in nearly the same way. In a further simplification, however, denotational semantics is defined in terms of only the values of all of the program’s variables. So, denotational semantics uses the state of the program to describe meaning, whereas operational semantics uses the state of a machine. The key difference between operational semantics and denotational semantics is that state changes in operational semantics are defined by coded algorithms, written in some programming language, whereas in denotational semantics, state changes are defined by mathematical functions.
Let the state s of a program be represented as a set of ordered pairs, as 
follows:
s 
\= {, , . . . , }
Each i is the name of a variable, and the associated v’s are the current values 
of those variables. Any of the v’s can have the special value undef , which indi\-
cates that its associated variable is currently undefined. Let VARMAP be a function of two parameters: a variable name and the program state. The value of VARMAP (i
j, s) is v j (the value paired with i j in state s). Most semantics 
mapping functions for programs and program constructs map states to states. These state changes are used to define the meanings of programs and program constructs. Some language constructs—for example, expressions—are mapped to values, not states.
3\.5\.2\.3 Expressions
Expressions are fundamental to most programming languages. We assume here that expressions have no side effects. Furthermore, we deal with only very simple expressions: The only operators are 
\+ and \*, and an expression can have 
at most one operator; the only operands are scalar integer variables and integer literals; there are no parentheses; and the value of an expression is an integer. Following is the BNF description of these expressions:
 →  
\|  \| 
 →   
 →  \| 
 →  \| 
 → \+ \| \*146 Chapter 3 Describing Syntax and Semantics 
The only error we consider in expressions is a variable having an unde\-
fined value. Obviously, other errors can occur, but most of them are machine\-dependent. Let Z be the set of integers, and let error be the error value. Then 
Z h {error } is the semantic domain for the denotational specification for our 
expressions.
The mapping function for a given expression E and state s follows. T o 
distinguish between mathematical function definitions and the assignment statements of programming languages, we use the symbol /H9004
\= to define 
mathematical functions. The implication symbol, \=\>, used in this definition 
connects the form of an operand with its associated case (or switch) con\-struct. Dot notation is used to refer to the child nodes of a node. For exam\-ple, . refers to the left child node of .
M
e(, s) Δ\= case  of
  \=\>Mdec(, s )
  \=\>if VARMAP (, s) \=\= undef
 then error
 else VARMAP (, s)
  \=\> 
 if(Me(.,s ) \=\= undef OR
 Me(., s ) \=\= undef)
 then error
 else if (. \=\= '\+') 
 then Me(., s ) \+
 Me(., s )
 else Me(., s ) \* 
 Me(., s )
3\.5\.2\.4 Assignment Statements
An assignment statement is an expression evaluation plus the setting of the 
target variable to the expression’s value. In this case, the meaning function maps a state to a state. This function can be described with the following:
M
a(x \= E, s) Δ\= if Me(E, s) \=\= error
 then error
 else s /H11032 \= {, , . . . , }, where 
 for j \= 1, 2, . . . , n
 if ij \=\= x 
 then vj/H11032 \= Me(E, s)
 else vj/H11032 \= VARMAP (ij, s)
Note that the comparison in the third last line above, i j \=\= x, is of names, not 
values.3\.5 Describing the Meanings of Programs: Dynamic Semantics 147
3\.5\.2\.5 Logical Pretest Loops
The denotational semantics of a logical pretest loop is deceptively simple. 
T o expedite the discussion, we assume that there are two other existing mapping functions, M
sl and M b, that map statement lists and states to states 
and Boolean expressions to Boolean values (or error ), respectively. The 
function is
Ml(while B do L, s) Δ\= if Mb(B, s) \=\= undef
 then error
 else if Mb(B, s) \=\= false
 then s else if M
sl(L, s) \=\= error
 then error
 else Ml(while B do L, Msl(L, s))
The meaning of the loop is simply the value of the program variables after the 
statements in the loop have been executed the prescribed number of times, assuming there have been no errors. In essence, the loop has been converted from iteration to recursion, where the recursion control is mathematically defined by other recursive state mapping functions. Recursion is easier to describe with mathematical rigor than iteration.
One significant observation at this point is that this definition, like actual 
program loops, may compute nothing because of nontermination.
3\.5\.2\.6 Evaluation
Objects and functions, such as those used in the earlier constructs, can be defined for the other syntactic entities of programming languages. When a complete system has been defined for a given language, it can be used to determine the meaning of complete programs in that language. This provides a framework for thinking about programming in a highly rigor\-ous way.
As stated previously, denotational semantics can be used as an aid to lan\-
guage design. For example, statements for which the denotational semantic description is complex and difficult may indicate to the designer that such statements may also be difficult for language users to understand and that an alternative design may be in order.
Because of the complexity of denotational descriptions, they are of little 
use to language users. On the other hand, they provide an excellent way to describe a language concisely.
Although the use of denotational semantics is normally attributed to Scott 
and Strachey (1971\), the general denotational approach to language description can be traced to the nineteenth century (Frege, 1892\).148 Chapter 3 Describing Syntax and Semantics 
3\.5\.3 Axiomatic Semantics
Axiomatic semantics , thus named because it is based on mathematical logic, is 
the most abstract approach to semantics specification discussed in this chapter. Rather than directly specifying the meaning of a program, axiomatic semantics specifies what can be proven about the program. Recall that one of the possible uses of semantic specifications is to prove the correctness of programs.
In axiomatic semantics, there is no model of the state of a machine or pro\-
gram or model of state changes that take place when the program is executed. The meaning of a program is based on relationships among program variables and constants, which are the same for every execution of the program.
Axiomatic semantics has two distinct applications: program verification and 
program semantics specification. This section focuses on program verification in its description of axiomatic semantics.
Axiomatic semantics was defined in conjunction with the development of 
an approach to proving the correctness of programs. Such correctness proofs, when they can be constructed, show that a program performs the computation described by its specification. In a proof, each statement of a program is both preceded and followed by a logical expression that specifies constraints on pro\-gram variables. These, rather than the entire state of an abstract machine (as with operational semantics), are used to specify the meaning of the statement. The notation used to describe constraints—indeed, the language of axiomatic semantics—is predicate calculus. Although simple Boolean expressions are often adequate to express constraints, in some cases they are not.
When axiomatic semantics is used to specify formally the meaning of a 
statement, the meaning is defined by the statement’s effect on assertions about the data affected by the statement.
3\.5\.3\.1 Assertions
The logical expressions used in axiomatic semantics are called predicates, or assertions . An assertion immediately preceding a program statement describes 
the constraints on the program variables at that point in the program. An asser\-tion immediately following a statement describes the new constraints on those variables (and possibly others) after execution of the statement. These asser\-tions are called the precondition and postcondition , respectively, of the state\-
ment. For two adjacent statements, the postcondition of the first serves as the precondition of the second. Developing an axiomatic description or proof of a given program requires that every statement in the program has both a pre\-condition and a postcondition.
In the following sections, we examine assertions from the point of view 
that preconditions for statements are computed from given postconditions, although it is possible to consider these in the opposite sense. We assume all variables are integer type. As a simple example, consider the following assign\-ment statement and postcondition:
sum \= 2 \* x \+ 1 {sum \> 1}3\.5 Describing the Meanings of Programs: Dynamic Semantics 149
Precondition and postcondition assertions are presented in braces to distin\-
guish them from parts of program statements. One possible precondition for this statement is {
x \> 10 }.
In axiomatic semantics, the meaning of a specific statement is defined by 
its precondition and its postcondition. In effect, the two assertions specify pre\-cisely the effect of executing the statement.
In the following subsections, we focus on correctness proofs of statements 
and programs, which is a common use of axiomatic semantics. The more gen\-eral concept of axiomatic semantics is to state precisely the meaning of state\-ments and programs in terms of logic expressions. Program verification is one application of axiomatic descriptions of languages.
3\.5\.3\.2 Weakest Preconditions
The weakest precondition is the least restrictive precondition that will guar\-
antee the validity of the associated postcondition. For example, in the state\-ment and postcondition given in Section 3\.5\.3\.1, {
x \> 10 }, {x \> 50 }, and 
{x \> 1000 } are all valid preconditions. The weakest of all preconditions in 
this case is { x \> 0 }.
If the weakest precondition can be computed from the most general 
postcondition for each of the statement types of a language, then the pro\-cesses used to compute these preconditions provide a concise description of the semantics of that language. Furthermore, correctness proofs can be con\-structed for programs in that language. A program proof is begun by using the characteristics of the results of the program’s execution as the postcondition of the last statement of the program. This postcondition, along with the last statement, is used to compute the weakest precondition for the last statement. This precondition is then used as the postcondition for the second last state\-ment. This process continues until the beginning of the program is reached. At that point, the precondition of the first statement states the conditions under which the program will compute the desired results. If these conditions are implied by the input specification of the program, the program has been verified to be correct.
An inference rule is a method of inferring the truth of one assertion on 
the basis of the values of other assertions. The general form of an inference rule is as follows:
S1, S2, c, Sn
S
This rule states that if S1, S2, . . . , and S n are true, then the truth of S can be 
inferred. The top part of an inference rule is called its antecedent ; the bottom 
part is called its consequent .
An axiom is a logical statement that is assumed to be true. Therefore, an 
axiom is an inference rule without an antecedent.
For some program statements, the computation of a weakest precondition 
from the statement and a postcondition is simple and can be specified by an 150 Chapter 3 Describing Syntax and Semantics 
axiom. In most cases, however, the weakest precondition can be specified only 
by an inference rule.
T o use axiomatic semantics with a given programming language, whether 
for correctness proofs or for formal semantics specifications, either an axiom or an inference rule must exist for each kind of statement in the language. In the following subsections, we present an axiom for assignment statements and inference rules for statement sequences, selection statements, and logical pre\-test loop statements. Note that we assume that neither arithmetic nor Boolean expressions have side effects.
3\.5\.3\.3 Assignment Statements
The precondition and postcondition of an assignment statement together define precisely its meaning. T o define the meaning of an assignment state\-ment, given a postcondition, there must be a way to compute its precondition from that postcondition.
Let x 
\= E be a general assignment statement and Q be its postcondition. 
Then, its precondition, P , is defined by the axiom
P \= QxSE
which means that P is computed as Q with all instances of x replaced by E. For 
example, if we have the assignment statement and postcondition
a \= b / 2 \- 1 {a \< 10}
the weakest precondition is computed by substituting b / 2 \- 1 for a in the 
postcondition { a \< 10 }, as follows:
b / 2 \- 1 \< 10
b \< 22
Thus, the weakest precondition for the given assignment statement and post\-
condition is { b \< 22 }. Remember that the assignment axiom is guaranteed to 
be correct only in the absence of side effects. An assignment statement has a side effect if it changes some variable other than its target.
The usual notation for specifying the axiomatic semantics of a given state\-
ment form is
{P}S{Q}
where P is the precondition, Q is the postcondition, and S is the statement form. In the case of the assignment statement, the notation is
{QxSE} x \= E{Q}3\.5 Describing the Meanings of Programs: Dynamic Semantics 151
As another example of computing a precondition for an assignment state\-
ment, consider the following:
x \= 2 \* y \- 3 {x \> 25}
The precondition is computed as follows:
2 \* y \- 3 \> 25
y \> 14
So {y \> 14 } is the weakest precondition for this assignment statement and 
postcondition.
Note that the appearance of the left side of the assignment statement in its 
right side does not affect the process of computing the weakest precondition. For example, for
x \= x \+ y \- 3 {x \> 10}
the weakest precondition is
x \+ y \- 3 \> 10
y \> 13 \- x
Recall that axiomatic semantics was developed to prove the correctness of 
programs. In light of that, it is natural at this point to wonder how the axiom for assignment statements can be used to prove anything. Here is how: A given assignment statement with both a precondition and a postcondition can be con\-sidered a logical statement, or theorem. If the assignment axiom, when applied to the postcondition and the assignment statement, produces the given pre\-condition, the theorem is proved. For example, consider the logical statement
{x \> 3} x \= x \- 3 {x \> 0}
Using the assignment axiom on
x \= x \- 3 {x \> 0}
produces { x \> 3 }, which is the given precondition. Therefore, we have proven 
the example logical statement.
Next, consider the logical statement
{x \> 5} x \= x \- 3 {x \> 0}
In this case, the given precondition, { x \> 5 }, is not the same as the assertion 
produced by the axiom. However, it is obvious that { x \> 5 } implies {x \> 3 }. 152 Chapter 3 Describing Syntax and Semantics 
T o use this in a proof, an inference rule, named the rule of consequence , is 
needed. The form of the rule of consequence is
{P} S {Q}, P /H11032\=\> P, Q \=\> Q/H11032
{P/H11032} S {Q /H11032}
The \=\> symbol means “implies,” and S can be any program statement. The rule 
can be stated as follows: If the logical statement {P} S {Q} is true, the assertion 
P/H11032 implies the assertion P , and the assertion Q implies the assertion Q /H11032, then it 
can be inferred that {P /H11032} S {Q /H11032}. In other words, the rule of consequence says 
that a postcondition can always be weakened and a precondition can always be strengthened. This is quite useful in program proofs. For example, it allows the completion of the proof of the last logical statement example above. If we let P be {
x \> 3 }, Q and Q /H11032 be {x \> 0 }, and P /H11032 be {x \> 5 }, we have
{x\>3}x \= x–3{x \>0},(x\>5\) \=\> {x\>3},(x\>0\) \=\> (x\>0\)
{x\>5}x \=x–3{x \>0}
The first term of the antecedent ( {x \> 3} x \= x – 3 {x \> 0} ) was proven 
with the assignment axiom. The second and third terms are obvious. There\-fore, by the rule of consequence, the consequent is true.
3\.5\.3\.4 Sequences
The weakest precondition for a sequence of statements cannot be described by an axiom, because the precondition depends on the particular kinds of state\-ments in the sequence. In this case, the precondition can only be described with an inference rule. Let S1 and S2 be adjacent program statements. If S1 and S2 have the following pre\- and postconditions
{P1} S1 {P2}
{P2} S2 {P3}
the inference rule for such a two\-statement sequence is
{P1} S1 {P2}, {P2} S2 {P3}
{P1} S1, S2 {P3}
So, for our example, {P1} S1; S2 {P3} describes the axiomatic semantics of 
the sequence S1; S2\. The inference rule states that to get the sequence pre\-condition, the precondition of the second statement is computed. This new assertion is then used as the postcondition of the first statement, which can then be used to compute the precondition of the first statement, which is also the precondition of the whole sequence. If S1 and S2 are the assignment statements3\.5 Describing the Meanings of Programs: Dynamic Semantics 153
x1\=E1
and
x2\=E2
then we have
{P3 x2SE2} x2\=E2 {P3}
{(P3 x2SE2\)x1SE1} x1\=E1 {P3 x2SE2}
Therefore, the weakest precondition for the sequence x1 \= E1; x2 \= E2 with 
postcondition P3 is {(P3 x2SE2\)x1SE1}.
For example, consider the following sequence and postcondition:
y \= 3 \* x \+ 1;
x \= y \+ 3;
{x \< 10}
The precondition for the second assignment statement is
y \< 7
which is used as the postcondition for the first statement. The precondition for 
the first assignment statement can now be computed:
3 \* x \+ 1 \< 7
x \< 2
So, {x \< 2} is the precondition of both the first statement and the two\-
statement sequence.
3\.5\.3\.5 Selection
We next consider the inference rule for selection statements, the general form of which is
if B then S1 else S2
We consider only selections that include else clauses. The inference rule is
{B and P} S1 {Q}, {(not B) and P} S2{Q}
{P} if B then S1 else S2 {Q}
This rule indicates that selection statements must be proven both when the 
Boolean control expression is true and when it is false. The first logical state\-ment above the line represents the 
then clause; the second represents the else 154 Chapter 3 Describing Syntax and Semantics 
clause. According to the inference rule, we need a precondition P that can be 
used in the precondition of both the then and else clauses.
Consider the following example of the computation of the precondition 
using the selection inference rule. The example selection statement is
if x \> 0 then
 y \= y \- 1
else 
 y \= y \+ 1
Suppose the postcondition, Q, for this selection statement is { y \> 0 }. We 
can use the axiom for assignment on the then clause
y \= y \- 1 {y \> 0}
This produces { y \- 1 \> 0 } or {y \> 1 }. It can be used as the P part of the 
precondition for the then clause. Now we apply the same axiom 
to the else clause
y \= y \+ 1 {y \> 0}
which produces the precondition { y \+ 1 \> 0 } or {y \> \-1 }. 
Because {y \> 1 } \=\> {y \> \-1 }, the rule of consequence allows us to 
use {y \> 1 } for the precondition of the whole selection statement.
3\.5\.3\.6 Logical Pretest Loops
Another essential construct of imperative programming languages is the logical pretest, or 
while loop. Computing the weakest pre\-
condition for a while loop is inherently more difficult than for 
a sequence, because the number of iterations cannot always be predetermined. In a case where the number of iterations is known, 
the loop can be unrolled and treated as a sequence.
The problem of computing the weakest precondition for loops is similar 
to the problem of proving a theorem about all positive integers. In the latter case, induction is normally used, and the same inductive method can be used for some loops. The principal step in induction is finding an inductive hypothesis. The corresponding step in the axiomatic semantics of a 
while loop is finding 
an assertion called a loop invariant , which is crucial to finding the weakest 
precondition.
The inference rule for computing the precondition for a while loop is
{I and B} S {I}
{I} while B do S end {I and (not B)}
where I is the loop invariant. This seems simple, but it is not. The complexity 
lies in finding an appropriate loop invariant.history note
A significant amount of work 
has been done on the possibility 
of using denotational language 
descriptions to generate 
 compilers automatically (Jones, 
1980; Milos et al., 1984; 
 Bodwin et al., 1982\). These 
efforts have shown that the 
method is feasible, but the work 
has never progressed to the 
point where it can be used to generate useful compilers.3\.5 Describing the Meanings of Programs: Dynamic Semantics 155
The axiomatic description of a while loop is written as
{P} while B do S end {Q}
The loop invariant must satisfy a number of requirements to be useful. 
First, the weakest precondition for the while loop must guarantee the truth 
of the loop invariant. In turn, the loop invariant must guarantee the truth of the postcondition upon loop termination. These constraints move us from the inference rule to the axiomatic description. During execution of the loop, the truth of the loop invariant must be unaffected by the evaluation of the loop\-controlling Boolean expression and the loop body statements. Hence, the name invariant.
Another complicating factor for 
while loops is the question of loop termi\-
nation. A loop that does not terminate cannot be correct, and in fact computes nothing. If Q is the postcondition that holds immediately after loop exit, then a precondition P for the loop is one that guarantees Q at loop exit and also guarantees that the loop terminates.
The complete axiomatic description of a 
while construct requires all of 
the following to be true, in which I is the loop invariant:
P \=\> I
{I and B} S {I}(I and (not B)) 
\=\> Q
the loop terminates
If a loop computes a sequence of numeric values, it may be possible to find 
a loop invariant using an approach that is used for determining the inductive hypothesis when mathematical induction is used to prove a statement about a mathematical sequence. The relationship between the number of iterations and the precondition for the loop body is computed for a few cases, with the hope that a pattern emerges that will apply to the general case. It is helpful to treat the process of producing a weakest precondition as a function, wp. In general
wp(statement, postcondition) 
\= precondition
A wp function is often called a predicate transformer , because it takes a predi\-
cate, or assertion, as a parameter and returns another predicate.
T o find I, the loop postcondition Q is used to compute preconditions for 
several different numbers of iterations of the loop body, starting with none. If the loop body contains a single assignment statement, the axiom for assign\-ment statements can be used to compute these cases. Consider the example loop:
while y \<\> x do y \= y \+ 1 end {y \= x}
Remember that the equal sign is being used for two different purposes here. 
In assertions, it means mathematical equality; outside assertions, it means the assignment operator.156 Chapter 3 Describing Syntax and Semantics 
For zero iterations, the weakest precondition is, obviously,
{y \= x}
For one iteration, it is
wp(y \= y \+ 1, {y \= x}) \= {y \+ 1 \= x}, or {y \= x \- 1}
For two iterations, it is
wp(y \= y \+ 1, {y \= x \- 1})\={y \+ 1 \= x \- 1}, or {y \= x \- 2}
For three iterations, it iswp
(y \= y \+ 1, {y \= x \- 2})\={y \+ 1 \= x \- 2}, or {y \= x – 3}
It is now obvious that { y \< x } will suffice for cases of one or more iterations. 
Combining this with { y \= x } for the zero iterations case, we get { y \<\= x }, 
which can be used for the loop invariant. A precondition for the while state\-
ment can be determined from the loop invariant. In fact, I can be used as the precondition, P .
We must ensure that our choice satisfies the four criteria for I for our 
example loop. First, because P 
\= I, P \=\> I. The second requirement is that it 
must be true that
{I and B} S {I}
In our example, we have
{y \<\= x and y \<\> x} y \= y \+ 1 {y \<\= x}
Applying the assignment axiom to
y \= y \+ 1 {y \<\= x}
we get {y \+ 1 \<\= x }, which is equivalent to { y \< x }, which is implied by 
{y \<\= x and y \<\> x }. So, the earlier statement is proven.
Next, we must have
{I and (not B)} \=\> Q
In our example, we have
{(y \<\= x) and not (y \<\> x)} \=\> {y \= x}
{(y \<\= x) and (y \= x)} \=\> {y \= x}
{y \= x} \=\> {y \= x}
So, this is obviously true. Next, loop termination must be considered. In this 
example, the question is whether the loop
{y \<\= x} while y \<\> x do y \= y \+ 1 end {y \= x}3\.5 Describing the Meanings of Programs: Dynamic Semantics 157
terminates. Recalling that x and y are assumed to be integer variables, it is easy 
to see that this loop does terminate. The precondition guarantees that y ini\-
tially is not larger than x. The loop body increments y with each iteration, until 
y is equal to x. No matter how much smaller y is than x initially, it will even\-
tually become equal to x. So the loop will terminate. Because our choice of I 
satisfies all four criteria, it is a satisfactory loop invariant and loop precondition.
The previous process used to compute the invariant for a loop does not 
always produce an assertion that is the weakest precondition (although it does in the example).
As another example of finding a loop invariant using the approach used in 
mathematical induction, consider the following loop statement:
while s \> 1 do s \= s / 2 end {s \= 1}
As before, we use the assignment axiom to try to find a loop invariant and a 
precondition for the loop. For zero iterations, the weakest precondition is {
s \= 1 }. For one iteration, it is
wp(s \= s / 2, {s \= 1}) \= {s / 2 \= 1} , or {s \= 2}
For two iterations, it iswp
(s \= s / 2, {s \= 2}) \= {s / 2 \= 2} , or {s \= 4}
For three iterations, it iswp
(s \= s / 2, {s \= 4}) \= {s / 2 \= 4} , or {s \= 8}
From these cases, we can see clearly that the invariant is
{s is a nonnegative power of 2}
Once again, the computed I can serve as P , and I passes the four requirements. 
Unlike our earlier example of finding a loop precondition, this one clearly is not a weakest precondition. Consider using the precondition {
s \> 1 }. The 
logical statement
{s \> 1} while s \> 1 do s \= s / 2 end {s \= 1}
can easily be proven, and this precondition is significantly broader than the 
one computed earlier. The loop and precondition are satisfied for any positive value for 
s, not just powers of 2, as the process indicates. Because of the rule of 
consequence, using a precondition that is stronger than the weakest precondi\-tion does not invalidate a proof.
Finding loop invariants is not always easy. It is helpful to understand the 
nature of these invariants. First, a loop invariant is a weakened version of the loop postcondition and also a precondition for the loop. So, I must be weak enough to be satisfied prior to the beginning of loop execution, but when combined with the loop exit condition, it must be strong enough to force the truth of the postcondition.158 Chapter 3 Describing Syntax and Semantics 
Because of the difficulty of proving loop termination, that requirement 
is often ignored. If loop termination can be shown, the axiomatic description of the loop is called total correctness . If the other conditions can be met but 
termination is not guaranteed, it is called partial correctness .
In more complex loops, finding a suitable loop invariant, even for partial 
correctness, requires a good deal of ingenuity. Because computing the pre\-condition for a 
while loop depends on finding a loop invariant, proving the 
correctness of programs with while loops using axiomatic semantics can be 
difficult.
3\.5\.3\.7 Program Proofs
This section provides validations for two simple programs. The first example of a correctness proof is for a very short program, consisting of a sequence of three assignment statements that interchange the values of two variables.
{x \= A AND y \= B}
t \= x;
x \= y;
y \= t;
{x \= B AND y \= A}
Because the program consists entirely of assignment statements in a 
sequence, the assignment axiom and the inference rule for sequences can be used to prove its correctness. The first step is to use the assignment axiom on the last statement and the postcondition for the whole program. This yields the precondition
{x \= B AND t \= A}
Next, we use this new precondition as a postcondition on the middle state\-
ment and compute its precondition, which is
{y \= B AND t \= A}
Next, we use this new assertion as the postcondition on the first statement 
and apply the assignment axiom, which yields
{y \= B AND x \= A}
which is the same as the precondition on the program, except for the order of operands on the 
AND operator. Because AND is a symmetric operator, our proof 
is complete.
The following example is a proof of correctness of a pseudocode program 
that computes the factorial function.3\.5 Describing the Meanings of Programs: Dynamic Semantics 159
{n \>\= 0}
count \= n;fact \= 1;while count \<\> 0 do
 fact \= fact \* count;
 count \= count \- 1;
end
{fact \= n!}
The method described earlier for finding the loop invariant does not work for 
the loop in this example. Some ingenuity is required here, which can be aided by a brief study of the code. The loop computes the factorial function in order of the last multiplication first; that is, 
(n \- 1\) \* n is done first, assuming n 
is greater than 1\. So, part of the invariant can be
fact \= (count \+ 1\) \* (count \+ 2\) \* . . . \* (n \- 1\) \* n
But we must also ensure that count is always nonnegative, which we can do 
by adding that to the assertion above, to get
I \= (fact \= (count \+ 1\) \* . . . \* n) AND (count \>\= 0\)
Next, we must confirm that this I meets the requirements for invariants. 
Once again we let I also be used for P , so P clearly implies I. The next ques\-tion is
{I and B} S {I}
I and B is 
((fact \= (count \+ 1\) \* . . . \* n) AND (count \>\= 0\)) AND
 (count \<\> 0\)
which reduces to
(fact \= (count \+ 1\) \* . . . \* n) AND (count \> 0\)
In our case, we must compute the precondition of the body of the loop, using 
the invariant for the postcondition. For
{P} count \= count \- 1 {I}
we compute P to be
{(fact \= count \* (count \+ 1\) \* . . . \* n) AND 
 (count \>\= 1\)}160 Chapter 3 Describing Syntax and Semantics 
Using this as the postcondition for the first assignment in the loop body,
{P} fact \= fact \* count {(fact \= count \* (count \+ 1\)
 \* . . . \* n) AND (count \>\= 1\)}
In this case, P is
{(fact \= (count \+ 1\) \* . . . \* n) AND (count \>\= 1\)}
It is clear that I and B implies this P , so by the rule of consequence, 
{I AND B} S {I}
is true. Finally, the last test of I isI 
AND (NOT B) \=\> Q
For our example, this is
((fact \= (count \+ 1\) \* . . . \* n) AND (count \>\= 0\)) AND
 (count \= 0\)) \=\> fact \= n!
This is clearly true, for when count \= 0 , the first part is precisely the defini\-
tion of factorial. So, our choice of I meets the requirements for a loop invariant. Now we can use our P (which is the same as I) from the 
while as the postcon\-
dition on the second assignment of the program
{P} fact \= 1 {(fact \= (count \+ 1\) \* . . . \* n) AND 
 (count \>\= 0\)}
which yields for P
(1 \= (count \+ 1\) \* . . . \* n) AND (count \>\= 0\))
Using this as the postcondition for the first assignment in the code
{P} count \= n {(1 \= (count \+ 1\) \* . . . \* n) AND 
 (count \>\= 0\))}
produces for P
{(n \+ 1\) \* . . . \* n \= 1\) AND (n \>\= 0\)}
The left operand of the AND operator is true (because 1 \= 1 ) and the right 
operand is exactly the precondition of the whole code segment, { n \>\= 0 }. 
Therefore, the program has been proven to be correct.
3\.5\.3\.8 Evaluation
As stated previously, to define the semantics of a complete programming lan\-guage using the axiomatic method, there must be an axiom or an inference rule for each statement type in the language. Defining axioms or inference rules for Bibliographic Notes 161
some of the statements of programming languages has proven to be a difficult 
task. An obvious solution to this problem is to design the language with the axiomatic method in mind, so that only statements for which axioms or infer\-ence rules can be written are included. Unfortunately, such a language would necessarily leave out some useful and powerful parts.
Axiomatic semantics is a powerful tool for research into program correct\-
ness proofs, and it provides an excellent framework in which to reason about programs, both during their construction and later. Its usefulness in describing the meaning of programming languages to language users and compiler writers is, however, highly limited.
SUMMARY
Backus\-Naur Form and context\-free grammars are equivalent metalanguages that are well suited for the task of describing the syntax of programming lan\-guages. Not only are they concise descriptive tools, but also the parse trees that can be associated with their generative actions give graphical evidence of the underlying syntactic structures. Furthermore, they are naturally related to recognition devices for the languages they generate, which leads to the rela\-tively easy construction of syntax analyzers for compilers for these languages.
An attribute grammar is a descriptive formalism that can describe both the 
syntax and static semantics of a language. Attribute grammars are extensions to context\-free grammars. An attribute grammar consists of a grammar, a set of attributes, a set of attribute computation functions, and a set of predicates, which together describe static semantics rules.
This chapter provides a brief introduction to three methods of semantic 
description: operational, denotational, and axiomatic. Operational semantics is a method of describing the meaning of language constructs in terms of their effects on an ideal machine. In denotational semantics, mathematical objects are used to represent the meanings of language constructs. Language entities are converted to these mathematical objects with recursive functions. Axiomatic semantics, which is based on formal logic, was devised as a tool for proving the correctness of programs.
BIBLIOGRAPHIC NOTES
Syntax description using context\-free grammars and BNF are thoroughly dis\-cussed in Cleaveland and Uzgalis (1976\). 
Research in axiomatic semantics was begun by Floyd (1967\) and fur\-
ther developed by Hoare (1969\). The semantics of a large part of Pascal was described by Hoare and Wirth (1973\) using this method. The parts they did not complete involved functional side effects and goto statements. These were found to be the most difficult to describe.162 Chapter 3 Describing Syntax and Semantics 
The technique of using preconditions and postconditions during the devel\-
opment of programs is described (and advocated) by Dijkstra (1976\) and also discussed in detail in Gries (1981\).
Good introductions to denotational semantics can be found in Gordon 
(1979\) and Stoy (1977\). Introductions to all of the semantics description methods discussed in this chapter can be found in Marcotty et al. (1976\). Another good reference for much of the chapter material is Pagan (1981\). The form of the deno\-tational semantic functions in this chapter is similar to that found in Meyer (1990\).
REVIEW QUESTIONS
 1\. Define syntax and semantics.
 2\. Who are language descriptions for? 3\. Describe the operation of a general language generator. 4\. Describe the operation of a general language recognizer. 5\. What is the difference between a sentence and a sentential form? 6\. Define a left\-recursive grammar rule. 7\. What three extensions are common to most EBNFs? 8\. Distinguish between static and dynamic semantics. 9\. What purpose do predicates serve in an attribute grammar? 10\. What is the difference between a synthesized and an inherited attribute? 11\. How is the order of evaluation of attributes determined for the trees of a 
given attribute grammar?
 12\. What is the primary use of attribute grammars? 13\. Explain the primary uses of a methodology and notation for describing 
the semantics of programming languages.
 14\. Why can machine languages not be used to define statements in opera\-
tional semantics?
 15\. Describe the two levels of uses of operational semantics. 16\. In denotational semantics, what are the syntactic and semantic domains? 17\. What is stored in the state of a program for denotational semantics? 18\. Which semantics approach is most widely known? 19\. What two things must be defined for each language entity in order to 
construct a denotational description of the language?
 20\. Which part of an inference rule is the antecedent? 21\. What is a predicate transformer function? 22\. What does partial correctness mean for a loop construct? 23\. On what branch of mathematics is axiomatic semantics based? 24\. On what branch of mathematics is denotational semantics based?Problem Set 163
 25\. What is the problem with using a software pure interpreter for opera\-
tional semantics?
 26\. Explain what the preconditions and postconditions of a given statement 
mean in axiomatic semantics.
 27\. Describe the approach of using axiomatic semantics to prove the correct\-
ness of a given program.
 28\. Describe the basic concept of denotational semantics. 29\. In what fundamental way do operational semantics and denotational 
semantics differ?
PROBLEM SET
 1\. The two mathematical models of language description are generation 
and recognition. Describe how each can define the syntax of a program\-ming language.
 2\. Write EBNF descriptions for the following:
 a. A Java class definition header statement
 b. A Java method call statement
 c. A C switch statement
 d. A C union definition
 e. C float literals
 3\. Rewrite the BNF of Example 3\.4 to give \+ precedence over \* and force \+ 
to be right associative.
 4\. Rewrite the BNF of Example 3\.4 to add the \+\+ and \-\- unary operators 
of Java.
 5\. Write a BNF description of the Boolean expressions of Java, including 
the three operators \&\&, \|\|, and ! and the relational expressions.
 6\. Using the grammar in Example 3\.2, show a parse tree and a leftmost 
derivation for each of the following statements:
 a. A \= A \* (B \+ (C \* A))
 b. B \= C \* (A \* C \+ B)
 c. A \= A \* (B \+ (C))
 7\. Using the grammar in Example 3\.4, show a parse tree and a leftmost 
derivation for each of the following statements:
 a. A \= ( A \+ B ) \* C
 b. A \= B \+ C \+ A
 c. A \= A \* (B \+ C)
 d. A \= B \* (C \* (A \+ B))164 Chapter 3 Describing Syntax and Semantics 
 8\. Prove that the following grammar is ambiguous:
 ~~→ → \+ \| 
 → a \| b \| c
 9\. Modify the grammar of Example 3\.4 to add a unary minus operator that 
has higher precedence than either \+ or \*.
 10\. Describe, in English, the language defined by the following grammar:
 ~~→ **→ a \| a
 **→ b  **\| b
 → c  \| c
 11\. Consider the following grammar:
 ~~→ a  **b
→ b \| b
 **→ a  **\| a
Which of the following sentences are in the language generated by this 
grammar?
 a. baab
 b. bbbab
 c. bbaaaaa
 d. bbaab
 12\. Consider the following grammar:
 ~~→ a  ~~c  **\| \| b
→ c \| c
 **→ d \| Which of the following sentences are in the language generated by this 
grammar?
 a. abcd
 b. acccbd
 c. acccbcc
 d. acd
 e. accc
 13\. Write a grammar for the language consisting of strings that have n 
copies of the letter a followed by the same number of copies of the letter b, where n 
\> 0\. For example, the strings ab, aaaabbbb, and 
aaaaaaaabbbbbbbb are in the language but a, abb, ba, and aaabb are not.
 14\. Draw parse trees for the sentences aabb and aaaabbbb, as derived from 
the grammar of Problem 13\.Problem Set 165
 15\. Convert the BNF of Example 3\.1 to EBNF .
 16\. Convert the BNF of Example 3\.3 to EBNF . 17\. Convert the following EBNF to BNF:
S → A{bA}
A → a\[b]A
 18\. What is the difference between an intrinsic attribute and a nonintrinsic 
synthesized attribute?
 19\. Write an attribute grammar whose BNF basis is that of Example 3\.6 in 
Section 3\.4\.5 but whose language rules are as follows: Data types cannot be mixed in expressions, but assignment statements need not have the same types on both sides of the assignment operator.
 20\. Write an attribute grammar whose base BNF is that of Example 3\.2 and 
whose type rules are the same as for the assignment statement example of Section 3\.4\.5\.
 21\. Using the virtual machine instructions given in Section 3\.5\.1\.1, give an 
operational semantic definition of the following:
 a. Java do\-while
 b. Ada for
 c. C\+\+ if\-then\-else
 d. C for
 e. C switch
 22\. Write a denotational semantics mapping function for the following 
statements:
 a. Ada for
 b. Java do\-while
 c. Java Boolean expressions
 d. Java for
 e. C switch
 23\. Compute the weakest precondition for each of the following assignment 
statements and postconditions:
 a. a \= 2 \* (b \- 1\) \- 1 {a \> 0}
 b. b \= (c \+ 10\) / 3 {b \> 6}
 c. a \= a \+ 2 \* b \- 1 {a \> 1}
 d. x \= 2 \* y \+ x \- 1 {x \> 11}
 24\. Compute the weakest precondition for each of the following sequences 
of assignment statements and their postconditions:
 a. a \= 2 \* b \+ 1;
 b \= a \- 3
 {b \< 0}166 Chapter 3 Describing Syntax and Semantics 
 b. a \= 3 \* (2 \* b \+ a);
 b \= 2 \* a \- 1 
 {b \> 5}
 25\. Compute the weakest precondition for each of the following selection 
constructs and their postconditions:
 a. if (a \=\= b)
 b \= 2 \* a \+ 1
 else
 b \= 2 \* a;
 {b \> 1}
 b. if (x \< y)
 x \= x \+ 1
 else
 x \= 3 \* x {x \< 0}
 c. if (x \> y)
 y \= 2 \* x \+ 1
 else
 y \= 3 \* x \- 1; {y \> 3}
 26\. Explain the four criteria for proving the correctness of a logical pretest 
loop construct of the form 
while B do S end
 27\. Prove that (n \+1\) \* c \* n\=1
 28\. Prove the following program is correct:
 {n \> 0}
 count \= n; sum \= 0; while count \<\> 0 do
 sum \= sum \+ count;
 count \= count \- 1;
 end
 {sum \= 1 \+ 2 \+ . . . \+ n}167 4\.1 Introduction
 4\.2 Lexical Analysis
 4\.3 The Parsing Problem
 4\.4 Recursive\-Descent Parsing
 4\.5 Bottom\-Up Parsing4
Lexical and Syntax 
Analysis168 Chapter 4 Lexical and Syntax Analysis
Aserious investigation of compiler design requires at least a semester of 
intensive study, including the design and implementation of a compiler for a 
small but realistic programming language. The first part of such a course is 
devoted to lexical and syntax analyses. The syntax analyzer is the heart of a compiler, 
because several other important components, including the semantic analyzer and 
the intermediate code generator, are driven by the actions of the syntax analyzer.
Some readers may wonder why a chapter on any part of a compiler would be 
included in a book on programming languages. There are at least two reasons to include a discussion of lexical and syntax analyses in this book: First, syntax analyzers 
are based directly on the grammars discussed in Chapter 3, so it is natural to discuss 
them as an application of grammars. Second, lexical and syntax analyzers are needed 
in numerous situations outside compiler design. Many applications, among them 
program listing formatters, programs that compute the complexity of programs, and 
programs that must analyze and react to the contents of a configuration file, all need 
to do lexical and syntax analyses. Therefore, lexical and syntax analyses are important 
topics for software developers, even if they never need to write a compiler. Further\-
more, some computer science programs no longer require students to take a compiler 
design course, which leaves students with no instruction in lexical or syntax analysis. 
In those cases, this chapter can be covered in the programming language course. In 
degree programs that require a compiler design course, this chapter can be skipped.
This chapter begins with an introduction to lexical analysis, along with a simple 
example. Next, the general parsing problem is discussed, including the two primary 
approaches to parsing and the complexity of parsing. Then, we introduce the recursive\-
descent implementation technique for top\-down parsers, including examples of parts of 
a recursive\-descent parser and a trace of a parse using one. The last section discusses 
bottom\-up parsing and the LR parsing algorithm. This section includes an example of a 
small LR parsing table and the parse of a string using the LR parsing process.
4\.1 Introduction
Three different approaches to implementing programming languages are 
introduced in Chapter 1: compilation, pure interpretation, and hybrid imple\-mentation. The compilation approach uses a program called a compiler, which translates programs written in a high\-level programming language into machine code. Compilation is typically used to implement programming lan\-guages that are used for large applications, often written in languages such as C\+\+ and COBOL. Pure interpretation systems perform no translation; rather, programs are interpreted in their original form by a software interpreter. Pure interpretation is usually used for smaller systems in which execution efficiency is not critical, such as scripts embedded in HTML documents, written in lan\-guages such as JavaScript. Hybrid implementation systems translate programs written in high\-level languages into intermediate forms, which are interpreted. These systems are now more widely used than ever, thanks in large part to the popularity of scripting languages. T raditionally, hybrid systems have resulted in much slower program execution than compiler systems. However, in recent 4\.2 Lexical Analysis 169
years the use of Just\-in\-Time ( JIT) compilers has become widespread, particu\-
larly for Java programs and programs written for the Microsoft .NET system. A JIT compiler, which translates intermediate code to machine code, is used on 
methods at the time they are first called. In effect, a JIT compiler transforms a hybrid system to a delayed compiler system.
All three of the implementation approaches just discussed use both lexical 
and syntax analyzers.
Syntax analyzers, or parsers, are nearly always based on a formal descrip\-
tion of the syntax of programs. The most commonly used syntax\-description formalism is context\-free grammars, or BNF , which is introduced in Chapter 3\. 
Using BNF , as opposed to using some informal syntax description, has at least three compelling advantages. First, BNF descriptions of the syntax of programs are clear and concise, both for humans and for software systems that use them. Second, the BNF description can be used as the direct basis for the syntax analyzer. Third, implementations based on BNF are relatively easy to maintain because of their modularity.
Nearly all compilers separate the task of analyzing syntax into two distinct 
parts, named lexical analysis and syntax analysis, although this terminology is confusing. The lexical analyzer deals with small\-scale language constructs, such as names and numeric literals. The syntax analyzer deals with the large\-scale constructs, such as expressions, statements, and program units. Section 4\.2 introduces lexical analyzers. Sections 4\.3, 4\.4, and 4\.5 discuss syntax analyzers.
There are three reasons why lexical analysis is separated from syntax 
analysis:
 1\. Simplicity—T echniques for lexical analysis are less complex than those 
required for syntax analysis, so the lexical\-analysis process can be sim\-pler if it is separate. Also, removing the low\-level details of lexical analy\-sis from the syntax analyzer makes the syntax analyzer both smaller and less complex.
 2\. Efficiency—Although it pays to optimize the lexical analyzer, because 
lexical analysis requires a significant portion of total compilation time, it is not fruitful to optimize the syntax analyzer. Separation facilitates this selective optimization.
 3\. Portability—Because the lexical analyzer reads input program files 
and often includes buffering of that input, it is somewhat platform dependent. However, the syntax analyzer can be platform independent. It is always good to isolate machine\-dependent parts of any software system.
4\.2 Lexical Analysis
A lexical analyzer is essentially a pattern matcher. A pattern matcher attempts to 
find a substring of a given string of characters that matches a given character pat\-tern. Pattern matching is a traditional part of computing. One of the earliest uses 170 Chapter 4 Lexical and Syntax Analysis
of pattern matching was with text editors, such as the ed line editor, which was 
introduced in an early version of UNIX. Since then, pattern matching has found its way into some programming languages—for example, Perl and JavaScript. It is also available through the standard class libraries of Java, C\+\+, and C\#.
A lexical analyzer serves as the front end of a syntax analyzer. T echnically, 
lexical analysis is a part of syntax analysis. A lexical analyzer performs syntax analysis at the lowest level of program structure. An input program appears to a compiler as a single string of characters. The lexical analyzer collects characters into logical groupings and assigns internal codes to the groupings according to their structure. In Chapter 3, these logical groupings are named lexemes , and 
the internal codes for categories of these groupings are named tokens . Lex\-
emes are recognized by matching the input character string against character string patterns. Although tokens are usually represented as integer values, for the sake of readability of lexical and syntax analyzers, they are often referenced through named constants.
Consider the following example of an assignment statement:
result \= oldsum – value / 100;
Following are the tokens and lexemes of this statement:
Lexical analyzers extract lexemes from a given input string and produce the 
corresponding tokens. In the early days of compilers, lexical analyzers often processed an entire source program file and produced a file of tokens and lexemes. Now, however, most lexical analyzers are subprograms that locate the next lexeme in the input, determine its associated token code, and return them to the caller, which is the syntax analyzer. So, each call to the lexical analyzer returns a single lexeme and its token. The only view of the input program seen by the syntax analyzer is the output of the lexical analyzer, one token at a time.
The lexical\-analysis process includes skipping comments and white space 
outside lexemes, as they are not relevant to the meaning of the program. Also, the lexical analyzer inserts lexemes for user\-defined names into the symbol table, which is used by later phases of the compiler. Finally, lexical analyzers detect syntactic errors in tokens, such as ill\-formed floating\-point literals, and report such errors to the user.T oken Lexeme
IDENT result
ASSIGN\_OP \=
IDENT oldsum
SUB\_OP \-
IDENT value
DIV\_OP /
INT\_LIT 100
SEMICOLON ; 4\.2 Lexical Analysis 171
There are three approaches to building a lexical analyzer:
 1\. Write a formal description of the token patterns of the language using 
a descriptive language related to regular expressions.1 These descrip\-
tions are used as input to a software tool that automatically generates a lexical analyzer. There are many such tools available for this. The oldest of these, named lex, is commonly included as part of UNIX systems.
 2\. Design a state transition diagram that describes the token patterns of 
the language and write a program that implements the diagram.
 3\. Design a state transition diagram that describes the token patterns of 
the language and hand\-construct a table\-driven implementation of the state diagram.
A state transition diagram, or just state diagram , is a directed graph. The 
nodes of a state diagram are labeled with state names. The arcs are labeled with the input characters that cause the transitions among the states. An arc may also include actions the lexical analyzer must perform when the transition is taken.
State diagrams of the form used for lexical analyzers are representations 
of a class of mathematical machines called finite automata . Finite automata 
can be designed to recognize members of a class of languages called regular 
languages . Regular grammars are generative devices for regular languages. 
The tokens of a programming language are a regular language, and a lexical analyzer is a finite automaton.
We now illustrate lexical\-analyzer construction with a state diagram and 
the code that implements it. The state diagram could simply include states and transitions for each and every token pattern. However, that approach results in a very large and complex diagram, because every node in the state diagram would need a transition for every character in the character set of the language being analyzed. We therefore consider ways to simplify it.
Suppose we need a lexical analyzer that recognizes only arithmetic expres\-
sions, including variable names and integer literals as operands. Assume that the variable names consist of strings of uppercase letters, lowercase letters, and digits but must begin with a letter. Names have no length limitation. The first thing to observe is that there are 52 different characters (any uppercase or low\-ercase letter) that can begin a name, which would require 52 transitions from the transition diagram’s initial state. However, a lexical analyzer is interested only in determining that it is a name and is not concerned with which specific name it happens to be. Therefore, we define a character class named LETTER for all 52 letters and use a single transition on the first letter of any name.
Another opportunity for simplifying the transition diagram is with the 
integer literal tokens. There are 10 different characters that could begin an integer literal lexeme. This would require 10 transitions from the start state of the state diagram. Because specific digits are not a concern of the lexical ana\-lyzer, we can build a much more compact state diagram if we define a character 
 1\. These regular expressions are the basis for the pattern\-matching facilities now part of many 
programming languages, either directly or through a class library.172 Chapter 4 Lexical and Syntax Analysis
class named DIGIT for digits and use a single transition on any character in 
this character class to a state that collects integer literals.
Because our names can include digits, the transition from the node fol\-
lowing the first character of a name can use a single transition on LETTER or DIGIT to continue collecting the characters of a name.
Next, we define some utility subprograms for the common tasks inside the 
lexical analyzer. First, we need a subprogram, which we can name 
getChar , that 
has several duties. When called, getChar gets the next character of input from 
the input program and puts it in the global variable nextChar . getChar must 
also determine the character class of the input character and put it in the global variable 
charClass . The lexeme being built by the lexical analyzer, which 
could be implemented as a character string or an array, will be named lexeme .
We implement the process of putting the character in nextChar into 
the string array lexeme in a subprogram named addChar . This subprogram 
must be explicitly called because programs include some characters that need not be put in 
lexeme , for example the white\-space characters between lex\-
emes. In a more realistic lexical analyzer, comments also would not be placed in 
lexeme .
When the lexical analyzer is called, it is convenient if the next character of 
input is the first character of the next lexeme. Because of this, a function named 
getNonBlank is used to skip white space every time the analyzer is called.
Finally, a subprogram named lookup is needed to compute the token code 
for the single\-character tokens. In our example, these are parentheses and the arithmetic operators. T oken codes are numbers arbitrarily assigned to tokens by the compiler writer.
The state diagram in Figure 4\.1 describes the patterns for our tokens. It 
includes the actions required on each transition of the state diagram.
The following is a C implementation of a lexical analyzer specified in 
the state diagram of Figure 4\.1, including a main driver function for testing purposes:
/\* front.c \- a lexical analyzer system for simple
 arithmetic expressions \*/
\#include 
\#include 
/\* Global declarations \*/
/\* Variables \*/
int charClass;
char lexeme \[100];
char nextChar;
int lexLen;
int token;
int nextToken;
FILE \*in\_fp, \*fopen(); 4\.2 Lexical Analysis 173
/\* Function declarations \*/
void addChar();
void getChar();
void getNonBlank();
int lex();
/\* Character classes \*/
\#define LETTER 0
\#define DIGIT 1
\#define UNKNOWN 99
/\* Token codes \*/
\#define INT\_LIT 10
\#define IDENT 11
\#define ASSIGN\_OP 20
\#define ADD\_OP 21
\#define SUB\_OP 22
\#define MULT\_OP 23
\#define DIV\_OP 24
\#define LEFT\_PAREN 25\#define RIGHT\_PAREN 26Figure 4\.1
A state diagram to 
recognize names, parentheses, and arithmetic operatorsLetter/Digit
Letter
StartaddChar; getChar
return lookup ( lexeme )
Digit
return Int\_Litid
addChar; getChar
addChar; getChar
Digit
addChar; getCharint
return t t←lookup (nextChar )
unknowngetCharDone174 Chapter 4 Lexical and Syntax Analysis
/\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*/
/\* main driver \*/main() {
/\* Open the input data file and process its contents \*/
 if ((in\_fp \= fopen("front.in", "r")) \=\= NULL)
 printf("ERROR \- cannot open front.in \\n");
 else {
 getChar();
 do {
 lex();
 } while (nextToken !\= EOF);
 }
}
/\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*/
/\* lookup \- a function to lookup operators and parentheses
 and return the token \*/
int lookup( char ch) {
 switch (ch) {
 case '(':
 addChar(); nextToken \= LEFT\_PAREN; break;
 case ')':
 addChar();
 nextToken \= RIGHT\_PAREN;
 break;
 case '\+':
 addChar();
 nextToken \= ADD\_OP;
 break;
 case '\-':
 addChar(); nextToken \= SUB\_OP;
 break; 
 case '\*':
 addChar();
 nextToken \= MULT\_OP; break; 4\.2 Lexical Analysis 175
 case '/':
 addChar(); nextToken \= DIV\_OP; break;
 default:
 addChar();
 nextToken \= EOF; break;
 } return nextToken;
}
/\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*/
/\* addChar \- a function to add nextChar to lexeme \*/
void addChar() {
 if (lexLen \<\= 98\) {
 lexeme\[lexLen\+\+] \= nextChar;
 lexeme\[lexLen] \= 0;
 } else
 printf("Error \- lexeme is too long \\n");
}
/\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*/
/\* getChar \- a function to get the next character of 
 input and determine its character class \*/
void getChar() {
 if ((nextChar \= getc(in\_fp)) !\= EOF) {
 if (isalpha(nextChar))
 charClass \= LETTER;
 else if (isdigit(nextChar))
 charClass \= DIGIT;
 else charClass \= UNKNOWN;
 }
 else
 charClass \= EOF;
}
/\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*/
/\* getNonBlank \- a function to call getChar until it
 returns a non\-whitespace character \*/
void getNonBlank() {
 while (isspace(nextChar))
 getChar();}176 Chapter 4 Lexical and Syntax Analysis
/
\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*\*//\* lex \- a simple lexical analyzer for arithmetic expressions \*/int lex() {
 lexLen \= 0;
 getNonBlank();
 switch (charClass) {
/\* Parse identifiers \*/
 case LETTER:
 addChar();
 getChar();
 while (charClass \=\= LETTER \|\| charClass \=\= DIGIT) {
 addChar(); getChar(); }
 nextToken \= IDENT;
 break;
/\* Parse integer literals \*/
 case DIGIT:
 addChar(); getChar(); while (charClass \=\= DIGIT) {
 addChar();
 getChar();
 }
 nextToken \= INT\_LIT;
 break;
/\* Parentheses and operators \*/
 case UNKNOWN:
 lookup(nextChar);
 getChar();
 break;
/\* EOF \*/
 case EOF:
 nextToken \= EOF;
 lexeme\[0] \= 'E';
 lexeme\[1] \= 'O';
 lexeme\[2] \= 'F';
 lexeme\[3] \= 0;
 break;
 } /\* End of switch \*/ 4\.3 The Parsing Problem 177
 printf("Next token is: %d, Next lexeme is %s\\n", 
 nextToken, lexeme); return nextToken;
} /\* End of function lex \*/
This code illustrates the relative simplicity of lexical analyzers. Of course, we 
have left out input buffering, as well as some other important details. Further\-more, we have dealt with a very small and simple input language.
Consider the following expression:
(sum \+ 47\) / total
Following is the output of the lexical analyzer of front.c when used on this 
expression:
Next token is: 25 Next lexeme is (
Next token is: 11 Next lexeme is sum
Next token is: 21 Next lexeme is \+Next token is: 10 Next lexeme is 47Next token is: 26 Next lexeme is )Next token is: 24 Next lexeme is /Next token is: 11 Next lexeme is total
Next token is: \-1 Next lexeme is EOF
Names and reserved words in programs have similar patterns. Although it is 
possible to build a state diagram to recognize every specific reserved word of a programming language, that would result in a prohibitively large state diagram. It is much simpler and faster to have the lexical analyzer recognize names and reserved words with the same pattern and use a lookup in a table of reserved words to determine which names are reserved words. Using this approach con\-siders reserved words to be exceptions in the names token category.
A lexical analyzer often is responsible for the initial construction of the 
symbol table, which acts as a database of names for the compiler. The entries in the symbol table store information about user\-defined names, as well as the attributes of the names. For example, if the name is that of a variable, the vari\-able’s type is one of its attributes that will be stored in the symbol table. Names are usually placed in the symbol table by the lexical analyzer. The attributes of a name are usually put in the symbol table by some part of the compiler that is subsequent to the actions of the lexical analyzer.
4\.3 The Parsing Problem
The part of the process of analyzing syntax that is referred to as syntax analysis 
is often called parsing . We will use these two interchangeably.
This section discusses the general parsing problem and introduces the two 
main categories of parsing algorithms, top\-down and bottom\-up, as well as the complexity of the parsing process.178 Chapter 4 Lexical and Syntax Analysis
4\.3\.1 Introduction to Parsing
Parsers for programming languages construct parse trees for given programs. 
In some cases, the parse tree is only implicitly constructed, meaning that per\-haps only a traversal of the tree is generated. But in all cases, the information required to build the parse tree is created during the parse. Both parse trees and derivations include all of the syntactic information needed by a language processor.
There are two distinct goals of syntax analysis: First, the syntax analyzer 
must check the input program to determine whether it is syntactically correct. When an error is found, the analyzer must produce a diagnostic message and recover. In this case, recovery means it must get back to a normal state and continue its analysis of the input program. This step is required so that the compiler finds as many errors as possible during a single analysis of the input program. If it is not done well, error recovery may create more errors, or at least more error messages. The second goal of syntax analysis is to produce a complete parse tree, or at least trace the structure of the complete parse tree, for syntactically correct input. The parse tree (or its trace) is used as the basis for translation.
Parsers are categorized according to the direction in which they build parse 
trees. The two broad classes of parsers are top\-down , in which the tree is built 
from the root downward to the leaves, and bottom\-up , in which the parse tree 
is built from the leaves upward to the root.
In this chapter, we use a small set of notational conventions for grammar 
symbols and strings to make the discussion less cluttered. For formal languages, they are as follows:
 1\. T erminal symbols—lowercase letters at the beginning of the alphabet 
(a, b, . . .)
 2\. Nonterminal symbols—uppercase letters at the beginning of the alpha\-
bet (A, B, . . .)
 3\. T erminals or nonterminals—uppercase letters at the end of the alphabet 
(W, X, Y, Z)
 4\. Strings of terminals—lowercase letters at the end of the alphabet (w, x, 
y, z)
 5\. Mixed strings (terminals and/or nonterminals)—lowercase Greek letters 
(/H9251, /H9252, /H9254, /H9253\)
For programming languages, terminal symbols are the small\-scale syntac\-
tic constructs of the language, what we have referred to as lexemes. The nonterminal symbols of programming languages are usually connotative names or abbreviations, surrounded by pointed brackets—for example, , , and . The sentences of a lan\-guage (programs, in the case of a programming language) are strings of terminals. Mixed strings describe right\-hand sides (RHSs) of grammar rules and are used in parsing algorithms. 4\.3 The Parsing Problem 179
4\.3\.2 Top\-Down Parsers
A top\-down parser traces or builds a parse tree in preorder. A preorder traversal 
of a parse tree begins with the root. Each node is visited before its branches are followed. Branches from a particular node are followed in left\-to\-right order. This corresponds to a leftmost derivation.
In terms of the derivation, a top\-down parser can be described as follows: 
Given a sentential form that is part of a leftmost derivation, the parser’s task is to find the next sentential form in that leftmost derivation. The general form of a left sentential form is xA /H9251, whereby our notational conventions x is a string 
of terminal symbols, A is a nonterminal, and /H9251 is a mixed string. Because x 
contains only terminals, A is the leftmost nonterminal in the sentential form, so it is the one that must be expanded to get the next sentential form in a left\-most derivation. Determining the next sentential form is a matter of choosing the correct grammar rule that has A as its LHS. For example, if the current sentential form is
xA/H9251
and the A\-rules are A → bB, A → cBb, and A → a, a top\-down parser must 
choose among these three rules to get the next sentential form, which could be xbB /H9251, xcBb /H9251, or xa /H9251\. This is the parsing decision problem for top\-down 
parsers.
Different top\-down parsing algorithms use different information to make 
parsing decisions. The most common top\-down parsers choose the correct RHS for the leftmost nonterminal in the current sentential form by com\-paring the next token of input with the first symbols that can be generated by the RHSs of those rules. Whichever RHS has that token at the left end of the string it generates is the correct one. So, in the sentential form xA /H9251, 
the parser would use whatever token would be the first generated by A to determine which A\-rule should be used to get the next sentential form. In the example above, the three RHSs of the A\-rules all begin with different terminal symbols. The parser can easily choose the correct RHS based on the next token of input, which must be a, b, or c in this example. In general, choosing the correct RHS is not so straightforward, because some of the RHSs of the leftmost nonterminal in the current sentential form may begin with a nonterminal.
The most common top\-down parsing algorithms are closely related. 
A recursive\-descent parser is a coded version of a syntax analyzer based 
directly on the BNF description of the syntax of language. The most com\-mon alternative to recursive descent is to use a parsing table, rather than code, to implement the BNF rules. Both of these, which are called LL algo\-
rithms , are equally powerful, meaning they work on the same subset of all 
context\-free grammars. The first L in LL specifies a left\-to\-right scan of the input; the second L specifies that a leftmost derivation is generated. Section 4\.4 introduces the recursive\-descent approach to implementing an LL parser.180 Chapter 4 Lexical and Syntax Analysis
4\.3\.3 Bottom\-Up Parsers
A bottom\-up parser constructs a parse tree by beginning at the leaves and 
progressing toward the root. This parse order corresponds to the reverse of a rightmost derivation. That is, the sentential forms of the derivation are pro\-duced in order of last to first. In terms of the derivation, a bottom\-up parser can be described as follows: Given a right sentential form 
/H9251,2 the parser must 
determine what substring of /H9251 is the RHS of the rule in the grammar that must 
be reduced to its LHS to produce the previous sentential form in the rightmost derivation. For example, the first step for a bottom\-up parser is to determine which substring of the initial given sentence is the RHS to be reduced to its corresponding LHS to get the second last sentential form in the derivation. The process of finding the correct RHS to reduce is complicated by the fact that a given right sentential form may include more than one RHS from the grammar of the language being parsed. The correct RHS is called the handle .
Consider the following grammar and derivation:
S→aAc
A→aA /H20841 b
S \=\> aAc \=\> aaAc \=\> aabc
A bottom\-up parser of this sentence, aabc, starts with the sentence and must 
find the handle in it. In this example, this is an easy task, for the string contains only one RHS, b. When the parser replaces b with its LHS, A, it gets the sec\-ond to last sentential form in the derivation, aaAc. In the general case, as stated previously, finding the handle is much more difficult, because a sentential form may include several different RHSs.
A bottom\-up parser finds the handle of a given right sentential form by 
examining the symbols on one or both sides of a possible handle. Symbols to the right of the possible handle are usually tokens in the input that have not yet been analyzed.
The most common bottom\-up parsing algorithms are in the LR family, 
where the L specifies a left\-to\-right scan of the input and the R specifies that a rightmost derivation is generated.
4\.3\.4 The Complexity of Parsing
Parsing algorithms that work for any unambiguous grammar are complicated and inefficient. In fact, the complexity of such algorithms is O(n
3\), which means 
the amount of time they take is on the order of the cube of the length of the string to be parsed. This relatively large amount of time is required because these algorithms frequently must back up and reparse part of the sentence being analyzed. Reparsing is required when the parser has made a mistake in 
 2\. A right sentential form is a sentential form that appears in a rightmost derivation. 4\.4 Recursive\-Descent Parsing 181
the parsing process. Backing up the parser also requires that part of the parse 
tree being constructed (or its trace) must be dismantled and rebuilt. O(n3\) algo\-
rithms are normally not useful for practical processes, such as syntax analysis for a compiler, because they are far too slow. In situations such as this, computer scientists often search for algorithms that are faster, though less general. Gen\-erality is traded for efficiency. In terms of parsing, faster algorithms have been found that work for only a subset of the set of all possible grammars. These algorithms are acceptable as long as the subset includes grammars that describe programming languages. (Actually, as discussed in Chapter 3, the whole class of context\-free grammars is not adequate to describe all of the syntax of most programming languages.)
All algorithms used for the syntax analyzers of commercial compilers 
have complexity O(n), which means the time they take is linearly related to the length of the string to be parsed. This is vastly more efficient than O(n
3\) 
algorithms.
4\.4 Recursive\-Descent Parsing
This section introduces the recursive\-descent top\-down parser implementa\-
tion process.
4\.4\.1 The Recursive\-Descent Parsing Process
A recursive\-descent parser is so named because it consists of a collection of subprograms, many of which are recursive, and it produces a parse tree in top\-down order. This recursion is a reflection of the nature of programming languages, which include several different kinds of nested structures. For example, statements are often nested in other statements. Also, parentheses in expressions must be properly nested. The syntax of these structures is naturally described with recursive grammar rules.
EBNF is ideally suited for recursive\-descent parsers. Recall from Chapter 
3 that the primary EBNF extensions are braces, which specify that what they enclose can appear zero or more times, and brackets, which specify that what they enclose can appear once or not at all. Note that in both cases, the enclosed symbols are optional. Consider the following examples:
 → 
if   \[ else ]
 → ident {, ident}
In the first rule, the else clause of an if statement is optional. In the second, 
an  is an identifier, followed by zero or more repetitions of a comma and an identifier.
A recursive\-descent parser has a subprogram for each nonterminal in its 
associated grammar. The responsibility of the subprogram associated with a particular nonterminal is as follows: When given an input string, it traces 182 Chapter 4 Lexical and Syntax Analysis
out the parse tree that can be rooted at that nonterminal and whose leaves 
match the input string. In effect, a recursive\-descent parsing subprogram is a parser for the language (set of strings) that is generated by its associated nonterminal.
Consider the following EBNF description of simple arithmetic expressions:
 →  {(
\+ \| \-) }
 →  {( \* \| /) }
 → id \| int\_constant \| (  )
Recall from Chapter 3 that an EBNF grammar for arithmetic expressions, such 
as this one, does not force any associativity rule. Therefore, when using such a grammar as the basis for a compiler, one must take care to ensure that the code generation process, which is normally driven by syntax analysis, produces code that adheres to the associativity rules of the language. This can easily be done when recursive\-descent parsing is used.
In the following recursive\-descent function, 
expr , the lexical analyzer is 
the function that is implemented in Section 4\.2\. It gets the next lexeme and puts its token code in the global variable 
nextToken . The token codes are defined 
as named constants, as in Section 4\.2\.
A recursive\-descent subprogram for a rule with a single RHS is relatively 
simple. For each terminal symbol in the RHS, that terminal symbol is com\-pared with 
nextToken . If they do not match, it is a syntax error. If they match, 
the lexical analyzer is called to get the next input token. For each nonterminal, the parsing subprogram for that nonterminal is called.
The recursive\-descent subprogram for the first rule in the previous exam\-
ple grammar, written in C, is
/\* expr
 Parses strings in the language generated by the rule:
  \-\>  {(\+ \| \-) }
 \*/
void expr() {
 printf("Enter \\n");
/\* Parse the first term \*/
 term();
/\* As long as the next token is \+ or \-, get
 the next token and parse the next term \*/
 while (nextToken \=\= ADD\_OP \|\| nextToken \=\= SUB\_OP) {
 lex();
 term();
 } printf("Exit \\n");} /\* End of function expr \*/ 4\.4 Recursive\-Descent Parsing 183
Notice that the expr function includes tracing output statements, which are 
included to produce the example output shown later in this section.
Recursive\-descent parsing subprograms are written with the convention 
that each one leaves the next token of input in nextToken . So, whenever 
a parsing function begins, it assumes that nextToken has the code for the 
leftmost token of the input that has not yet been used in the parsing process.
The part of the language that the expr function parses consists of one or 
more terms, separated by either plus or minus operators. This is the language generated by the nonterminal . Therefore, first it calls the function that parses terms (
term ). Then it continues to call that function as long as it 
finds ADD\_OP or SUB\_OP tokens (which it passes over by calling lex). This 
recursive\-descent function is simpler than most, because its associated rule has only one RHS. Furthermore, it does not include any code for syntax error detection or recovery, because there are no detectable errors associated with the grammar rule.
A recursive\-descent parsing subprogram for a nonterminal whose rule has 
more than one RHS begins with code to determine which RHS is to be parsed. Each RHS is examined (at compiler construction time) to determine the set of terminal symbols that can appear at the beginning of sentences it can generate. By matching these sets against the next token of input, the parser can choose the correct RHS.
The parsing subprogram for  is similar to that for :
/\* term
 Parses strings in the language generated by the rule:
  \-\>  {(\* \| /) ) 
 \*/
void term() {
 printf("Enter \\n");
/\* Parse the first factor \*/
 factor();
/\* As long as the next token is \* or /, get the
 next token and parse the next factor \*/
 while (nextToken \=\= MULT\_OP \|\| nextToken \=\= DIV\_OP) {
 lex();
 factor();
 }
 printf("Exit \\n");
} /\* End of function term \*/
The function for the  nonterminal of our arithmetic expression 
grammar must choose between its two RHSs. It also includes error detection. In the function for , the reaction to detecting a syntax error is simply to call the 
error function. In a real parser, a diagnostic message must be 184 Chapter 4 Lexical and Syntax Analysis
produced when an error is detected. Furthermore, parsers must recover from 
the error so that the parsing process can continue.
/\* factor
 Parses strings in the language generated by the rule:
  \-\> id \| int\_constant \| ( \\n");
/\* Determine which RHS \*/
 if (nextToken \=\= IDENT \|\| nextToken \=\= INT\_LIT)
/\* Get the next token \*/
 lex();
/\* If the RHS is ( ), call lex to pass over the 
 left parenthesis, call expr, and check for the right
 parenthesis \*/
 else {
 if (nextToken \=\= LEFT\_PAREN) {
 lex(); expr(); if (nextToken \=\= RIGHT\_PAREN)
 lex();
 else
 error();
 } /\* End of if (nextToken \=\= ... \*/
/\* It was not an id, an integer literal, or a left
 parenthesis \*/
 else
 error();
 } /\* End of else \*/
 printf("Exit \\n");;
} /\* End of function factor \*/
Following is the trace of the parse of the example expression (sum \+ 47\) / 
total , using the parsing functions expr , term , and factor , and the function 
lex from Section 4\.2\. Note that the parse begins by calling lex and the start 
symbol routine, in this case, expr .
Next token is: 25 Next lexeme is (
Enter Enter Enter  4\.4 Recursive\-Descent Parsing 185
Figure 4\.2
Parse tree for 
(sum \+47\) / total
sum total () / 47
\+
Next token is: 11 Next lexeme is sum
Enter Enter Enter Next token is: 21 Next lexeme is \+
Exit 
Exit 
Next token is: 10 Next lexeme is 47
Enter 
Enter 
Next token is: 26 Next lexeme is )
Exit 
Exit 
Exit Next token is: 24 Next lexeme is /Exit 
Next token is: 11 Next lexeme is total
Enter 
Next token is: \-1 Next lexeme is EOF
Exit Exit Exit 
The parse tree traced by the parser for the preceding expression is shown in 
Figure 4\.2\.186 Chapter 4 Lexical and Syntax Analysis
One more example grammar rule and parsing function should help solidify 
the reader’s understanding of recursive\-descent parsing. Following is a gram\-matical description of the Java 
if statement:
 → if ()  \[ else ]
The recursive\-descent subprogram for this rule follows:
/\* Function ifstmt
 Parses strings in the language generated by the rule:  \-\> if ()  \[else ]
 \*/
void ifstmt() {
/\* Be sure the first token is 'if' \*/
 if (nextToken !\= IF\_CODE)
 error();
 else {
/\* Call lex to get to the next token \*/
 lex();
/\* Check for the left parenthesis \*/
 if (nextToken !\= LEFT\_PAREN)
 error();
 else {
/\* Call boolexpr to parse the Boolean expression \*/
 boolexpr();
/\* Check for the right parenthesis \*/
 if (nextToken !\= RIGHT\_PAREN)
 error();
 else {
/\* Call statement to parse the then clause \*/
 statement();
/\* If an else is next, parse the else clause \*/
 if (nextToken \=\= ELSE\_CODE) {
/\* Call lex to get over the else \*/
 lex();
 statement();
 } /\* end of if (nextToken \=\= ELSE\_CODE ... \*/
 } /\* end of else of if (nextToken !\= RIGHT ... \*/
 } /\* end of else of if (nextToken !\= LEFT ... \*/
 } /\* end of else of if (nextToken !\= IF\_CODE ... \*/
} /\* end of ifstmt \*/
Notice that this function uses parser functions for statements and Boolean 
expressions, which are not given in this section.
The objective of these examples is to convince you that a recursive\-descent 
parser can be easily written if an appropriate grammar is available for the 4\.4 Recursive\-Descent Parsing 187
language. The characteristics of a grammar that allows a recursive\-descent 
parser to be built are discussed in the following subsection.
4\.4\.2 The LL Grammar Class
Before choosing to use recursive descent as a parsing strategy for a compiler or other program analysis tool, one must consider the limitations of the approach, in terms of grammar restrictions. This section discusses these restrictions and their possible solutions.
One simple grammar characteristic that causes a catastrophic problem for 
LL parsers is left recursion. For example, consider the following rule:
A→A\+B
A recursive\-descent parser subprogram for A immediately calls itself to parse 
the first symbol in its RHS. That activation of the A parser subprogram then immediately calls itself again, and again, and so forth. It is easy to see that this leads nowhere (except to a stack overflow).
The left recursion in the rule A →A\+B is called direct left recursion , 
because it occurs in one rule. Direct left recursion can be eliminated from a grammar by the following process:
For each nonterminal, A,
 1\. Group the A\-rules as A →A/H9251
1, /H20841c /H20841A/H9251m /H20841 /H92521 /H20841 /H92522 /H20841 c /H20841 /H9252n
where none of the /H9252\>s begins with A
 2\. Replace the original A\-rules with
A→/H92521A/H11032 /H20841 /H92522A/H11032 /H20841 c /H20841 /H9252nA/H11032
A/H11032→/H92511A/H11032 /H20841 /H92512A/H11032 /H20841 /H9251mA/H11032/H20841 /H9255
Note that /H9255 specifies the empty string. A rule that has /H9255 as its RHS is called an 
erasure rule , because its use in a derivation effectively erases its LHS from the 
sentential form.
Consider the following example grammar and the application of the 
above process:
E→E\+T /H20841 T
T→T \* F /H20841 F
F→(E) /H20841 id
For the E\-rules, we have /H92511\=\+ T and /H9252\=T, so we replace the E\-rules with
E→T E/H11032
E/H11032→\+ T E/H11032 /H20841 /H9255
For the T\-rules, we have /H92511\=\* F and /H9252\=F, so we replace the T\-rules with
T→F T/H11032
T/H11032→\* F T /H11032 /H20841 /H9255188 Chapter 4 Lexical and Syntax Analysis
Because there is no left recursion in the F\-rules, they remain the same, so the 
complete replacement grammar is
E→T E/H11032
E/H11032→\+ T E/H11032 /H20841 /H9255
T→F T/H11032
T/H11032→\* F T/H11032 /H20841 /H9255
F →(E) /H20841 id
This grammar generates the same language as the original grammar but is not 
left recursive.
As was the case with the expression grammar written using EBNF in 
Section 4\.4\.1, this grammar does not specify left associativity of operators. However, it is relatively easy to design the code generation based on this grammar so that the addition and multiplication operators will have left associativity.
Indirect left recursion poses the same problem as direct left recursion. For 
example, suppose we have
A→B a A
B→A b
A recursive\-descent parser for these rules would have the A subprogram imme\-
diately call the subprogram for B, which immediately calls the A subprogram. So, the problem is the same as for direct left recursion. The problem of left recursion is not confined to the recursive\-descent approach to building top\-down parsers. It is a problem for all top\-down parsing algorithms. Fortunately, left recursion is not a problem for bottom\-up parsing algorithms.
There is an algorithm to modify a given grammar to remove indirect left 
recursion (Aho et al., 2006\), but it is not covered here. When writing a gram\-mar for a programming language, one can usually avoid including left recur\-sion, both direct and indirect.
Left recursion is not the only grammar trait that disallows top\-down pars\-
ing. Another is whether the parser can always choose the correct RHS on the basis of the next token of input, using only the first token generated by the leftmost nonterminal in the current sentential form. There is a relatively simple test of a non–left recursive grammar that indicates whether this can be done, called the pairwise disjointness test . This test requires the ability to compute 
a set based on the RHSs of a given nonterminal symbol in a grammar. These sets, which are called FIRST, are defined as
FIRST( /H9251\)\={a /H20841 /H9251 \=\>\* a/H9252} (If /H9251 \=\>\* /H9255, /H9255 is in FIRS
T(/H9251\))
in which \=\>\* means 0 or more derivation steps.
An algorithm to compute FIRST for any mixed string /H9251 can be found in 
Aho et al. (2006\). For our purposes, FIRST can usually be computed by inspec\-tion of the grammar. 4\.4 Recursive\-Descent Parsing 189
The pairwise disjointness test is as follows:
 For each nonterminal, A, in the grammar that has more than one RHS, 
for each pair of rules, A →/H9251i and A →/H9251j, it must be true that
FIRST( /H9251i) x FIRST( /H9251j)\=/H9278
 (The intersection of the two sets, FIRS T(/H9251i) and FIRS T(/H9251j), must be 
empty.)
In other words, if a nonterminal A has more than one RHS, the first ter\-
minal symbol that can be generated in a derivation for each of them must be unique to that RHS. Consider the following rules:
A→aB /H20841 bAb /H20841 Bb
B→cB /H20841 d
The FIRST sets for the RHSs of the A\-rules are {a}, {b}, and {c, d}, which 
are clearly disjoint. Therefore, these rules pass the pairwise disjointness test. What this means, in terms of a recursive\-descent parser, is that the code of the subprogram for parsing the nonterminal A can choose which RHS it is dealing with by seeing only the first terminal symbol of input (token) that is generated by the nonterminal. Now consider the rules
A→aB /H20841 BAb
B→aB /H20841 b
The FIRST sets for the RHSs in the A\-rules are {a} and {a, b}, which are clearly not 
disjoint. So, these rules fail the pairwise disjointness test. In terms of the parser, the subprogram for A could not determine which RHS was being parsed by looking at the next symbol of input, because if it were an a, it could be either RHS. This issue is of course more complex if one or more of the RHSs begin with nonterminals.
In many cases, a grammar that fails the pairwise disjointness test can be 
modified so that it will pass the test. For example, consider the rule
 → identifier /H20841 identifier \[]
This states that a  is either an identifier or an identifier followed by 
an expression in brackets (a subscript). These rules clearly do not pass the pair\-wise disjointness test, because both RHSs begin with the same terminal, identi\-fier. This problem can be alleviated through a process called left factoring .
We now take an informal look at left factoring. Consider our rules for 
. Both RHSs begin with identifier. The parts that follow identifier in the two RHSs are /H9255 (the empty string) and \[]. The two rules can 
be replaced by the following two rules:
 → identifier 
 → /H9255 /H20841 \[]190 Chapter 4 Lexical and Syntax Analysis
It is not difficult to see that together, these two rules generate the same lan\-
guage as the two rules with which we began. However, these two pass the pairwise disjointness test.
If the grammar is being used as the basis for a recursive\-descent parser, an 
alternative to left factoring is available. With an EBNF extension, the problem disappears in a way that is very similar to the left factoring solution. Consider the original rules above for . The subscript can be made optional by placing it in square brackets, as in
 → identifier \[ \[ E \+ T 
 \=\> E \+ T \* F 
 \=\> E \+ T \* id 
 \=\> E \+ F \* id 
 \=\> E \+ id \* id
 \=\> T \+ id \* id
 \=\> F \+ id \* id
 \=\> id \+ id \* id 4\.5 Bottom\-Up Parsing 191
The underlined part of each sentential form in this derivation is the RHS that 
is rewritten as its corresponding LHS to get the previous sentential form. The process of bottom\-up parsing produces the reverse of a rightmost derivation. So, in the example derivation, a bottom\-up parser starts with the last sentential form (the input sentence) and produces the sequence of sentential forms from there until all that remains is the start symbol, which in this grammar is E. In each step, the task of the bottom\-up parser is to find the specific RHS, the handle, in the sentential form that must be rewritten to get the next (previous) sentential form. As mentioned earlier, a right sentential form may include more than one RHS. For example, the right sentential form
E 
\+ T \* id
includes three RHSs, E \+ T, T, and id. Only one of these is the handle. For 
example, if the RHS E \+ T were chosen to be rewritten in this sentential form, the resulting sentential form would be E \* id, but 
E \* id is not a legal right 
sentential form for the given grammar.
The handle of a right sentential form is unique. The task of a bottom\-up 
parser is to find the handle of any given right sentential form that can be gener\-ated by its associated grammar. Formally, handle is defined as follows:
Definition: /H9252 is the handle of the right sentential form /H9253\=/H9251/H9252w if and 
only if S \=7\*
rm /H9251Aw \=7rm /H9251/H9252w
In this definition, \=7rm specifies a rightmost derivation step, and \=7\*rm 
specifies zero or more rightmost derivation steps. Although the definition of a handle is mathematically concise, it provides little help in finding the handle of a given right sentential form. In the following, we provide the definitions of several substrings of sentential forms that are related to handles. The purpose of these is to provide some intuition about handles.
Definition: /H9252 is a phrase of the right sentential form /H9253 if and only if 
S \=7\* /H9253\=/H9251
1A/H92512 \=7 \+ /H92511/H9252/H92512
In this definition, \=\>\+ means one or more derivation steps.
Definition: /H9252 is a simple phrase of the right sentential form /H9253 if and 
only if S \=7\* /H9253\=/H92511A/H92512 \=7 /H92511/H9252/H92512
If these two definitions are compared carefully, it is clear that they differ only 
in the last derivation specification. The definition of phrase uses one or more steps, while the definition of simple phrase uses exactly one step.
The definitions of phrase and simple phrase may appear to have the same 
lack of practical value as that of a handle, but that is not true. Consider what a phrase is relative to a parse tree. It is the string of all of the leaves of the par\-tial parse tree that is rooted at one particular internal node of the whole parse tree. A simple phrase is just a phrase that takes a single derivation step from its 192 Chapter 4 Lexical and Syntax Analysis
root nonterminal node. In terms of a parse tree, a phrase can be derived from 
a single nonterminal in one or more tree levels, but a simple phrase can be derived in just a single tree level. Consider the parse tree shown in Figure 4\.3\.
The leaves of the parse tree in Figure 4\.3 comprise the sentential form 
E \+ T \* id. Because there are three internal nodes, there are three phrases. Each internal node is the root of a subtree, whose leaves are a phrase. The root node of the whole parse tree, E, generates all of the resulting sentential form, E \+ T \* id, which is a phrase. The internal node, T, generates the leaves T \* id, which is another phrase. Finally, the internal node, F , generates id, which is also a phrase. So, the phrases of the sentential form E \+ T \* id are E \+ T \* id, T \* id, and id. Notice that phrases are not necessarily RHSs in the underlying grammar.
The simple phrases are a subset of the phrases. In the previous example, 
the only simple phrase is id. A simple phrase is always an RHS in the grammar.
The reason for discussing phrases and simple phrases is this: The handle 
of any rightmost sentential form is its leftmost simple phrase. So now we have a highly intuitive way to find the handle of any right sentential form, assum\-ing we have the grammar and can draw a parse tree. This approach to finding handles is of course not practical for a parser. (If you already have a parse tree, why do you need a parser?) Its only purpose is to provide the reader with some intuitive feel for what a handle is, relative to a parse tree, which is easier than trying to think about handles in terms of sentential forms.
We can now consider bottom\-up parsing in terms of parse trees, although 
the purpose of a parser is to produce a parse tree. Given the parse tree for an entire sentence, you easily can find the handle, which is the first thing to rewrite in the sentence to get the previous sentential form. Then the handle can be pruned from the parse tree and the process repeated. Continuing to the root of the parse tree, the entire rightmost derivation can be constructed.
4\.5\.2 Shift\-Reduce Algorithms
Bottom\-up parsers are often called shift\-reduce algorithms , because shift 
and reduce are the two most common actions they specify. An integral part of every bottom\-up parser is a stack. As with other parsers, the input to a Figure 4\.3
A parse tree for 
E\+T \* id
F
TE
\* id \+T
E 4\.5 Bottom\-Up Parsing 193
bottom\-up parser is the stream of tokens of a program and the output is a 
sequence of grammar rules. The shift action moves the next input token onto the parser’s stack. A reduce action replaces an RHS (the handle) on top of the parser’s stack by its corresponding LHS. Every parser for a programming lan\-guage is a pushdown automaton (PDA ), because a PDA is a recognizer for 
a context\-free language. You need not be intimate with PDAs to understand how a bottom\-up parser works, although it helps. A PDA is a very simple mathematical machine that scans strings of symbols from left to right. A PDA is so named because it uses a pushdown stack as its memory. PDAs can be used as recognizers for context\-free languages. Given a string of symbols over the alphabet of a context\-free language, a PDA that is designed for the purpose can determine whether the string is or is not a sentence in the language. In the process, the PDA can produce the information needed to construct a parse tree for the sentence.
With a PDA, the input string is examined, one symbol at a time, left to 
right. The input is treated very much as if it were stored in another stack, because the PDA never sees more than the leftmost symbol of the input.
Note that a recursive\-descent parser is also a PDA. In that case, the stack 
is that of the run\-time system, which records subprogram calls (among other things), which correspond to the nonterminals of the grammar.
4\.5\.3 LR Parsers
Many different bottom\-up parsing algorithms have been devised. Most of them are variations of a process called LR. LR parsers use a relatively small program and a parsing table that is built for a specific programming lan\-guage. The original LR algorithm was designed by Donald Knuth (Knuth, 1965\). This algorithm, which is sometimes called canonical LR , was not 
used in the years immediately following its publication because producing the required parsing table required large amounts of computer time and memory. Subsequently, several variations on the canonical LR table con\-struction process were developed (DeRemer, 1971; DeRemer and Pennello, 1982\). These are characterized by two properties: (1\) They require far less computer resources to produce the required parsing table than the canoni\-cal LR algorithm, and (2\) they work on smaller classes of grammars than the canonical LR algorithm.
There are three advantages to LR parsers:
 1\. They can be built for all programming languages.
 2\. They can detect syntax errors as soon as it is possible in a left\-to\-right 
scan.
 3\. The LR class of grammars is a proper superset of the class parsable by 
LL parsers (for example, many left recursive grammars are LR, but none are LL).
The only disadvantage of LR parsing is that it is difficult to produce by hand 
the parsing table for a given grammar for a complete programming language. 194 Chapter 4 Lexical and Syntax Analysis
This is not a serious disadvantage, however, for there are several programs 
available that take a grammar as input and produce the parsing table, as dis\-cussed later in this section.
Prior to the appearance of the LR parsing algorithm, there were a number 
of parsing algorithms that found handles of right sentential forms by looking both to the left and to the right of the substring of the sentential form that was suspected of being the handle. Knuth’s insight was that one could effectively look to the left of the suspected handle all the way to the bottom of the parse stack to determine whether it was the handle. But all of the information in the parse stack that was relevant to the parsing process could be represented by a single state, which could be stored on the top of the stack. In other words, Knuth discovered that regardless of the length of the input string, the length of the sentential form, or the depth of the parse stack, there were only a relatively small number of different situations, as far as the parsing process is concerned. Each situation could be represented by a state and stored in the parse stack, one state symbol for each grammar symbol on the stack. At the top of the stack would always be a state symbol, which represented the relevant information from the entire history of the parse, up to the current time. We will use sub\-scripted uppercase S’s to represent the parser states.
Figure 4\.4 shows the structure of an LR parser. The contents of the parse 
stack for an LR parser have the following form:
S
0X1S1X2 cXmSm (top)
where the S’s are state symbols and the X’s are grammar symbols. An LR parser 
configuration is a pair of strings (stack, input), with the detailed form
(S0X1S1X2S2 c XmSm, aiai\+1 c an$)
Figure 4\.4
The structure of an LR 
parserParse StackTop
Parser
CodeInput
ParsingTableS0X1S1 XmSm ai $ ai\+1 an
Notice that the input string has a dollar sign at its right end. This sign is put 
there during initialization of the parser. It is used for normal termination of the parser. Using this parser configuration, we can formally define the LR parser process, which is based on the parsing table. 4\.5 Bottom\-Up Parsing 195
An LR parsing table has two parts, named ACTION and GOTO. The 
ACTION part of the table specifies most of what the parser does. It has state symbols as its row labels and the terminal symbols of the grammar as its column labels. Given a current parser state, which is represented by the state symbol on top of the parse stack, and the next symbol (token) of input, the parse table specifies what the parser should do. The two primary parser actions are shift and reduce. Either the parser shifts the next input symbol onto the parse stack or it already has the handle on top of the stack, which it reduces to the LHS of the rule whose RHS is the same as the handle. T wo other actions are possible: accept, which means the parser has successfully completed the parse of the input, and error, which means the parser has detected a syntax error.
The rows of the GOTO part of the LR parsing table have state symbols 
as labels. This part of the table has nonterminals as column labels. The values in the GOTO part of the table indicate which state symbol should be pushed onto the parse stack after a reduction has been completed, which means the handle has been removed from the parse stack and the new nonterminal has been pushed onto the parse stack. The specific symbol is found at the row whose label is the state symbol on top of the parse stack after the handle and its associated state symbols have been removed. The column of the GOTO table that is used is the one with the label that is the LHS of the rule used in the reduction.
Consider the traditional grammar for arithmetic expressions that follows:
 1\. E → E 
\+ T
 2\. E → T
 3\. T → T \* F
 4\. T → F
 5\. F → (E)
 6\. F → id
The rules of this grammar are numbered to provide a simple way to reference 
them in a parsing table.
Figure 4\.5 shows the LR parsing table for this grammar. Abbreviations are 
used for the actions: R for reduce and S for shift. R4 means reduce using rule 4; S6 means shift the next symbol of input onto the stack and push state S6 onto the stack. Empty positions in the ACTION table indicate syntax errors. In a complete parser, these could have calls to error\-handling routines.
LR parsing tables can easily be constructed using a software tool, such as 
yacc ( Johnson, 1975\), which takes the grammar as input. Although LR parsing tables can be produced by hand, for a grammar of a real programming lan\-guage, the task would be lengthy, tedious, and error prone. For real compilers, LR parsing tables are always generated with software tools.
The initial configuration of an LR parser is
(S
0, a1 c an$)196 Chapter 4 Lexical and Syntax Analysis
The parser actions are informally defined as follows:
 1\. The Shift process is simple: The next symbol of input is pushed onto the 
stack, along with the state symbol that is part of the Shift specification in the ACTION table.
 2\. For a Reduce action, the handle must be removed from the stack. 
Because for every grammar symbol on the stack there is a state symbol, the number of symbols removed from the stack is twice the number of symbols in the handle. After removing the handle and its associated state symbols, the LHS of the rule is pushed onto the stack. Finally, the GOTO table is used, with the row label being the symbol that was exposed when the handle and its state symbols were removed from the stack, and the column label being the nonterminal that is the LHS of the rule used in the reduction.
 3\. When the action is Accept, the parse is complete and no errors were 
found.
 4\. When the action is Error, the parser calls an error\-handling routine.
Although there are many parsing algorithms based on the LR concept, they 
differ only in the construction of the parsing table. All LR parsers use this same parsing algorithm.
Perhaps the best way to become familiar with the LR parsing process is 
through an example. Initially, the parse stack has the single symbol 0, which Figure 4\.5
The LR parsing table 
for an arithmetic expression grammarAction Goto
id \+ \*
S5
S5
S5
S5S4S4
S4
S40
123456789
1011S6
S7 R2
R4 R4State ()$ E T F
R6 R6
S6
R1
R3
R5R2
R4
R6
S11
R1
R3
R5R4
R6
R1
R3
R5R3
R5accept
R2
S712 3
2 3
38
9
10 Summary 197
represents state 0 of the parser. The input contains the input string with an 
end marker, in this case a dollar sign, attached to its right end. At each step, the parser actions are dictated by the top (rightmost in Figure 4\.4\) symbol of the parse stack and the next (leftmost in Figure 4\.4\) token of input. The cor\-rect action is chosen from the corresponding cell of the ACTION part of the parse table. The GOTO part of the parse table is used after a reduction action. Recall that GOTO is used to determine which state symbol is placed on the parse stack after a reduction.
Following is a trace of a parse of the string id 
\+ id \* id, using the LR pars\-
ing algorithm and the parsing table shown in Figure 4\.5\.
The algorithms to generate LR parsing tables from given grammars, which 
are described in Aho et al. (2006\), are not overly complex but are beyond the scope of a book on programming languages. As stated previously, there are a number of different software systems available to generate LR pars\-ing tables.
SUMMARY
Syntax analysis is a common part of language implementation, regardless of the implementation approach used. Syntax analysis is normally based on a formal syntax description of the language being implemented. A context\-free gram\-mar, which is also called BNF , is the most common approach for describing syntax. The task of syntax analysis is usually divided into two parts: lexical analysis and syntax analysis. There are several reasons for separating lexical analysis—namely, simplicity, efficiency, and portability.Stack Input Action
0 id \+ id \* id $ Shift 5
0id5 \+ id \* id $ Reduce 6 (use GOTO\[0, F])0F3 \+ id \* id $ Reduce 4 (use GOTO\[0, T])0T2 \+ id \* id $ Reduce 2 (use GOTO\[0, E])0E1 \+ id \* id $ Shift 60E1\+6 id \* id $ Shift 50E1\+6id5 \* id $ Reduce 6 (use GOTO\[6, F])0E1\+6F3 \* id $ Reduce 4 (use GOTO\[6, T])0E1\+6T9 \* id $ Shift 70E1\+6T9\*7 id $ Shift 50E1\+6T9\*7id5 $ Reduce 6 (use GOTO\[7, F])0E1\+6T9\*7F10 $ Reduce 3 (use GOTO\[6, T])0E1\+6T9 $ Reduce 1 (use GOTO\[0, E])0E1 $ Accept198 Chapter 4 Lexical and Syntax Analysis
A lexical analyzer is a pattern matcher that isolates the small\-scale parts 
of a program, which are called lexemes. Lexemes occur in categories, such as integer literals and names. These categories are called tokens. Each token is assigned a numeric code, which along with the lexeme is what the lexical ana\-lyzer produces. There are three distinct approaches to constructing a lexical analyzer: using a software tool to generate a table for a table\-driven analyzer, building such a table by hand, and writing code to implement a state diagram description of the tokens of the language being implemented. The state dia\-gram for tokens can be reasonably small if character classes are used for transi\-tions, rather than having transitions for every possible character from every state node. Also, the state diagram can be simplified by using a table lookup to recognize reserved words.
Syntax analyzers have two goals: to detect syntax errors in a given program 
and to produce a parse tree, or possibly only the information required to build such a tree, for a given program. Syntax analyzers are either top\-down, mean\-ing they construct leftmost derivations and a parse tree in top\-down order, or bottom\-up, in which case they construct the reverse of a rightmost derivation and a parse tree in bottom\-up order. Parsers that work for all unambiguous grammars have complexity O(n
3\). However, parsers used for implementing 
syntax analyzers for programming languages work on subclasses of unambigu\-ous grammars and have complexity O(n).
A recursive\-descent parser is an LL parser that is implemented by writing 
code directly from the grammar of the source language. EBNF is ideal as the basis for recursive\-descent parsers. A recursive\-descent parser has a subpro\-gram for each nonterminal in the grammar. The code for a given grammar rule is simple if the rule has a single RHS. The RHS is examined left to right. For each nonterminal, the code calls the associated subprogram for that non\-terminal, which parses whatever the nonterminal generates. For each terminal, 
the code compares the terminal with the next token of input. If they match, the code simply calls the lexical analyzer to get the next token. If they do not, the subprogram reports a syntax error. If a rule has more than one RHS, the sub\-program must first determine which RHS it should parse. It must be possible to make this determination on the basis of the next token of input.
T wo distinct grammar characteristics prevent the construction of a 
 recursive\-descent parser based on the grammar. One of these is left recursion. 
The process of eliminating direct left recursion from a grammar is relatively simple. Although we do not cover it, an algorithm exists to remove both direct and indirect left recursion from a grammar. The other problem is detected with the pairwise disjointness test, which tests whether a parsing subprogram can determine which RHS is being parsed on the basis of the next token of input. Some grammars that fail the pairwise disjointness test often can be modified to pass it, using left factoring.
The parsing problem for bottom\-up parsers is to find the substring of the 
current sentential form that must be reduced to its associated LHS to get the next (previous) sentential form in the rightmost derivation. This substring is called the handle of the sentential form. A parse tree can provide an intuitive Review Questions 199
basis for recognizing a handle. A bottom\-up parser is a shift\-reduce algorithm, 
because in most cases it either shifts the next lexeme of input onto the parse stack or reduces the handle that is on top of the stack.
The LR family of shift\-reduce parsers is the most commonly used bottom\-
up parsing approach for programming languages, because parsers in this fam\-ily have several advantages over alternatives. An LR parser uses a parse stack, which contains grammar symbols and state symbols to maintain the state of the parser. The top symbol on the parse stack is always a state symbol that represents all of the information in the parse stack that is relevant to the pars\-ing process. LR parsers use two parsing tables: ACTION and GOTO. The ACTION part specifies what the parser should do, given the state symbol on top of the parse stack and the next token of input. The GOTO table is used to determine which state symbol should be placed on the parse stack after a reduction has been done.
REVIEW QUESTIONS
 1\. What are three reasons why syntax analyzers are based on grammars?
 2\. Explain the three reasons why lexical analysis is separated from syntax 
analysis.
 3\. Define lexeme and token .
 4\. What are the primary tasks of a lexical analyzer? 5\. Describe briefly the three approaches to building a lexical analyzer. 6\. What is a state transition diagram? 7\. Why are character classes used, rather than individual characters, for the 
letter and digit transitions of a state diagram for a lexical analyzer?
 8\. What are the two distinct goals of syntax analysis? 9\. Describe the differences between top\-down and bottom\-up parsers. 10\. Describe the parsing problem for a top\-down parser. 11\. Describe the parsing problem for a bottom\-up parser. 12\. Explain why compilers use parsing algorithms that work on only a subset 
of all grammars.
 13\. Why are named constants used, rather than numbers, for token codes? 14\. Describe how a recursive\-descent parsing subprogram is written for a 
rule with a single RHS.
 15\. Explain the two grammar characteristics that prohibit them from being 
used as the basis for a top\-down parser.
 16\. What is the FIRST set for a given grammar and sentential form? 17\. Describe the pairwise disjointness test. 18\. What is left factoring?200 Chapter 4 Lexical and Syntax Analysis
 19\. What is a phrase of a sentential form?
 20\. What is a simple phrase of a sentential form? 21\. What is the handle of a sentential form? 22\. What is the mathematical machine on which both top\-down and 
bottom\-up parsers are based?
 23\. Describe three advantages of LR parsers. 24\. What was Knuth’s insight in developing the LR parsing technique? 25\. Describe the purpose of the ACTION table of an LR parser. 26\. Describe the purpose of the GOTO table of an LR parser. 27\. Is left recursion a problem for LR parsers?
PROBLEM SET
 1\. Perform the pairwise disjointness test for the following grammar rules.
 a. A→aB /H20841 b /H20841 cBB
 b. B→aB /H20841 bA /H20841 aBb
 c. C→aaA /H20841 b /H20841 caB
 2\. Perform the pairwise disjointness test for the following grammar rules.
 a. S→aSb /H20841 bAA
 b. A→b{aB} /H20841 a
 c. B→aB /H20841 a
 3\. Show a trace of the recursive descent parser given in Section 4\.4\.1 for 
the string a \+ b \* c .
 4\. Show a trace of the recursive descent parser given in Section 4\.4\.1 for 
the string a \* (b \+ c) .
 5\. Given the following grammar and the right sentential form, draw a parse 
tree and show the phrases and simple phrases, as well as the handle.
S→aAb /H20841 bBA A →ab /H20841 aAB B →aB /H20841 b
 a. aaAbb
 b. bBab
 c. aaAbBb
 6\. Given the following grammar and the right sentential form, draw a parse 
tree and show the phrases and simple phrases, as well as the handle.
S→AbB /H20841 bAc A →Ab /H20841 aBB B →Ac /H20841 cBb /H20841 c
 a. aAcccbbc
 b. AbcaBccb
 c. baBcBbbc Programming Exercises 201
 7\. Show a complete parse, including the parse stack contents, input string, 
and action for the string id \* (id \+ id), using the grammar and parse 
table in Section 4\.5\.3\.
 8\. Show a complete parse, including the parse stack contents, input string, 
and action for the string (id \+ id) \* id, using the grammar and parse 
table in Section 4\.5\.3\.
 9\. Write an EBNF rule that describes the while statement of Java or C\+\+. 
Write the recursive\-descent subprogram in Java or C\+\+ for this rule.
 10\. Write an EBNF rule that describes the for statement of Java or C\+\+. 
Write the recursive\-descent subprogram in Java or C\+\+ for this rule.
 11\. Get the algorithm to remove the indirect left recursion from a 
grammar from Aho et al. (2006\). Use this algorithm to remove all left recursion from the following grammar: 
S→Aa /H20841 Bb A →Aa /H20841 Abc /H20841 c /H20841 Sb B →bb
PROGRAMMING EXERCISES
 1\. Design a state diagram to recognize one form of the comments of the 
C\-based programming languages, those that begin with /\* and end with \*/.
 2\. Design a state diagram to recognize the floating\-point literals of your 
favorite programming language.
 3\. Write and test the code to implement the state diagram of Problem 1\. 4\. Write and test the code to implement the state diagram of Problem 2\. 5\. Modify the lexical analyzer given in Section 4\.2 to recognize the follow\-
ing list of reserved words and return their respective token codes: 
for (FOR\_CODE , 30\), if (IF\_CODE , 31\), else (ELSE\_CODE , 32\), while 
(WHILE\_CODE , 33\), do (DO\_CODE , 34\), int (INT\_CODE , 35\), float 
(FLOAT\_CODE , 36\), switch (SWITCH\_CODE , 37\).
 6\. Convert the lexical analyzer (which is written in C) given in Section 4\.2 
to Java.
 7\. Convert the recursive descent parser routines for , , and 
 given in Section 4\.4\.1 to Java.
 8\. For those rules that pass the test in Problem 1, write a recursive\-descent 
parsing subprogram that parses the language generated by the rules. Assume you have a lexical analyzer named 
lex and an error\-handling sub\-
program named error , which is called whenever a syntax error is detected.
 9\. For those rules that pass the test in Problem 2, write a recursive\-descent 
parsing subprogram that parses the language generated by the rules. Assume you have a lexical analyzer named 
lex and an error\-handling sub\-
program named error , which is called whenever a syntax error is detected.
 10\. Implement and test the LR parsing algorithm given in Section 4\.5\.3\.This page intentionally left blank 203 5\.1 Introduction
 5\.2 Names
 5\.3 Variables
 5\.4 The Concept of Binding
 5\.5 Scope
 5\.6 Scope and Lifetime
 5\.7 Referencing Environments
 5\.8 Named Constants5
Names, Bindings, 
and Scopes204 Chapter 5 Names, Bindings, and Scopes 
This chapter introduces the fundamental semantic issues of variables. It 
begins by describing the nature of names and special words in program\-ming languages. The attributes of variables, including type, a ddress, and 
value, are then discussed, including the issue of aliases. The important concepts 
of binding and binding times are introduced next, including the different possible 
binding times for variable attributes and how they define four different categories of variables. Following that, two very different scoping rules for names, static and dynamic, are described, along with the concept of a referencing environment of a statement. Finally, named constants and variable initialization are discussed.
5\.1 Introduction
Imperative programming languages are, to varying degrees, abstractions of 
the underlying von Neumann computer architecture. The architecture’s two primary components are its memory, which stores both instructions and data, and its processor, which provides operations for modifying the contents of the memory. The abstractions in a language for the memory cells of the machine are variables. In some cases, the characteristics of the abstractions are very close to the characteristics of the cells; an example of this is an integer variable, which is usually represented directly in one or more bytes of memory. In other cases, the abstractions are far removed from the organization of the hardware memory, as with a three\-dimensional array, which requires a software mapping function to support the abstraction.
A variable can be characterized by a collection of properties, or attributes, 
the most important of which is type, a fundamental concept in programming languages. Designing the data types of a language requires that a variety of issues be considered. (Data types are discussed in Chapter 6\.) Among the most important of these issues are the scope and lifetime of variables.
Functional programming languages allow expressions to be named. These 
named expressions appear like assignments to variable names in imperative languages, but are fundamentally different in that they cannot be changed. So, they are like the named constants of the imperative languages. Pure functional languages do not have variables that are like those of the imperative languages. However, many functional languages do include such variables.
In the remainder of this book, families of languages will often be referred to 
as if they were single languages. For example, Fortran will mean all of the versions of Fortran. This is also the case for Ada. Likewise, a reference to C will mean the original version of C, as well as C89 and C99\. When a specific version of a language is named, it is because it is different from the other family members within the topic being discussed. If we add a plus sign (\+) to the name of a version of a language, we mean all versions of the language beginning with the one named. For example, Fortran 95\+ means all versions of Fortran beginning with Fortran 95\. The phrase C\-based languages will be used to refer to C, Objective\-C, C\+\+, Java, and C\#.
1
 1\. We were tempted to include the scripting languages JavaScript and PHP as C\-based lan\-
guages, but decided they were just a bit too different from their ancestors. 5\.2 Names 205
5\.2 Names
Before beginning our discussion of variables, the design of one of the funda\-
mental attributes of variables, names, must be covered. Names are also associ\-ated with subprograms, formal parameters, and other program constructs. The term identifier is often used interchangeably with name .
5\.2\.1 Design Issues
The following are the primary design issues for names:
• Are names case sensitive?
• Are the special words of the language reserved words or keywords?
These issues are discussed in the following two subsections, which also include 
examples of several design choices.
5\.2\.2 Name Forms
A name is a string of characters used to identify some entity in a program.
Fortran 95\+ allows up to 31 characters in its names. C99 has no length 
limitation on its internal names, but only the first 63 are significant. External names in C99 (those defined outside functions, which must be handled by the 
linker) are restricted to 31 characters. Names in Java, C\#, and Ada have no length limit, and all characters in them are significant. C\+\+ does not specify a length limit on names, although imple\-mentors sometimes do.
Names in most programming languages have the same form: 
a letter followed by a string consisting of letters, digits, and underscore characters ( 
\_ ). Although the use of underscore char\-
acters to form names was widely used in the 1970s and 1980s, that practice is now far less popular. In the C\-based languages, it has to a large extent been replaced by the so\-called camel notation , in 
which all of the words of a multiple\-word name except the first are capitalized, as in 
myStack .2 Note that the use of underscores 
and mixed case in names is a programming style issue, not a lan\-guage design issue.
All variable names in PHP must begin with a dollar sign. In 
Perl, the special character at the beginning of a variable’s name, 
$, @, or %, specifies its type (although in a different sense than in 
other languages). In Ruby, special characters at the beginning of 
a variable’s name, @ or @@, indicate that the variable is an instance or a class 
variable, respectively.
 2\. It is called “camel” because words written in it often have embedded uppercase letters, which 
look like a camel’s humps.history note
The earliest programming lan\-
guages used single\-character 
names. This notation was natu\-
ral because early programming 
was primarily mathematical, and mathematicians have long used single\-character names 
for unknowns in their formal 
notations.
Fortran I broke with the 
tradition of the single\-character 
name, allowing up to six charac\-
ters in its names.206 Chapter 5 Names, Bindings, and Scopes 
In many languages, notably the C\-based languages, uppercase and lowercase 
letters in names are distinct; that is, names in these languages are case sensitive . 
For example, the following three names are distinct in C\+\+: rose , ROSE , and 
Rose . T o some people, this is a serious detriment to readability, because names 
that look very similar in fact denote different entities. In that sense, case sensitiv\-ity violates the design principle that language constructs that look similar should have similar meanings. But in languages whose variable names are case\-sensitive, although 
Rose and rose look similar, there is no connection between them.
Obviously, not everyone agrees that case sensitivity is bad for names. In 
C, the problems of case sensitivity are avoided by the convention that variable names do not include uppercase letters. In Java and C\#, however, the prob\-lem cannot be escaped because many of the predefined names include both uppercase and lowercase letters. For example, the Java method for converting a string to an integer value is 
parseInt , and spellings such as ParseInt and 
parseint are not recognized. This is a problem of writability rather than 
readability, because the need to remember specific case usage makes it more difficult to write correct programs. It is a kind of intolerance on the part of the language designer, which is enforced by the compiler.
5\.2\.3 Special Words
Special words in programming languages are used to make programs more readable by naming actions to be performed. They also are used to separate the syntactic parts of statements and programs. In most languages, special words are classified as reserved words, which means they cannot be redefined by program\-mers, but in some they are only keywords, which means they can be redefined.
A keyword is a word of a programming language that is special only in 
certain contexts. Fortran is the only remaining widely used language whose special words are keywords. In Fortran, the word 
Integer , when found at 
the beginning of a statement and followed by a name, is considered a keyword that indicates the statement is a declarative statement. However, if the word 
Integer is followed by the assignment operator, it is considered a variable 
name. These two uses are illustrated in the following:
Integer Apple
Integer \= 4
Fortran compilers and people reading Fortran programs must distinguish 
between names and special words by context.
A reserved word is a special word of a programming language that can\-
not be used as a name. As a language design choice, reserved words are better than keywords because the ability to redefine keywords can be confusing. For example, in Fortran, one could have the following statements:
Integer Real
Real Integer 5\.3 Variables 207
These statements declare the program variable Real to be of Integer type 
and the variable Integer to be of Real type.3 In addition to the strange 
appearance of these declaration statements, the appearance of Real and Inte\-
ger as variable names elsewhere in the program could be misleading to pro\-
gram readers.
There is one potential problem with reserved words: If the language 
includes a large number of reserved words, the user may have difficulty mak\-ing up names that are not reserved. The best example of this is COBOL, which has 300 reserved words. Unfortunately, some of the most commonly chosen names by programmers are in the list of reserved words—for example, 
LENGTH , 
BOTTOM , DESTINATION , and COUNT .
In program code examples in this book, reserved words are presented in 
boldface.
In most languages, names that are defined in other program units, such as 
Java packages and C and C\+\+ libraries, can be made visible to a program. These names are predefined, but visible only if explicitly imported. Once imported, they cannot be redefined.
5\.3 Variables
A program variable is an abstraction of a computer memory cell or collection 
of cells. Programmers often think of variable names as names for memory loca\-tions, but there is much more to a variable than just a name.
The move from machine languages to assembly languages was largely one 
of replacing absolute numeric memory addresses for data with names, making programs far more readable and therefore easier to write and maintain. That step also provided an escape from the problem of manual absolute addressing, because the translator that converted the names to actual addresses also chose those addresses.
A variable can be characterized as a sextuple of attributes: (name, address, 
value, type, lifetime, and scope). Although this may seem too complicated for such an apparently simple concept, it provides the clearest way to explain the various aspects of variables.
Our discussion of variable attributes will lead to examinations of the impor\-
tant related concepts of aliases, binding, binding times, declarations, scoping rules, and referencing environments.
The name, address, type, and value attributes of variables are discussed in 
the following subsections. The lifetime and scope attributes are discussed in Sections 5\.4\.3 and 5\.5, respectively.
 3\. Of course, any professional programmer who would write such code should not expect job 
security.208 Chapter 5 Names, Bindings, and Scopes 
5\.3\.1 Name
Variable names are the most common names in programs. They were dis\-
cussed at length in Section 5\.2 in the general context of entity names in programs. Most variables have names. The ones that do not are discussed in Section 5\.4\.3\.3\.
5\.3\.2 Address
The address of a variable is the machine memory address with which it is 
associated. This association is not as simple as it may at first appear. In many languages, it is possible for the same variable to be associated with different addresses at different times in the program. For example, if a subprogram has a local variable that is allocated from the run\-time stack when the subprogram is called, different calls may result in that variable having different addresses. These are in a sense different instantiations of the same variable.
The process of associating variables with addresses is further discussed in 
Section 5\.4\.3\. An implementation model for subprograms and their activations is discussed in Chapter 10\.
The address of a variable is sometimes called its l\-value , because the 
address is what is required when the name of a variable appears in the left side of an assignment.
It is possible to have multiple variables that have the same address. When 
more than one variable name can be used to access the same memory location, the variables are called aliases . Aliasing is a hindrance to readability because it 
allows a variable to have its value changed by an assignment to a different vari\-able. For example, if variables named 
total and sum are aliases, any change 
to the value of total also changes the value of sum and vice versa. A reader of 
the program must always remember that total and sum are different names 
for the same memory cell. Because there can be any number of aliases in a program, this may be very difficult in practice. Aliasing also makes program verification more difficult.
Aliases can be created in programs in several different ways. One common 
way in C and C\+\+ is with their union types. Unions are discussed at length in Chapter 6\.
T wo pointer variables are aliases when they point to the same memory 
location. The same is true for reference variables. This kind of aliasing is simply a side effect of the nature of pointers and references. When a C\+\+ pointer is set to point at a named variable, the pointer, when dereferenced, and the variable’s name are aliases.
Aliasing can be created in many languages through subprogram param\-
eters. These kinds of aliases are discussed in Chapter 9\.
The time when a variable becomes associated with an address is very 
important to an understanding of programming languages. This subject is dis\-cussed in Section 5\.4\.3\. 5\.4 The Concept of Binding 209
5\.3\.3 Type
The type of a variable determines the range of values the variable can store 
and the set of operations that are defined for values of the type. For example, the 
int type in Java specifies a value range of \-2147483648 to 2147483647 
and arithmetic operations for addition, subtraction, multiplication, division, and modulus.
5\.3\.4 Value
The value of a variable is the contents of the memory cell or cells associ\-
ated with the variable. It is convenient to think of computer memory in terms of abstract cells, rather than physical cells. The physical cells, or individually 
addressable units, of most contemporary computer memories are byte\-size, with a byte usually being eight bits in length. This size is too small for most program variables. An abstract memory cell has the size required by the vari\-able with which it is associated. For example, although floating\-point values may occupy four physical bytes in a particular implementation of a particular language, a floating\-point value is thought of as occupying a single abstract memory cell. The value of each simple nonstructured type is considered to occupy a single abstract cell. Henceforth, the term memory cell means abstract 
memory cell.
A variable’s value is sometimes called its r\-value because it is what is 
required when the name of the variable appears in the right side of an assign\-ment statement. T o access the r\-value, the l\-value must be determined first. 
Such determinations are not always simple. For example, scoping rules can greatly complicate matters, as is discussed in Section 5\.5\.
5\.4 The Concept of Binding
A binding is an association between an attribute and an entity, such as 
between a variable and its type or value, or between an operation and a sym\-bol. The time at which a binding takes place is called binding time . Binding 
and binding times are prominent concepts in the semantics of programming languages. Bindings can take place at language design time, language imple\-mentation time, compile time, load time, link time, or run time. For example, the asterisk symbol (
\*) is usually bound to the multiplication operation at 
language design time. A data type, such as int in C, is bound to a range of 
possible values at language implementation time. At compile time, a variable in a Java program is bound to a particular data type. A variable may be bound to a storage cell when the program is loaded into memory. That same bind\-ing does not happen until run time in some cases, as with variables declared in Java methods. A call to a library subprogram is bound to the subprogram code at link time.210 Chapter 5 Names, Bindings, and Scopes 
Consider the following Java assignment statement:
count \= count \+ 5;
Some of the bindings and their binding times for the parts of this assignment 
statement are as follows:
• The type of count is bound at compile time.
• The set of possible values of count is bound at compiler design time.
• The meaning of the operator symbol \+ is bound at compile time, when the 
types of its operands have been determined.
• The internal representation of the literal 5 is bound at compiler design 
time.
• The value of count is bound at execution time with this statement.
A complete understanding of the binding times for the attributes of program 
entities is a prerequisite for understanding the semantics of a programming lan\-guage. For example, to understand what a subprogram does, one must under\-stand how the actual parameters in a call are bound to the formal parameters in its definition. T o determine the current value of a variable, it may be necessary to know when the variable was bound to storage and with which statement or statements.
5\.4\.1 Binding of Attributes to Variables
A binding is static if it first occurs before run time begins and remains 
unchanged throughout program execution. If the binding first occurs dur\-ing run time or can change in the course of program execution, it is called dynamic . The physical binding of a variable to a storage cell in a virtual 
memory environment is complex, because the page or segment of the address space in which the cell resides may be moved in and out of memory many times during program execution. In a sense, such variables are bound and unbound repeatedly. These bindings, however, are maintained by computer hardware, and the changes are invisible to the program and the user. Because they are not important to the discussion, we are not concerned with these hardware bindings. The essential point is to distinguish between static and dynamic bindings.
5\.4\.2 Type Bindings
Before a variable can be referenced in a program, it must be bound to a data type. The two important aspects of this binding are how the type is specified and when the binding takes place. T ypes can be specified statically through some form of explicit or implicit declaration. 5\.4 The Concept of Binding 211
5\.4\.2\.1 Static Type Binding
An explicit declaration is a statement in a program that lists variable names 
and specifies that they are a particular type. An implicit declaration is a means 
of associating variables with types through default conventions, rather than declaration statements. In this case, the first appearance of a variable name in a program constitutes its implicit declaration. Both explicit and implicit declara\-tions create static bindings to types.
Most widely used programming languages that use static type binding 
exclusively and were designed since the mid\-1960s require explicit declarations of all variables (Perl, JavaScript, Ruby, and ML are some exceptions).
Implicit variable type binding is done by the language processor, either 
a compiler or an interpreter. There are several different bases for implicit variable type bindings. The simplest of these is naming conventions. In this case, the compiler or interpreter binds a variable to a type based on the syntactic form of the variable’s name. For example, in Fortran, an identi\-fier that appears in a program that is not explicitly declared is implicitly declared according to the following convention: If the identifier begins with one of the letters 
I, J, K, L, M, or N, or their lowercase versions, it is 
implicitly declared to be Integer type; otherwise, it is implicitly declared 
to be Real type.
Although they are a minor convenience to programmers, implicit dec\-
larations can be detrimental to reliability because they prevent the compila\-tion process from detecting some typographical and programmer errors. In Fortran, variables that are accidentally left undeclared by the programmer are given default types and possibly unexpected attributes, which could cause subtle errors that are difficult to diagnose. Many Fortran programmers now include the declaration 
Implicit none in their programs. This declaration instructs 
the compiler to not implicitly declare any variables, thereby avoiding the poten\-tial problems of accidentally undeclared variables.
Some of the problems with implicit declarations can be avoided by requir\-
ing names for specific types to begin with particular special characters. For example, in Perl any name that begins with 
$ is a scalar, which can store either 
a string or a numeric value. If a name begins with @, it is an array; if it begins 
with a %, it is a hash structure.4 This creates different namespaces for different 
type variables. In this scenario, the names @apple and %apple are unrelated, 
because each is from a different namespace. Furthermore, a program reader always knows the type of a variable when reading its name. Note that this design is different from Fortran, because Fortran has both implicit and explicit declara\-tions, so the type of a variable cannot necessarily be determined from the spell\-ing of its name.
Another kind of implicit type declarations uses context. This is sometimes 
called type inference . In the simpler case, the context is the type of the value 
assigned to the variable in a declaration statement. For example, in C\# a 
var 
 4\. Both arrays and hashes are considered types—both can store any scalar in their elements.212 Chapter 5 Names, Bindings, and Scopes 
declaration of a variable must include an initial value, whose type is made the 
type of the variable. Consider the following declarations:
var sum \= 0;
var total \= 0\.0;
var name \= "Fred";
The types of sum, total , and name are int, float , and string , respectively. 
Keep in mind that these are statically typed variables—their types are fixed for the lifetime of the unit in which they are declared.
Visual BASIC 9\.0\+, Go, and the functional languages ML, Haskell, OCaml, 
and F\# also use type inferencing. In these functional languages, the context of the appearance of a name is the basis for determining its type. This kind of type inferencing is discussed in detail in Chapter 15\.
5\.4\.2\.2 Dynamic Type Binding
With dynamic type binding, the type of a variable is not specified by a declara\-tion statement, nor can it be determined by the spelling of its name. Instead, the variable is bound to a type when it is assigned a value in an assignment state\-ment. When the assignment statement is executed, the variable being assigned is bound to the type of the value of the expression on the right side of the assignment. Such an assignment may also bind the variable to an address and a memory cell, because different type values may require different amounts of storage. Any variable can be assigned any type value. Furthermore, a variable’s type can change any number of times during program execution. It is important to realize that the type of a variable whose type is dynamically bound may be temporary.
When the type of a variable is statically bound, the name of the variable can 
be thought of being bound to a type, in the sense that the type and name of a variable are simultaneously bound. However, when a variable’s type is dynami\-cally bound, its name can be thought of as being only temporarily bound to a type. In reality, the names of variables are never bound to types. Names can be bound to variables and variables can be bound to types.
Languages in which types are dynamically bound are dramatically differ\-
ent from those in which types are statically bound. The primary advantage of dynamic binding of variables to types is that it provides more programming flexibility. For example, a program to process numeric data in a language that uses dynamic type binding can be written as a generic program, meaning that it is capable of dealing with data of any numeric type. Whatever type data is input will be acceptable, because the variables in which the data are to be stored can be bound to the correct type when the data is assigned to the variables after input. By contrast, because of static binding of types, one cannot write a C program to process data without knowing the type of that data.
Before the mid\-1990s, the most commonly used programming lan\-
guages used static type binding, the primary exceptions being some functional 5\.4 The Concept of Binding 213
languages such as LISP . However, since then there has been a significant shift 
to languages that use dynamic type binding. In Python, Ruby, JavaScript, and PHP , type binding is dynamic. For example, a JavaScript script may contain the following statement:
list \= \[10\.2, 3\.5];
Regardless of the previous type of the variable named list , this assignment 
causes it to become the name of a single\-dimensioned array of length 2\. If the statement
list \= 47;
followed the previous example assignment, list would become the name of 
a scalar variable.
The option of dynamic type binding was introduced in C\# 2010\. A variable 
can be declared to use dynamic type binding by including the dynamic reserved 
word in its declaration, as in the following example:
dynamic any;
This is similar, although also different from declaring any to have type 
object . It is similar in that any can be assigned a value of any type, just as 
if it were declared object . It is different in that it is not useful for several 
different situations of interoperation; for example, with dynamically typed languages such as IronPython and IronRuby (.NET versions of Python and Ruby, respectively). However, it is useful when data of unknown type come into a program from an external source. Class members, properties, method parameters, method return values, and local variables can all be declared 
dynamic .
In pure object\-oriented languages—for example, Ruby—all variables are 
references and do not have types; all data are objects and any variable can reference any object. Variables in such languages are, in a sense, all the same type—they are references. However, unlike the references in Java, which are restricted to referencing one specific type of value, variables in Ruby can refer\-ence any object.
There are two disadvantages to dynamic type binding. First, it causes 
programs to be less reliable, because the error\-detection capability of the compiler is diminished relative to a compiler for a language with static type bindings. Dynamic type binding allows any variable to be assigned a value of any type. Incorrect types of right sides of assignments are not detected as errors; rather, the type of the left side is simply changed to the incorrect type. For example, suppose that in a particular JavaScript program, 
i and 
x are currently the names of scalar numeric variables and y is currently the 
name of an array. Furthermore, suppose that the program needs the assign\-ment statement214 Chapter 5 Names, Bindings, and Scopes 
i \= x;
but because of a keying error, it has the assignment statement 
i \= y;
In JavaScript (or any other language that uses dynamic type binding), no error 
is detected in this statement by the interpreter—the type of the variable named 
i is simply changed to an array. But later uses of i will expect it to be a scalar, 
and correct results will be impossible. In a language with static type binding, such as Java, the compiler would detect the error in the assignment 
i \= y , and 
the program would not get to execution.
Note that this disadvantage is also present to some extent in some languages 
that use static type binding, such as Fortran, C, and C\+\+, which in many cases auto\-matically convert the type of the RHS of an assignment to the type of the LHS.
Perhaps the greatest disadvantage of dynamic type binding is cost. The 
cost of implementing dynamic attribute binding is considerable, particularly in execution time. T ype checking must be done at run time. Furthermore, every variable must have a run\-time descriptor associated with it to maintain the cur\-rent type. The storage used for the value of a variable must be of varying size, because different type values require different amounts of storage.
Finally, languages that have dynamic type binding for variables are usually 
implemented using pure interpreters rather than compilers. Computers do not have instructions whose operand types are not known at compile time. There\-fore, a compiler cannot build machine instructions for the expression A \+ B if the types of A and B are not known at compile time. Pure interpretation typically takes at least 10 times as long as it does to execute equivalent machine code. Of course, if a language is implemented with a pure interpreter, the time to do dynamic type binding is hidden by the overall time of interpretation, so it seems less costly in that environment. On the other hand, languages with static type bindings are seldom implemented by pure interpretation, because programs in these languages can be easily translated to very efficient machine code versions.
5\.4\.3 Storage Bindings and Lifetime
The fundamental character of an imperative programming language is in large part determined by the design of the storage bindings for its variables. It is therefore important to have a clear understanding of these bindings.
The memory cell to which a variable is bound somehow must be taken from 
a pool of available memory. This process is called allocation . Deallocation is 
the process of placing a memory cell that has been unbound from a variable back into the pool of available memory.
The lifetime of a variable is the time during which the variable is bound 
to a specific memory location. So, the lifetime of a variable begins when it is bound to a specific cell and ends when it is unbound from that cell. T o investigate storage bindings of variables, it is convenient to separate scalar 5\.4 The Concept of Binding 215
(unstructured) variables into four categories, according to their lifetimes. These 
categories are named static, stack\-dynamic, explicit heap\-dynamic, and implicit heap\-dynamic. In the following sections, we discuss the definitions of these four categories, along with their purposes, advantages, and disadvantages.
5\.4\.3\.1 Static Variables
Static variables are those that are bound to memory cells before program execu\-
tion begins and remain bound to those same memory cells until program execu\-tion terminates. Statically bound variables have several valuable applications in programming. Globally accessible variables are often used throughout the execu\-tion of a program, thus making it necessary to have them bound to the same storage during that execution. Sometimes it is convenient to have subprograms that are history sensitive. Such a subprogram must have local static variables.
One advantage of static variables is efficiency. All addressing of static vari\-
ables can be direct;
5 other kinds of variables often require indirect addressing, 
which is slower. Also, no run\-time overhead is incurred for allocation and deal\-location of static variables, although this time is often negligible.
One disadvantage of static binding to storage is reduced flexibility; in 
particular, a language that has only static variables cannot support recursive subprograms. Another disadvantage is that storage cannot be shared among variables. For example, suppose a program has two subprograms, both of which require large arrays. Furthermore, suppose that the two subprograms are never active at the same time. If the arrays are static, they cannot share the same stor\-age for their arrays.
C and C\+\+ allow programmers to include the 
static specifier on a vari\-
able definition in a function, making the variables it defines static. Note that when the 
static modifier appears in the declaration of a variable in a class 
definition in C\+\+, Java, and C\#, it also implies that the variable is a class vari\-able, rather than an instance variable. Class variables are created statically some time before the class is first instantiated.
5\.4\.3\.2 Stack\-Dynamic Variables
Stack\-dynamic variables are those whose storage bindings are created when 
their declaration statements are elaborated, but whose types are statically bound. Elaboration of such a declaration refers to the storage allocation and 
binding process indicated by the declaration, which takes place when execution reaches the code to which the declaration is attached. Therefore, elaboration occurs during run time. For example, the variable declarations that appear at the beginning of a Java method are elaborated when the method is called and the variables defined by those declarations are deallocated when the method completes its execution.
 5\. In some implementations, static variables are addressed through a base register, making 
accesses to them as costly as for stack\-allocated variables.216 Chapter 5 Names, Bindings, and Scopes 
As their name indicates, stack\-dynamic variables are allocated from the 
run\-time stack.
Some languages—for example, C\+\+ and Java—allow variable declarations 
to occur anywhere a statement can appear. In some implementations of these languages, all of the stack\-dynamic variables declared in a function or method (not including those declared in nested blocks) may be bound to storage at the beginning of execution of the function or method, even though the declara\-tions of some of these variables do not appear at the beginning. In such cases, the variable becomes visible at the declaration, but the storage binding (and initialization, if it is specified in the declaration) occurs when the function or method begins execution. The fact that storage binding of a variable takes place before it becomes visible does not affect the semantics of the language.
The advantages of stack\-dynamic variables are as follows: T o be useful, at 
least in most cases, recursive subprograms require some form of dynamic local storage so that each active copy of the recursive subprogram has its own ver\-sion of the local variables. These needs are conveniently met by stack\-dynamic variables. Even in the absence of recursion, having stack\-dynamic local storage for subprograms is not without merit, because all subprograms share the same memory space for their locals.
The disadvantages, relative to static variables, of stack\-dynamic variables 
are the run\-time overhead of allocation and deallocation, possibly slower accesses because indirect addressing is required, and the fact that subprograms cannot be history sensitive. The time required to allocate and deallocate stack\-dynamic variables is not significant, because all of the stack\-dynamic variables that are declared at the beginning of a subprogram are allocated and deallocated together, rather than by separate operations.
Fortran 95\+ allows implementors to use stack\-dynamic variables for locals, 
but includes the following statement:
Save list
This declaration allows the programmer to specify that some or all of the vari\-
ables (those in the list) in the subprogram in which Save is placed will be static.
In Java, C\+\+, and C\#, variables defined in methods are by default stack 
dynamic. In Ada, all non\-heap variables defined in subprograms are stack dynamic.
All attributes other than storage are statically bound to stack\-dynamic 
scalar variables. That is not the case for some structured types, as is discussed in Chapter 6\. Implementation of allocation/deallocation processes for stack\-dynamic variables is discussed in Chapter 10\.
5\.4\.3\.3 Explicit Heap\-Dynamic Variables
Explicit heap\-dynamic variables are nameless (abstract) memory cells that are 
allocated and deallocated by explicit run\-time instructions written by the pro\-grammer. These variables, which are allocated from and deallocated to the heap, can only be referenced through pointer or reference variables. The heap is a col\-lection of storage cells whose organization is highly disorganized because of the 5\.4 The Concept of Binding 217
unpredictability of its use. The pointer or reference variable that is used to access 
an explicit heap\-dynamic variable is created as any other scalar variable. An explicit heap\-dynamic variable is created by either an operator (for example, in C\+\+) or a call to a system subprogram provided for that purpose (for example, in C).
In C\+\+, the allocation operator, named 
new, uses a type name as its 
operand. When executed, an explicit heap\-dynamic variable of the operand type is created and its address is returned. Because an explicit heap\-dynamic variable is bound to a type at compile time, that binding is static. However, such variables are bound to storage at the time they are created, which is during run time.
In addition to a subprogram or operator for creating explicit heap\-dynamic 
variables, some languages include a subprogram or operator for explicitly destroying them.
As an example of explicit heap\-dynamic variables, consider the following 
C\+\+ code segment:
int \*intnode; // Create a pointer
intnode \= new int; // Create the heap\-dynamic variable
. . .
delete intnode; // Deallocate the heap\-dynamic variable
 // to which intnode points
In this example, an explicit heap\-dynamic variable of int type is created by 
the new operator. This variable can then be referenced through the pointer, 
intnode . Later, the variable is deallocated by the delete operator. C\+\+ 
requires the explicit deallocation operator delete , because it does not use 
implicit storage reclamation, such as garbage collection.
In Java, all data except the primitive scalars are objects. Java objects are 
explicitly heap dynamic and are accessed through reference variables. Java has no way of explicitly destroying a heap\-dynamic variable; rather, implicit gar\-bage collection is used. Garbage collection is discussed in Chapter 6\.
C\# has both explicit heap\-dynamic and stack\-dynamic objects, all of which 
are implicitly deallocated. In addition, C\# supports C\+\+\-style pointers. Such pointers are used to reference heap, stack, and even static variables and objects. These pointers have the same dangers as those of C\+\+, and the objects they reference on the heap are not implicitly deallocated. Pointers are included in C\# to allow C\# components to interoperate with C and C\+\+ components. T o discourage their use, and also to make clear to any program reader that the code uses pointers, the header of any method that defines a pointer must include the reserved word 
unsafe .
Explicit heap\-dynamic variables are often used to construct dynamic struc\-
tures, such as linked lists and trees, that need to grow and/or shrink during execution. Such structures can be built conveniently using pointers or refer\-ences and explicit heap\-dynamic variables.
The disadvantages of explicit heap\-dynamic variables are the difficulty of 
using pointer and reference variables correctly, the cost of references to the 218 Chapter 5 Names, Bindings, and Scopes 
variables, and the complexity of the required storage management implementa\-
tion. This is essentially the problem of heap management, which is costly and complicated. Implementation methods for explicit heap\-dynamic variables are discussed at length in Chapter 6\.
5\.4\.3\.4 Implicit Heap\-Dynamic Variables
Implicit heap\-dynamic variables are bound to heap storage only when 
they are assigned values. In fact, all their attributes are bound every time they are assigned. For example, consider the following JavaScript assignment statement:
highs \= \[74, 84, 86, 90, 71];
Regardless of whether the variable named highs was previously used in the 
program or what it was used for, it is now an array of five numeric values.
The advantage of such variables is that they have the highest degree of 
flexibility, allowing highly generic code to be written. One disadvantage of implicit heap\-dynamic variables is the run\-time overhead of maintaining all the dynamic attributes, which could include array subscript types and ranges, among others. Another disadvantage is the loss of some error detection by the compiler, as discussed in Section 5\.4\.2\.2\. Examples of implicit heap\-dynamic variables in JavaScript appear in Section 5\.4\.2\.2\.
5\.5 Scope 
One of the important factors in understanding variables is scope. The scope of 
a variable is the range of statements in which the variable is visible. A variable is visible in a statement if it can be referenced in that statement.
The scope rules of a language determine how a particular occurrence of a 
name is associated with a variable, or in the case of a functional language, how a name is associated with an expression. In particular, scope rules determine how references to variables declared outside the currently executing subpro\-gram or block are associated with their declarations and thus their attributes (blocks are discussed in Section 5\.5\.2\). A clear understanding of these rules for a language is therefore essential to the ability to write or read programs in that language.
A variable is local in a program unit or block if it is declared there. 
The nonlocal variables of a program unit or block are those that are vis\-ible within the program unit or block but are not declared there. Global variables are a special category of nonlocal variables. They are discussed in Section 5\.5\.4\.
Scoping issues of classes, packages, and namespaces are discussed in 
Chapter 11\. 5\.5 Scope 219
5\.5\.1 Static Scope
ALGOL 60 introduced the method of binding names to nonlocal variables 
called static scoping ,6 which has been copied by many subsequent imperative 
languages and many nonimperative languages as well. Static scoping is so named because the scope of a variable can be statically determined—that is, prior to execution. This permits a human program reader (and a compiler) to determine the type of every variable in the program simply by examining its source code.
There are two categories of static\-scoped languages: those in which sub\-
programs can be nested, which creates nested static scopes, and those in which subprograms cannot be nested. In the latter category, static scopes are also created by subprograms but nested scopes are created only by nested class definitions and blocks.
Ada, JavaScript, Common LISP , Scheme, Fortran 2003\+, F\#, and Python 
allow nested subprograms, but the C\-based languages do not.
Our discussion of static scoping in this section focuses on those lan\-
guages that allow nested subprograms. Initially, we assume that all scopes are 
associated with program units and that all referenced nonlocal variables are declared in other program units.
7 In this chapter, it is assumed that scoping 
is the only method of accessing nonlocal variables in the languages under discussion. This is not true for all languages. It is not even true for all lan\-
guages that use static scoping, but the assumption simplifies the discussion here.
When the reader of a program finds a reference to a variable, the attri\-
butes of the variable can be determined by finding the statement in which it is declared (either explicitly or implicitly). In static\-scoped languages with nested subprograms, this process can be thought of in the following way. Suppose a reference is made to a variable 
x in subprogram sub1 . The correct declara\-
tion is found by first searching the declarations of subprogram sub1 . If no 
declaration is found for the variable there, the search continues in the declara\-tions of the subprogram that declared subprogram 
sub1 , which is called its 
static parent . If a declaration of x is not found there, the search continues to 
the next\-larger enclosing unit (the unit that declared sub1 ’s parent), and so 
forth, until a declaration for x is found or the largest unit’s declarations have 
been searched without success. In that case, an undeclared variable error is reported. The static parent of subprogram 
sub1 , and its static parent, and 
so forth up to and including the largest enclosing subprogram, are called the static ancestors of 
sub1 . Actual implementation techniques for static scop\-
ing, which are discussed in Chapter 10, are usually much more efficient than the process just described.
 6\. Static scoping is sometimes called lexical scoping .
 7\. Nonlocal variables not defined in other program units are discussed in Section 5\.5\.4\.220 Chapter 5 Names, Bindings, and Scopes 
Consider the following JavaScript function, big, in which the two func\-
tions sub1 and sub2 are nested:
function big() {
 function sub1() {
 var x \= 7;
 sub2();
 } function sub2() {
 var y \= x;
 } var x \= 3;
 sub1();
}
Under static scoping, the reference to the variable x in sub2 is to the x declared 
in the procedure big. This is true because the search for x begins in the pro\-
cedure in which the reference occurs, sub2 , but no declaration for x is found 
there. The search continues in the static parent of sub2 , big, where the dec\-
laration of x is found. The x declared in sub1 is ignored, because it is not in 
the static ancestry of sub2 .
In some languages that use static scoping, regardless of whether nested 
subprograms are allowed, some variable declarations can be hidden from some other code segments. For example, consider again the JavaScript function 
big. 
The variable x is declared in both big and in sub1 , which is nested inside big. 
Within sub1 , every simple reference to x is to the local x. Therefore, the outer 
x is hidden from sub1 .
In Ada, hidden variables from ancestor scopes can be accessed with selec\-
tive references, which include the ancestor scope’s name. For example, if our previous example function 
big were written in Ada, the x declared in big 
could be accessed in sub1 by the reference big.x .
5\.5\.2 Blocks
Many languages allow new static scopes to be defined in the midst of execut\-able code. This powerful concept, introduced in ALGOL 60, allows a section of code to have its own local variables whose scope is minimized. Such vari\-ables are typically stack dynamic, so their storage is allocated when the section is entered and deallocated when the section is exited. Such a section of code is called a block . Blocks provide the origin of the phrase block \-structured 
language .
The C\-based languages allow any compound statement (a statement 
sequence surrounded by matched braces) to have declarations and thereby define a new scope. Such compound statements are called blocks. For example, if 
list were an integer array, one could write 5\.5 Scope 221
if (list\[i] \< list\[j]) {
 int temp;
 temp \= list\[i]; list\[i] \= list\[j]; list\[j] \= temp;}
The scopes created by blocks, which could be nested in larger blocks, 
are treated exactly like those created by subprograms. References to vari\-ables in a block that are not declared there are connected to declarations by searching enclosing scopes (blocks or subprograms) in order of increasing size.
Consider the following skeletal C function:
void sub() {
 int count;
 . . .
 while (. . .) {
 int count;
 count\+\+; . . .
 }
 . . .
}
The reference to count in the while loop is to that loop’s local count . In 
this case, the count of sub is hidden from the code inside the while loop. In 
general, a declaration for a variable effectively hides any declaration of a vari\-able with the same name in a larger enclosing scope.
8 Note that this code is 
legal in C and C\+\+ but illegal in Java and C\#. The designers of Java and C\# believed that the reuse of names in nested blocks was too error prone to be allowed.
Although JavaScript uses static scoping for its nested functions, non\-
function blocks cannot be defined in the language.
Most functional programming languages include a construct that is related 
to the blocks of the imperative languages, usually named let. These constructs 
have two parts, the first of which is to bind names to values, usually specified as expressions. The second part is an expression that uses the names defined in the first part. Programs in functional languages are comprised of expressions, rather than statements. Therefore, the final part of a 
let construct is an expression, 
 8\. As discussed in Section 5\.5\.4, in C\+\+, such hidden global variables can be accessed in the 
inner scope using the scope operator ( ::).222 Chapter 5 Names, Bindings, and Scopes 
rather than a statement. In Scheme, a let construct is a call to the function LET 
with the following form:
(LET (
 (name1 expression1\)
 . . .
 (namen expressionn))
 expression
)
The semantics of the call to LET is as follows: The first n expressions are 
evaluated and the values are assigned to the associated names. Then, the final expression is evaluated and the return value of 
LET is that value. This differs 
from a block in an imperative language in that the names are of values; they are not variables in the imperative sense. Once set, they cannot be changed. However, they are like local variables in a block in an imperative language in that their scope is local to the call to 
LET. Consider the following call to LET:
(LET (
 (top (\+ a b))
 (bottom (\- c d)))
 (/ top bottom)
)
This call computes and returns the value of the expression (a \+ b) / (c – d) .
In ML, the form of a let construct is as follows:
let
 val name1 \= expression1
 . . . 
val namen \= expressionn
in 
 expression
end;
Each val statement binds a name to an expression. As with Scheme, the 
names in the first part are like the named constants of imperative languages; once set, they cannot be changed.
9 Consider the following let construct:
let
 val top \= a \+ b
 val bottom \= c \- d
in
 top / bottom
end;
 9\. In Chapter 15, we will see that they can be reset, but that the process actually creates a new 
name. 5\.5 Scope 223
The general form of a let construct in F\# is as follows:
let left\_side \= expression
The left\_side of let can be a name or a tuple pattern (a sequence of names 
separated by commas).
The scope of a name defined with let inside a function definition is from 
the end of the defining expression to the end of the function. The scope of let 
can be limited by indenting the following code, which creates a new local scope. Although any indentation will work, the convention is that the indentation is four spaces. Consider the following code:
let n1 \=
 let n2 \= 7
 let n3 \= n2 \+ 3
 n3;;
let n4 \= n3 \+ n1;;
The scope of n1 extends over all of the code. However, the scope of n2 and 
n3 ends when the indentation ends. So, the use of n3 in the last let causes an 
error. The last line of the let n1 scope is the value bound to n1; it could be 
any expression.
Chapter 15, includes more details of the let constructs in Scheme, ML, 
Haskell, and F\#.
5\.5\.3 Declaration Order
In C89, as well as in some other languages, all data declarations in a function except those in nested blocks must appear at the beginning of the function. However, some languages—for example, C99, C\+\+, Java, JavaScript, and C\#—allow variable declarations to appear anywhere a statement can appear in a program unit. Declarations may create scopes that are not associated with compound statements or subprograms. For example, in C99, C\+\+, and Java, the scope of all local variables is from their declarations to the ends of the blocks in which those declarations appear. However, in C\#, the scope of any variable declared in a block is the whole block, regardless of the posi\-tion of the declaration in the block, as long as it is not in a nested block. The same is true for methods. Note that C\# still requires that all variables be declared before they are used. Therefore, although the scope of a vari\-able extends from the declaration to the top of the block or subprogram in which that declaration appears, the variable still cannot be used above its declaration.
In JavaScript, local variables can be declared anywhere in a function, 
but the scope of such a variable is always the entire function. If used before its declaration in the function, such a variable has the value 
undefined .224 Chapter 5 Names, Bindings, and Scopes 
The for statements of C\+\+, Java, and C\# allow variable definitions in 
their initialization expressions. In early versions of C\+\+, the scope of such a variable was from its definition to the end of the smallest enclosing block. In the standard version, however, the scope is restricted to the 
for construct, as 
is the case with Java and C\#. Consider the following skeletal method:
void fun() {
 . . .
 for (int count \= 0; count \< 10; count\+\+){
 . . .
 }
 . . .
}
In later versions of C\+\+, as well as in Java and C\#, the scope of count is from 
the for statement to the end of its body.
5\.5\.4 Global Scope
Some languages, including C, C\+\+, PHP , JavaScript, and Python, allow a program structure that is a sequence of function definitions, in which vari\-able definitions can appear outside the functions. Definitions outside func\-tions in a file create global variables, which potentially can be visible to those functions.
C and C\+\+ have both declarations and definitions of global data. Declara\-
tions specify types and other attributes but do not cause allocation of storage. Definitions specify attributes and cause storage allocation. For a specific global 
name, a C program can have any number of compatible declarations, but only a single definition.
A declaration of a variable outside function definitions specifies that the 
variable is defined in a different file. A global variable in C is implicitly visible in all subsequent functions in the file, except those that include a declaration of a local variable with the same name. A global variable that is defined after a function can be made visible in the function by declaring it to be external, as in the following:
extern int sum;
In C99, definitions of global variables usually have initial values. Declarations 
of global variables never have initial values. If the declaration is outside function definitions, it need not include the 
extern qualifier.
This idea of declarations and definitions carries over to the functions 
of C and C\+\+, where prototypes declare names and interfaces of functions but do not provide their code. Function definitions, on the other hand, are complete. 5\.5 Scope 225
In C\+\+, a global variable that is hidden by a local with the same name can 
be accessed using the scope operator ( ::). For example, if x is a global that is 
hidden in a function by a local named x, the global could be referenced as ::x.
PHP statements can be interspersed with function definitions. Variables 
in PHP are implicitly declared when they appear in statements. Any variable that is implicitly declared outside any function is a global variable; variables implicitly declared in functions are local variables. The scope of global variables extends from their declarations to the end of the program but skips over any subsequent function definitions. So, global variables are not implicitly visible in any function. Global variables can be made visible in functions in their scope in two ways: (1\) If the function includes a local variable with the same name as a global, that global can be accessed through the 
$GLOBALS array, using 
the name of the global as a string literal subscript, and (2\) if there is no local variable in the function with the same name as the global, the global can be made visible by including it in a 
global declaration statement. Consider the 
following example:
$day \= "Monday";
$month \= "January";
function calendar() {
 $day \= "Tuesday";
 global $month;
 print "local day is $day   
";
 $gday \= $GLOBALS\['day'];
 print "global day is $gday   
";
 print "global month is $month****~~~~******~~******~~~~